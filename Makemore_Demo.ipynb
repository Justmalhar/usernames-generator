{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5JnrhP0W/SR4HZruJd099",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justmalhar/usernames-generator/blob/main/Makemore_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f01TdPn3SYlL",
        "outputId": "882f72f7-22f9-4e74-9f1d-a1825ca73d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'makemore'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 64 (delta 20), reused 16 (delta 16), pack-reused 41 (from 1)\u001b[K\n",
            "Receiving objects: 100% (64/64), 123.29 KiB | 17.61 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/makemore.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iniZbXTT1La",
        "outputId": "603359e5-9266-46d9-d56b-5f2e86ca36d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mmakemore\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd makemore/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykywwTJ4T2tS",
        "outputId": "af13187f-bed8-43d7-ef4f-ad6ec7884665"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/makemore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd && ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmB4ZlzaT4hq",
        "outputId": "5fe94e98-53ea-405a-c7bc-2e0b4db8b96a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/makemore\n",
            "total 276\n",
            "drwxr-xr-x 3 root root   4096 Sep  2 05:57 .\n",
            "drwxr-xr-x 1 root root   4096 Sep  2 05:57 ..\n",
            "drwxr-xr-x 8 root root   4096 Sep  2 05:57 .git\n",
            "-rw-r--r-- 1 root root   1072 Sep  2 05:57 LICENSE\n",
            "-rw-r--r-- 1 root root  29659 Sep  2 05:57 makemore.py\n",
            "-rw-r--r-- 1 root root 228145 Sep  2 05:57 names.txt\n",
            "-rw-r--r-- 1 root root   3033 Sep  2 05:57 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/Justmalhar/usernames-generator/main/users.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDvfCe6xT7o8",
        "outputId": "1265819b-a168-4183-d989-6a0de09e7c1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 12.9M  100 12.9M    0     0  10.3M      0  0:00:01  0:00:01 --:--:-- 10.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9aCeaTUlTR",
        "outputId": "65a96e1f-8f5b-4cf5-eabd-3a61952d7c2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE  makemore.py  names.txt  README.md  users.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat README.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7K6cbuqUmxe",
        "outputId": "5b635e72-f577-4ed0-cde0-b185b4792fa3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# makemore\n",
            "\n",
            "makemore takes one text file as input, where each line is assumed to be one training thing, and generates more things like it. Under the hood, it is an autoregressive character-level language model, with a wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT). For example, we can feed it a database of names, and makemore will generate cool baby name ideas that all sound name-like, but are not already existing names. Or if we feed it a database of company names then we can generate new ideas for a name of a company. Or we can just feed it valid scrabble words and generate english-like babble.\n",
            "\n",
            "This is not meant to be too heavyweight library with a billion switches and knobs. It is one hackable file, and is mostly intended for educational purposes. [PyTorch](https://pytorch.org) is the only requirement.\n",
            "\n",
            "Current implementation follows a few key papers:\n",
            "\n",
            "- Bigram (one character predicts the next one with a lookup table of counts)\n",
            "- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
            "- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)\n",
            "- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
            "- LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
            "- GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
            "- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)\n",
            "\n",
            "### Usage\n",
            "\n",
            "The included `names.txt` dataset, as an example, has the most common 32K names takes from [ssa.gov](https://www.ssa.gov/oact/babynames/) for the year 2018. It looks like:\n",
            "\n",
            "```\n",
            "emma\n",
            "olivia\n",
            "ava\n",
            "isabella\n",
            "sophia\n",
            "charlotte\n",
            "...\n",
            "```\n",
            "\n",
            "Let's point the script at it:\n",
            "\n",
            "```bash\n",
            "$ python makemore.py -i names.txt -o names\n",
            "```\n",
            "\n",
            "Training progress and logs and model will all be saved to the working directory `names`. The default model is a super tiny 200K param transformer; Many more training configurations are available - see the argparse and read the code. Training does not require any special hardware, it runs on my Macbook Air and will run on anything else, but if you have a GPU then training will fly faster. As training progresses the script will print some samples throughout. However, if you'd like to sample manually, you can use the `--sample-only` flag, e.g. in a separate terminal do:\n",
            "\n",
            "```bash\n",
            "$ python makemore.py -i names.txt -o names --sample-only\n",
            "```\n",
            "\n",
            "This will load the best model so far and print more samples on demand. Here are some unique baby names that get eventually generated from current default settings (test logprob of ~1.92, though much lower logprobs are achievable with some hyperparameter tuning):\n",
            "\n",
            "```\n",
            "dontell\n",
            "khylum\n",
            "camatena\n",
            "aeriline\n",
            "najlah\n",
            "sherrith\n",
            "ryel\n",
            "irmi\n",
            "taislee\n",
            "mortaz\n",
            "akarli\n",
            "maxfelynn\n",
            "biolett\n",
            "zendy\n",
            "laisa\n",
            "halliliana\n",
            "goralynn\n",
            "brodynn\n",
            "romima\n",
            "chiyomin\n",
            "loghlyn\n",
            "melichae\n",
            "mahmed\n",
            "irot\n",
            "helicha\n",
            "besdy\n",
            "ebokun\n",
            "lucianno\n",
            "```\n",
            "\n",
            "Have fun!\n",
            "\n",
            "### License\n",
            "\n",
            "MIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3pXTn4hKJes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python makemore.py -i users.txt -o users"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1HZvIZzUrJ0",
        "outputId": "ddec4c86-c12c-4d25-8a2e-53b9c6a014e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step 789720 | loss 2.4853 | step time 39.16ms\n",
            "step 789730 | loss 2.4137 | step time 40.83ms\n",
            "step 789740 | loss 2.3403 | step time 36.68ms\n",
            "step 789750 | loss 2.4455 | step time 37.67ms\n",
            "step 789760 | loss 2.4023 | step time 38.82ms\n",
            "step 789770 | loss 2.5135 | step time 59.39ms\n",
            "step 789780 | loss 2.3874 | step time 67.63ms\n",
            "step 789790 | loss 2.4388 | step time 55.82ms\n",
            "step 789800 | loss 2.4080 | step time 52.80ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "fireppd\n",
            "mustforfucksofave1\n",
            "Nookay\n",
            "Twaklot\n",
            "Monteine86\n",
            "wutchz001\n",
            "thatvissamethrowary\n",
            "shutinquestion\n",
            "Nelsley\n",
            "skdfask\n",
            "--------------------------------------------------------------------------------\n",
            "step 789810 | loss 2.3863 | step time 55.92ms\n",
            "step 789820 | loss 2.5762 | step time 55.79ms\n",
            "step 789830 | loss 2.5498 | step time 61.32ms\n",
            "step 789840 | loss 2.4455 | step time 36.30ms\n",
            "step 789850 | loss 2.4477 | step time 40.26ms\n",
            "step 789860 | loss 2.5571 | step time 38.52ms\n",
            "step 789870 | loss 2.3828 | step time 35.69ms\n",
            "step 789880 | loss 2.4104 | step time 37.69ms\n",
            "step 789890 | loss 2.4040 | step time 36.72ms\n",
            "step 789900 | loss 2.4116 | step time 35.68ms\n",
            "step 789910 | loss 2.4527 | step time 47.28ms\n",
            "step 789920 | loss 2.3936 | step time 36.79ms\n",
            "step 789930 | loss 2.4862 | step time 40.65ms\n",
            "step 789940 | loss 2.3930 | step time 38.52ms\n",
            "step 789950 | loss 2.4532 | step time 38.06ms\n",
            "step 789960 | loss 2.4572 | step time 40.67ms\n",
            "step 789970 | loss 2.5546 | step time 38.58ms\n",
            "step 789980 | loss 2.2845 | step time 39.64ms\n",
            "step 789990 | loss 2.4912 | step time 36.14ms\n",
            "step 790000 | loss 2.7046 | step time 35.40ms\n",
            "step 790000 train loss: 2.430018424987793 test loss: 2.4799625873565674\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "fornojunctilisflofer\n",
            "throwyphychonor\n",
            "BullyCommentSqn\n",
            "Autom\n",
            "fgheadisup\n",
            "inlinearrage\n",
            "john33c\n",
            "Dendo_991\n",
            "asdfk2guejgzzlXc\n",
            "yamo18\n",
            "--------------------------------------------------------------------------------\n",
            "step 790010 | loss 2.3756 | step time 37.20ms\n",
            "step 790020 | loss 2.6704 | step time 43.81ms\n",
            "step 790030 | loss 2.3794 | step time 47.04ms\n",
            "step 790040 | loss 2.5576 | step time 37.77ms\n",
            "step 790050 | loss 2.3330 | step time 56.43ms\n",
            "step 790060 | loss 2.3538 | step time 52.22ms\n",
            "step 790070 | loss 2.4336 | step time 51.14ms\n",
            "step 790080 | loss 2.4850 | step time 60.16ms\n",
            "step 790090 | loss 2.4774 | step time 55.46ms\n",
            "step 790100 | loss 2.6068 | step time 61.50ms\n",
            "step 790110 | loss 2.5062 | step time 57.89ms\n",
            "step 790120 | loss 2.4673 | step time 57.66ms\n",
            "step 790130 | loss 2.1494 | step time 62.29ms\n",
            "step 790140 | loss 2.3918 | step time 54.26ms\n",
            "step 790150 | loss 2.3759 | step time 61.12ms\n",
            "step 790160 | loss 2.3367 | step time 35.45ms\n",
            "step 790170 | loss 2.3894 | step time 38.30ms\n",
            "step 790180 | loss 2.4822 | step time 36.97ms\n",
            "step 790190 | loss 2.5644 | step time 41.66ms\n",
            "step 790200 | loss 2.5441 | step time 41.94ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "mamadaman\n",
            "Ases26\n",
            "GrimBreastlyDemon\n",
            "throw2266\n",
            "momrogophilla\n",
            "Jkco\n",
            "etteriagon\n",
            "confusedswithdavs\n",
            "Faedreade\n",
            "fordevvilling\n",
            "--------------------------------------------------------------------------------\n",
            "step 790210 | loss 2.5829 | step time 40.00ms\n",
            "step 790220 | loss 2.4049 | step time 39.78ms\n",
            "step 790230 | loss 2.4305 | step time 40.04ms\n",
            "step 790240 | loss 2.2600 | step time 40.53ms\n",
            "step 790250 | loss 2.3364 | step time 39.71ms\n",
            "step 790260 | loss 2.4019 | step time 46.89ms\n",
            "step 790270 | loss 2.6001 | step time 36.82ms\n",
            "step 790280 | loss 2.5083 | step time 38.53ms\n",
            "step 790290 | loss 2.3548 | step time 40.87ms\n",
            "step 790300 | loss 2.4148 | step time 36.85ms\n",
            "step 790310 | loss 2.5363 | step time 37.95ms\n",
            "step 790320 | loss 2.4627 | step time 36.62ms\n",
            "step 790330 | loss 2.5813 | step time 39.37ms\n",
            "step 790340 | loss 2.4310 | step time 37.48ms\n",
            "step 790350 | loss 2.4723 | step time 34.83ms\n",
            "step 790360 | loss 2.5366 | step time 39.94ms\n",
            "step 790370 | loss 2.3565 | step time 38.01ms\n",
            "step 790380 | loss 2.3653 | step time 38.54ms\n",
            "step 790390 | loss 2.3902 | step time 39.62ms\n",
            "step 790400 | loss 2.3881 | step time 62.67ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "CovertAtTorwast\n",
            "matthoillmoochkil\n",
            "FA2LU\n",
            "LoGalade716\n",
            "HiloError\n",
            "NeonOvers\n",
            "Numbfox\n",
            "Great_gama\n",
            "Ankalightlytoactevan\n",
            "halick123412343929\n",
            "--------------------------------------------------------------------------------\n",
            "step 790410 | loss 2.3260 | step time 64.01ms\n",
            "step 790420 | loss 2.4421 | step time 58.90ms\n",
            "step 790430 | loss 2.5995 | step time 63.59ms\n",
            "step 790440 | loss 2.4272 | step time 58.24ms\n",
            "step 790450 | loss 2.5184 | step time 57.51ms\n",
            "step 790460 | loss 2.5529 | step time 38.95ms\n",
            "step 790470 | loss 2.3057 | step time 36.39ms\n",
            "step 790480 | loss 2.5922 | step time 37.87ms\n",
            "step 790490 | loss 2.4240 | step time 36.59ms\n",
            "step 790500 | loss 2.3914 | step time 37.78ms\n",
            "step 790500 train loss: 2.434743642807007 test loss: 2.4807116985321045\n",
            "step 790510 | loss 2.3782 | step time 42.58ms\n",
            "step 790520 | loss 2.4295 | step time 36.88ms\n",
            "step 790530 | loss 2.4424 | step time 38.19ms\n",
            "step 790540 | loss 2.4722 | step time 39.29ms\n",
            "step 790550 | loss 2.6393 | step time 36.38ms\n",
            "step 790560 | loss 2.3577 | step time 39.38ms\n",
            "step 790570 | loss 2.6035 | step time 38.12ms\n",
            "step 790580 | loss 2.6533 | step time 36.20ms\n",
            "step 790590 | loss 2.6483 | step time 36.84ms\n",
            "step 790600 | loss 2.3258 | step time 41.64ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "fozentel31\n",
            "tursstan12\n",
            "blazex\n",
            "talkcatch56\n",
            "rydishxtraplement\n",
            "magic_specia\n",
            "digalb1\n",
            "sccyrongs\n",
            "DeadTigero\n",
            "theharrying\n",
            "--------------------------------------------------------------------------------\n",
            "step 790610 | loss 2.3682 | step time 38.08ms\n",
            "step 790620 | loss 2.6977 | step time 39.65ms\n",
            "step 790630 | loss 2.2957 | step time 36.42ms\n",
            "step 790640 | loss 2.3610 | step time 38.38ms\n",
            "step 790650 | loss 2.4169 | step time 39.23ms\n",
            "step 790660 | loss 2.3783 | step time 37.17ms\n",
            "step 790670 | loss 2.3646 | step time 36.90ms\n",
            "step 790680 | loss 2.4180 | step time 54.15ms\n",
            "step 790690 | loss 2.5080 | step time 54.42ms\n",
            "step 790700 | loss 2.5077 | step time 50.87ms\n",
            "step 790710 | loss 2.6625 | step time 59.82ms\n",
            "step 790720 | loss 2.5301 | step time 55.52ms\n",
            "step 790730 | loss 2.4362 | step time 61.11ms\n",
            "step 790740 | loss 2.2863 | step time 52.87ms\n",
            "step 790750 | loss 2.5665 | step time 55.20ms\n",
            "step 790760 | loss 2.5343 | step time 60.40ms\n",
            "step 790770 | loss 2.2976 | step time 34.36ms\n",
            "step 790780 | loss 2.5488 | step time 36.35ms\n",
            "step 790790 | loss 2.3612 | step time 34.70ms\n",
            "step 790800 | loss 2.6579 | step time 38.50ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "bluejobistobooker\n",
            "RyanAcc92\n",
            "kewkorky\n",
            "House_woah\n",
            "myasassforrone\n",
            "krayam002\n",
            "Furappancer\n",
            "QzfgjeWsw7fn\n",
            "VanKNoqwoli23\n",
            "GoAmKnight\n",
            "--------------------------------------------------------------------------------\n",
            "step 790810 | loss 2.5870 | step time 36.09ms\n",
            "step 790820 | loss 2.5941 | step time 38.04ms\n",
            "step 790830 | loss 2.3872 | step time 40.75ms\n",
            "step 790840 | loss 2.4098 | step time 37.17ms\n",
            "step 790850 | loss 2.6056 | step time 36.72ms\n",
            "step 790860 | loss 2.4388 | step time 36.62ms\n",
            "step 790870 | loss 2.3049 | step time 36.65ms\n",
            "step 790880 | loss 2.2376 | step time 35.92ms\n",
            "step 790890 | loss 2.3595 | step time 37.07ms\n",
            "step 790900 | loss 2.4676 | step time 43.05ms\n",
            "step 790910 | loss 2.3994 | step time 38.03ms\n",
            "step 790920 | loss 2.2663 | step time 37.36ms\n",
            "step 790930 | loss 2.5478 | step time 43.60ms\n",
            "step 790940 | loss 2.3697 | step time 39.35ms\n",
            "step 790950 | loss 2.5507 | step time 40.18ms\n",
            "step 790960 | loss 2.5221 | step time 43.27ms\n",
            "step 790970 | loss 2.5143 | step time 37.62ms\n",
            "step 790980 | loss 2.4221 | step time 41.64ms\n",
            "step 790990 | loss 2.5364 | step time 35.55ms\n",
            "step 791000 | loss 2.4681 | step time 37.45ms\n",
            "step 791000 train loss: 2.4282209873199463 test loss: 2.484074592590332\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "not_joker_kcoloid\n",
            "ldurren\n",
            "jesusfine\n",
            "helixix\n",
            "Z-C-s\n",
            "poobrated\n",
            "coanay\n",
            "Nonameforme\n",
            "sNeedRahWhich\n",
            "aiaka\n",
            "--------------------------------------------------------------------------------\n",
            "step 791010 | loss 2.3947 | step time 68.24ms\n",
            "step 791020 | loss 2.4688 | step time 58.55ms\n",
            "step 791030 | loss 2.4200 | step time 55.22ms\n",
            "step 791040 | loss 2.4667 | step time 57.85ms\n",
            "step 791050 | loss 2.4884 | step time 58.32ms\n",
            "step 791060 | loss 2.3513 | step time 43.25ms\n",
            "step 791070 | loss 2.4391 | step time 37.18ms\n",
            "step 791080 | loss 2.3925 | step time 38.29ms\n",
            "step 791090 | loss 2.3922 | step time 39.47ms\n",
            "step 791100 | loss 2.5056 | step time 45.15ms\n",
            "step 791110 | loss 2.5506 | step time 40.17ms\n",
            "step 791120 | loss 2.4146 | step time 36.23ms\n",
            "step 791130 | loss 2.3341 | step time 38.47ms\n",
            "step 791140 | loss 2.4506 | step time 34.51ms\n",
            "step 791150 | loss 2.4345 | step time 37.40ms\n",
            "step 791160 | loss 2.5372 | step time 37.47ms\n",
            "step 791170 | loss 2.3870 | step time 37.25ms\n",
            "step 791180 | loss 2.5031 | step time 50.82ms\n",
            "step 791190 | loss 2.5268 | step time 37.69ms\n",
            "step 791200 | loss 2.3012 | step time 39.01ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "mahfefot\n",
            "yerhelpsy2\n",
            "liveanthunder405\n",
            "spublsjr\n",
            "TaemanDiaper\n",
            "WorriorBirdz\n",
            "ShoppingDong\n",
            "outlacgaton\n",
            "limelaurae\n",
            "yousayissuresaremypa\n",
            "--------------------------------------------------------------------------------\n",
            "step 791210 | loss 2.4359 | step time 37.40ms\n",
            "step 791220 | loss 2.3951 | step time 36.53ms\n",
            "step 791230 | loss 2.6708 | step time 35.93ms\n",
            "step 791240 | loss 2.5320 | step time 36.03ms\n",
            "step 791250 | loss 2.3366 | step time 35.54ms\n",
            "step 791260 | loss 2.4483 | step time 35.66ms\n",
            "step 791270 | loss 2.4928 | step time 36.07ms\n",
            "step 791280 | loss 2.4224 | step time 37.86ms\n",
            "step 791290 | loss 2.3949 | step time 37.08ms\n",
            "step 791300 | loss 2.6365 | step time 51.25ms\n",
            "step 791310 | loss 2.5293 | step time 57.01ms\n",
            "step 791320 | loss 2.3997 | step time 52.89ms\n",
            "step 791330 | loss 2.4781 | step time 58.15ms\n",
            "step 791340 | loss 2.3950 | step time 57.84ms\n",
            "step 791350 | loss 2.4528 | step time 54.87ms\n",
            "step 791360 | loss 2.2984 | step time 54.86ms\n",
            "step 791370 | loss 2.3797 | step time 56.53ms\n",
            "step 791380 | loss 2.3414 | step time 38.91ms\n",
            "step 791390 | loss 2.5778 | step time 37.09ms\n",
            "step 791400 | loss 2.5533 | step time 39.05ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "WhomoRusters\n",
            "SphenPower\n",
            "WowNotOpianon\n",
            "Dginger9172\n",
            "brittpizarruu\n",
            "lalanwulf\n",
            "julkiesdisgunner\n",
            "ImInATheBigGuy\n",
            "coffeet028\n",
            "arkie436\n",
            "--------------------------------------------------------------------------------\n",
            "step 791410 | loss 2.4997 | step time 39.25ms\n",
            "step 791420 | loss 2.4263 | step time 40.41ms\n",
            "step 791430 | loss 2.3979 | step time 36.58ms\n",
            "step 791440 | loss 2.3987 | step time 38.98ms\n",
            "step 791450 | loss 2.4090 | step time 38.88ms\n",
            "step 791460 | loss 2.5800 | step time 42.86ms\n",
            "step 791470 | loss 2.5187 | step time 35.16ms\n",
            "step 791480 | loss 2.4829 | step time 36.61ms\n",
            "step 791490 | loss 2.3826 | step time 40.12ms\n",
            "step 791500 | loss 2.5157 | step time 37.66ms\n",
            "step 791500 train loss: 2.482391595840454 test loss: 2.4862122535705566\n",
            "step 791510 | loss 2.3787 | step time 40.01ms\n",
            "step 791520 | loss 2.5505 | step time 35.29ms\n",
            "step 791530 | loss 2.5843 | step time 38.07ms\n",
            "step 791540 | loss 2.5227 | step time 39.14ms\n",
            "step 791550 | loss 2.4432 | step time 38.29ms\n",
            "step 791560 | loss 2.3080 | step time 36.83ms\n",
            "step 791570 | loss 2.6608 | step time 56.38ms\n",
            "step 791580 | loss 2.2347 | step time 37.37ms\n",
            "step 791590 | loss 2.3886 | step time 36.38ms\n",
            "step 791600 | loss 2.4066 | step time 61.83ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "man0117\n",
            "conserfit\n",
            "FloweeeInspace\n",
            "Atlas_strusstiPeda\n",
            "fammore_dick_product\n",
            "msternascar\n",
            "Meyersharp\n",
            "ksjdsue\n",
            "malla_lug\n",
            "asauranzor\n",
            "--------------------------------------------------------------------------------\n",
            "step 791610 | loss 2.6004 | step time 55.72ms\n",
            "step 791620 | loss 2.5530 | step time 55.77ms\n",
            "step 791630 | loss 2.3992 | step time 53.83ms\n",
            "step 791640 | loss 2.2788 | step time 60.00ms\n",
            "step 791650 | loss 2.5243 | step time 59.05ms\n",
            "step 791660 | loss 2.5311 | step time 66.17ms\n",
            "step 791670 | loss 2.4658 | step time 38.80ms\n",
            "step 791680 | loss 2.3872 | step time 40.54ms\n",
            "step 791690 | loss 2.3362 | step time 37.64ms\n",
            "step 791700 | loss 2.5775 | step time 44.94ms\n",
            "step 791710 | loss 2.5093 | step time 39.21ms\n",
            "step 791720 | loss 2.3864 | step time 39.65ms\n",
            "step 791730 | loss 2.4748 | step time 38.16ms\n",
            "step 791740 | loss 2.5143 | step time 36.88ms\n",
            "step 791750 | loss 2.4590 | step time 47.40ms\n",
            "step 791760 | loss 2.3758 | step time 36.22ms\n",
            "step 791770 | loss 2.4370 | step time 38.57ms\n",
            "step 791780 | loss 2.4585 | step time 41.40ms\n",
            "step 791790 | loss 2.3152 | step time 35.97ms\n",
            "step 791800 | loss 2.3846 | step time 38.50ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "womanleRetardso\n",
            "newbaccalled_joke\n",
            "RegisteVB\n",
            "__Amrashar\n",
            "bradwallreal\n",
            "publici2221\n",
            "KROSSboLA\n",
            "Makedolesauce\n",
            "Flietwood99\n",
            "invision\n",
            "--------------------------------------------------------------------------------\n",
            "step 791810 | loss 2.4512 | step time 37.73ms\n",
            "step 791820 | loss 2.4840 | step time 38.02ms\n",
            "step 791830 | loss 2.4178 | step time 43.14ms\n",
            "step 791840 | loss 2.3842 | step time 41.61ms\n",
            "step 791850 | loss 2.4958 | step time 39.50ms\n",
            "step 791860 | loss 2.1687 | step time 35.06ms\n",
            "step 791870 | loss 2.3714 | step time 36.25ms\n",
            "step 791880 | loss 2.7187 | step time 38.11ms\n",
            "step 791890 | loss 2.5733 | step time 35.42ms\n",
            "step 791900 | loss 2.4347 | step time 35.79ms\n",
            "step 791910 | loss 2.4821 | step time 59.84ms\n",
            "step 791920 | loss 2.3564 | step time 54.02ms\n",
            "step 791930 | loss 2.6084 | step time 58.60ms\n",
            "step 791940 | loss 2.2615 | step time 56.32ms\n",
            "step 791950 | loss 2.4037 | step time 60.23ms\n",
            "step 791960 | loss 2.4345 | step time 60.25ms\n",
            "step 791970 | loss 2.4789 | step time 59.72ms\n",
            "step 791980 | loss 2.6274 | step time 55.84ms\n",
            "step 791990 | loss 2.5642 | step time 60.10ms\n",
            "step 792000 | loss 2.3898 | step time 39.25ms\n",
            "step 792000 train loss: 2.4182844161987305 test loss: 2.4841909408569336\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "thisguyyourmodes\n",
            "CareermanKid\n",
            "bozzyhelpme\n",
            "snorax\n",
            "dMergie512\n",
            "mrhidj44\n",
            "supa6fi_class\n",
            "Jachi32\n",
            "bk-to-dark-v\n",
            "FuriBrookA-Minecraft\n",
            "--------------------------------------------------------------------------------\n",
            "step 792010 | loss 2.4473 | step time 38.51ms\n",
            "step 792020 | loss 2.4659 | step time 36.78ms\n",
            "step 792030 | loss 2.5879 | step time 39.19ms\n",
            "step 792040 | loss 2.5296 | step time 39.01ms\n",
            "step 792050 | loss 2.4565 | step time 38.28ms\n",
            "step 792060 | loss 2.6247 | step time 38.50ms\n",
            "step 792070 | loss 2.5452 | step time 36.82ms\n",
            "step 792080 | loss 2.5165 | step time 41.49ms\n",
            "step 792090 | loss 2.4366 | step time 37.61ms\n",
            "step 792100 | loss 2.3812 | step time 37.79ms\n",
            "step 792110 | loss 2.7346 | step time 36.11ms\n",
            "step 792120 | loss 2.3486 | step time 37.45ms\n",
            "step 792130 | loss 2.5341 | step time 41.65ms\n",
            "step 792140 | loss 2.3867 | step time 39.09ms\n",
            "step 792150 | loss 2.6914 | step time 39.24ms\n",
            "step 792160 | loss 2.3695 | step time 37.75ms\n",
            "step 792170 | loss 2.3380 | step time 37.93ms\n",
            "step 792180 | loss 2.4505 | step time 38.44ms\n",
            "step 792190 | loss 2.6694 | step time 37.50ms\n",
            "step 792200 | loss 2.4233 | step time 42.74ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "hkjf\n",
            "Bobdiequestogo\n",
            "fuyd\n",
            "HarrySchlord\n",
            "ciivey_internet\n",
            "bigtineer\n",
            "Bullet307\n",
            "Drakka777\n",
            "NEonavu\n",
            "Lucos132\n",
            "--------------------------------------------------------------------------------\n",
            "step 792210 | loss 2.4133 | step time 54.86ms\n",
            "step 792220 | loss 2.5102 | step time 62.63ms\n",
            "step 792230 | loss 2.4690 | step time 56.65ms\n",
            "step 792240 | loss 2.5444 | step time 57.27ms\n",
            "step 792250 | loss 2.3738 | step time 57.17ms\n",
            "step 792260 | loss 2.5105 | step time 59.11ms\n",
            "step 792270 | loss 2.2979 | step time 57.26ms\n",
            "step 792280 | loss 2.4431 | step time 38.35ms\n",
            "step 792290 | loss 2.4242 | step time 39.88ms\n",
            "step 792300 | loss 2.5523 | step time 37.75ms\n",
            "step 792310 | loss 2.2134 | step time 36.86ms\n",
            "step 792320 | loss 2.6419 | step time 37.34ms\n",
            "step 792330 | loss 2.4404 | step time 38.22ms\n",
            "step 792340 | loss 2.4819 | step time 37.55ms\n",
            "step 792350 | loss 2.2975 | step time 37.54ms\n",
            "step 792360 | loss 2.4052 | step time 40.55ms\n",
            "step 792370 | loss 2.3955 | step time 37.30ms\n",
            "step 792380 | loss 2.4576 | step time 39.23ms\n",
            "step 792390 | loss 2.5489 | step time 36.61ms\n",
            "step 792400 | loss 2.4519 | step time 41.75ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "CallonTheLil\n",
            "SnowPeanuts\n",
            "rvlhotan\n",
            "zenyakii\n",
            "schellathansasatatp\n",
            "PbrooKatlanss\n",
            "NotAMVDaughter\n",
            "Rajirch\n",
            "justfeelshoer69\n",
            "Shootx\n",
            "--------------------------------------------------------------------------------\n",
            "step 792410 | loss 2.6109 | step time 38.51ms\n",
            "step 792420 | loss 2.3187 | step time 38.63ms\n",
            "step 792430 | loss 2.4914 | step time 39.35ms\n",
            "step 792440 | loss 2.5241 | step time 38.80ms\n",
            "step 792450 | loss 2.4142 | step time 37.41ms\n",
            "step 792460 | loss 2.2266 | step time 37.02ms\n",
            "step 792470 | loss 2.2924 | step time 41.80ms\n",
            "step 792480 | loss 2.5562 | step time 40.83ms\n",
            "step 792490 | loss 2.4658 | step time 44.09ms\n",
            "step 792500 | loss 2.4105 | step time 37.51ms\n",
            "step 792500 train loss: 2.4487504959106445 test loss: 2.480879068374634\n",
            "step 792510 | loss 2.5149 | step time 51.79ms\n",
            "step 792520 | loss 2.3619 | step time 58.86ms\n",
            "step 792530 | loss 2.4648 | step time 63.11ms\n",
            "step 792540 | loss 2.3378 | step time 56.64ms\n",
            "step 792550 | loss 2.6569 | step time 58.10ms\n",
            "step 792560 | loss 2.4151 | step time 59.35ms\n",
            "step 792570 | loss 2.3022 | step time 56.57ms\n",
            "step 792580 | loss 2.5309 | step time 61.65ms\n",
            "step 792590 | loss 2.3465 | step time 39.91ms\n",
            "step 792600 | loss 2.4575 | step time 38.04ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Jerrr_Sage\n",
            "Mollekgay2\n",
            "massatin4lyfe\n",
            "DeleteBaom\n",
            "SpooterGurufs\n",
            "cipkaphquis\n",
            "Your_ferdude_my_pls\n",
            "Koreakamahaze\n",
            "Enderwalker\n",
            "Bot1988L\n",
            "--------------------------------------------------------------------------------\n",
            "step 792610 | loss 2.4499 | step time 37.71ms\n",
            "step 792620 | loss 2.3604 | step time 37.53ms\n",
            "step 792630 | loss 2.4304 | step time 38.99ms\n",
            "step 792640 | loss 2.3270 | step time 40.70ms\n",
            "step 792650 | loss 2.6115 | step time 37.92ms\n",
            "step 792660 | loss 2.2498 | step time 38.25ms\n",
            "step 792670 | loss 2.3064 | step time 36.53ms\n",
            "step 792680 | loss 2.4351 | step time 37.41ms\n",
            "step 792690 | loss 2.4902 | step time 36.37ms\n",
            "step 792700 | loss 2.1736 | step time 36.78ms\n",
            "step 792710 | loss 2.5149 | step time 39.00ms\n",
            "step 792720 | loss 2.5644 | step time 36.84ms\n",
            "step 792730 | loss 2.4591 | step time 43.07ms\n",
            "step 792740 | loss 2.5103 | step time 38.46ms\n",
            "step 792750 | loss 2.4460 | step time 40.07ms\n",
            "step 792760 | loss 2.5813 | step time 43.99ms\n",
            "step 792770 | loss 2.3630 | step time 38.30ms\n",
            "step 792780 | loss 2.5007 | step time 40.92ms\n",
            "step 792790 | loss 2.3779 | step time 40.14ms\n",
            "step 792800 | loss 2.1749 | step time 36.74ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "unrpscepc\n",
            "billy895\n",
            "UrTroll\n",
            "NaeLoyal\n",
            "sosio10007\n",
            "n_o\n",
            "Greirr1124\n",
            "nonwhere\n",
            "GetFallanHQ\n",
            "menwybulplus\n",
            "--------------------------------------------------------------------------------\n",
            "step 792810 | loss 2.3947 | step time 54.62ms\n",
            "step 792820 | loss 2.3373 | step time 51.08ms\n",
            "step 792830 | loss 2.4621 | step time 48.88ms\n",
            "step 792840 | loss 2.4868 | step time 67.35ms\n",
            "step 792850 | loss 2.5413 | step time 60.44ms\n",
            "step 792860 | loss 2.4091 | step time 53.80ms\n",
            "step 792870 | loss 2.2725 | step time 57.65ms\n",
            "step 792880 | loss 2.4783 | step time 58.72ms\n",
            "step 792890 | loss 2.4363 | step time 57.57ms\n",
            "step 792900 | loss 2.2968 | step time 69.05ms\n",
            "step 792910 | loss 2.3346 | step time 36.79ms\n",
            "step 792920 | loss 2.4633 | step time 38.41ms\n",
            "step 792930 | loss 2.5606 | step time 38.84ms\n",
            "step 792940 | loss 2.3327 | step time 39.02ms\n",
            "step 792950 | loss 2.6397 | step time 39.38ms\n",
            "step 792960 | loss 2.4786 | step time 38.31ms\n",
            "step 792970 | loss 2.4388 | step time 36.85ms\n",
            "step 792980 | loss 2.4929 | step time 43.38ms\n",
            "step 792990 | loss 2.5052 | step time 40.25ms\n",
            "step 793000 | loss 2.3316 | step time 38.53ms\n",
            "step 793000 train loss: 2.465548276901245 test loss: 2.484851360321045\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "gaybreadz\n",
            "tokylion\n",
            "Gaming486\n",
            "ASALSR3YNJE\n",
            "h3gebapt4\n",
            "ifanseways\n",
            "mspsGoohaza\n",
            "9mobgluey\n",
            "silent_j\n",
            "sixbaw86\n",
            "--------------------------------------------------------------------------------\n",
            "step 793010 | loss 2.2843 | step time 39.89ms\n",
            "step 793020 | loss 2.5447 | step time 38.26ms\n",
            "step 793030 | loss 2.5831 | step time 36.79ms\n",
            "step 793040 | loss 2.4757 | step time 36.63ms\n",
            "step 793050 | loss 2.3507 | step time 38.30ms\n",
            "step 793060 | loss 2.1547 | step time 36.30ms\n",
            "step 793070 | loss 2.3053 | step time 50.53ms\n",
            "step 793080 | loss 2.4649 | step time 38.25ms\n",
            "step 793090 | loss 2.3058 | step time 41.32ms\n",
            "step 793100 | loss 2.4206 | step time 36.70ms\n",
            "step 793110 | loss 2.2823 | step time 36.44ms\n",
            "step 793120 | loss 2.4407 | step time 39.18ms\n",
            "step 793130 | loss 2.4468 | step time 62.07ms\n",
            "step 793140 | loss 2.5684 | step time 52.41ms\n",
            "step 793150 | loss 2.5046 | step time 49.64ms\n",
            "step 793160 | loss 2.3206 | step time 72.88ms\n",
            "step 793170 | loss 2.5064 | step time 56.38ms\n",
            "step 793180 | loss 2.4661 | step time 54.54ms\n",
            "step 793190 | loss 2.5449 | step time 57.72ms\n",
            "step 793200 | loss 2.5192 | step time 55.26ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "misods\n",
            "throwawayaccount\n",
            "aTehNoutor84\n",
            "olukesike\n",
            "luckforeverbread\n",
            "kristybren\n",
            "rugtowlarore\n",
            "bignsh69\n",
            "noyamiekush\n",
            "LorsN\n",
            "--------------------------------------------------------------------------------\n",
            "step 793210 | loss 2.4418 | step time 38.81ms\n",
            "step 793220 | loss 2.5085 | step time 38.62ms\n",
            "step 793230 | loss 2.2998 | step time 35.35ms\n",
            "step 793240 | loss 2.5665 | step time 40.15ms\n",
            "step 793250 | loss 2.2863 | step time 40.30ms\n",
            "step 793260 | loss 2.4780 | step time 37.47ms\n",
            "step 793270 | loss 2.4414 | step time 36.13ms\n",
            "step 793280 | loss 2.4135 | step time 36.51ms\n",
            "step 793290 | loss 2.2388 | step time 36.95ms\n",
            "step 793300 | loss 2.5814 | step time 37.22ms\n",
            "step 793310 | loss 2.4124 | step time 38.56ms\n",
            "step 793320 | loss 2.3479 | step time 41.34ms\n",
            "step 793330 | loss 2.3533 | step time 50.63ms\n",
            "step 793340 | loss 2.5359 | step time 37.83ms\n",
            "step 793350 | loss 2.6705 | step time 35.55ms\n",
            "step 793360 | loss 2.4361 | step time 39.63ms\n",
            "step 793370 | loss 2.2639 | step time 36.81ms\n",
            "step 793380 | loss 2.4160 | step time 38.32ms\n",
            "step 793390 | loss 2.5323 | step time 38.34ms\n",
            "step 793400 | loss 2.4571 | step time 37.79ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "_brain_dogs_the_\n",
            "devilide\n",
            "kkkcs\n",
            "caterinicarweator\n",
            "SLPAGC\n",
            "anthronicpza\n",
            "rtowe\n",
            "egwarton34\n",
            "IAMINOLLA\n",
            "ferqu\n",
            "--------------------------------------------------------------------------------\n",
            "step 793410 | loss 2.3218 | step time 37.34ms\n",
            "step 793420 | loss 2.5440 | step time 44.41ms\n",
            "step 793430 | loss 2.5475 | step time 37.73ms\n",
            "step 793440 | loss 2.2592 | step time 60.57ms\n",
            "step 793450 | loss 2.4136 | step time 56.21ms\n",
            "step 793460 | loss 2.3057 | step time 59.29ms\n",
            "step 793470 | loss 2.6988 | step time 55.52ms\n",
            "step 793480 | loss 2.4746 | step time 58.75ms\n",
            "step 793490 | loss 2.3036 | step time 54.05ms\n",
            "step 793500 | loss 2.2684 | step time 64.76ms\n",
            "step 793500 train loss: 2.4536795616149902 test loss: 2.486358642578125\n",
            "step 793510 | loss 2.4103 | step time 58.03ms\n",
            "step 793520 | loss 2.3930 | step time 38.13ms\n",
            "step 793530 | loss 2.3888 | step time 38.40ms\n",
            "step 793540 | loss 2.2984 | step time 40.10ms\n",
            "step 793550 | loss 2.6305 | step time 37.80ms\n",
            "step 793560 | loss 2.3929 | step time 36.30ms\n",
            "step 793570 | loss 2.4429 | step time 37.69ms\n",
            "step 793580 | loss 2.6212 | step time 41.15ms\n",
            "step 793590 | loss 2.6329 | step time 40.80ms\n",
            "step 793600 | loss 2.3688 | step time 39.44ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "lol3ts6666\n",
            "DCthrowaway\n",
            "sprin14\n",
            "easnex\n",
            "InformPanta\n",
            "AlexBrazima\n",
            "landy_moo\n",
            "wisesexar\n",
            "tvbangle\n",
            "Reddit410\n",
            "--------------------------------------------------------------------------------\n",
            "step 793610 | loss 2.4991 | step time 37.06ms\n",
            "step 793620 | loss 2.4866 | step time 38.48ms\n",
            "step 793630 | loss 2.3954 | step time 49.58ms\n",
            "step 793640 | loss 2.5625 | step time 35.59ms\n",
            "step 793650 | loss 2.4392 | step time 37.77ms\n",
            "step 793660 | loss 2.3660 | step time 35.97ms\n",
            "step 793670 | loss 2.3068 | step time 38.85ms\n",
            "step 793680 | loss 2.5830 | step time 42.58ms\n",
            "step 793690 | loss 2.5307 | step time 37.64ms\n",
            "step 793700 | loss 2.5937 | step time 39.53ms\n",
            "step 793710 | loss 2.5076 | step time 36.71ms\n",
            "step 793720 | loss 2.5881 | step time 40.15ms\n",
            "step 793730 | loss 2.2139 | step time 43.10ms\n",
            "step 793740 | loss 2.4817 | step time 39.18ms\n",
            "step 793750 | loss 2.4343 | step time 57.37ms\n",
            "step 793760 | loss 2.6012 | step time 52.96ms\n",
            "step 793770 | loss 2.5187 | step time 57.22ms\n",
            "step 793780 | loss 2.3266 | step time 57.83ms\n",
            "step 793790 | loss 2.3472 | step time 58.62ms\n",
            "step 793800 | loss 2.3827 | step time 57.77ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Keinzan\n",
            "Criticstudent03\n",
            "throwawaycalvention\n",
            "lilncx\n",
            "sicklemeredpill\n",
            "fulshoprofessort\n",
            "ThowLegBitzZones\n",
            "Wrkicker\n",
            "zmoneyw\n",
            "Thebbg12\n",
            "--------------------------------------------------------------------------------\n",
            "step 793810 | loss 2.4700 | step time 56.45ms\n",
            "step 793820 | loss 2.6046 | step time 40.36ms\n",
            "step 793830 | loss 2.4154 | step time 36.81ms\n",
            "step 793840 | loss 2.6118 | step time 36.31ms\n",
            "step 793850 | loss 2.5180 | step time 40.20ms\n",
            "step 793860 | loss 2.5023 | step time 36.34ms\n",
            "step 793870 | loss 2.4771 | step time 35.47ms\n",
            "step 793880 | loss 2.5616 | step time 36.53ms\n",
            "step 793890 | loss 2.2848 | step time 40.37ms\n",
            "step 793900 | loss 2.3030 | step time 36.97ms\n",
            "step 793910 | loss 2.3752 | step time 36.76ms\n",
            "step 793920 | loss 2.4198 | step time 47.35ms\n",
            "step 793930 | loss 2.4571 | step time 42.03ms\n",
            "step 793940 | loss 2.5907 | step time 37.25ms\n",
            "step 793950 | loss 2.4255 | step time 38.47ms\n",
            "step 793960 | loss 2.4632 | step time 35.98ms\n",
            "step 793970 | loss 2.5947 | step time 39.11ms\n",
            "step 793980 | loss 2.2488 | step time 38.93ms\n",
            "step 793990 | loss 2.4959 | step time 35.10ms\n",
            "step 794000 | loss 2.5592 | step time 39.65ms\n",
            "step 794000 train loss: 2.465346574783325 test loss: 2.48213267326355\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Grok-I-M\n",
            "Flefle_prof_starus\n",
            "poze\n",
            "cookiecow133\n",
            "HitLikeBurrito\n",
            "nohiWA_r\n",
            "mattek28y\n",
            "Mikeshynow\n",
            "osaybitgranakiro\n",
            "4byte8u8e390\n",
            "--------------------------------------------------------------------------------\n",
            "step 794010 | loss 2.5645 | step time 37.33ms\n",
            "step 794020 | loss 2.4018 | step time 36.83ms\n",
            "step 794030 | loss 2.5542 | step time 35.81ms\n",
            "step 794040 | loss 2.2693 | step time 51.56ms\n",
            "step 794050 | loss 2.4377 | step time 50.56ms\n",
            "step 794060 | loss 2.4268 | step time 51.78ms\n",
            "step 794070 | loss 2.4385 | step time 49.99ms\n",
            "step 794080 | loss 2.4553 | step time 59.66ms\n",
            "step 794090 | loss 2.5463 | step time 60.93ms\n",
            "step 794100 | loss 2.5231 | step time 70.17ms\n",
            "step 794110 | loss 2.3574 | step time 54.45ms\n",
            "step 794120 | loss 2.3627 | step time 53.80ms\n",
            "step 794130 | loss 2.5045 | step time 35.77ms\n",
            "step 794140 | loss 2.5854 | step time 39.70ms\n",
            "step 794150 | loss 2.3527 | step time 38.85ms\n",
            "step 794160 | loss 2.3900 | step time 36.35ms\n",
            "step 794170 | loss 2.4524 | step time 36.79ms\n",
            "step 794180 | loss 2.5092 | step time 38.14ms\n",
            "step 794190 | loss 2.4988 | step time 40.02ms\n",
            "step 794200 | loss 2.4520 | step time 38.95ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "rolikimen\n",
            "PlayHob\n",
            "ViniasAhiller1795\n",
            "advaguejefferneds\n",
            "woat_here28\n",
            "WhoDopp\n",
            "Pattoandowlove\n",
            "tommyorstem\n",
            "derby__garrappension\n",
            "MerlyyYututa\n",
            "--------------------------------------------------------------------------------\n",
            "step 794210 | loss 2.4797 | step time 36.96ms\n",
            "step 794220 | loss 2.2345 | step time 36.85ms\n",
            "step 794230 | loss 2.5096 | step time 45.95ms\n",
            "step 794240 | loss 2.4394 | step time 37.69ms\n",
            "step 794250 | loss 2.6017 | step time 36.98ms\n",
            "step 794260 | loss 2.2920 | step time 36.43ms\n",
            "step 794270 | loss 2.7343 | step time 38.53ms\n",
            "step 794280 | loss 2.4256 | step time 35.97ms\n",
            "step 794290 | loss 2.3308 | step time 40.47ms\n",
            "step 794300 | loss 2.3768 | step time 38.91ms\n",
            "step 794310 | loss 2.3555 | step time 35.34ms\n",
            "step 794320 | loss 2.2975 | step time 37.86ms\n",
            "step 794330 | loss 2.4160 | step time 38.29ms\n",
            "step 794340 | loss 2.2839 | step time 38.03ms\n",
            "step 794350 | loss 2.4544 | step time 36.18ms\n",
            "step 794360 | loss 2.4719 | step time 38.07ms\n",
            "step 794370 | loss 2.4734 | step time 58.58ms\n",
            "step 794380 | loss 2.5029 | step time 49.51ms\n",
            "step 794390 | loss 2.4327 | step time 51.53ms\n",
            "step 794400 | loss 2.5231 | step time 53.21ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Nicefish92\n",
            "claria\n",
            "Chizota\n",
            "caveheyq\n",
            "nevfaqp5OK\n",
            "Neoo\n",
            "N3RFPodaroo\n",
            "Spumas_82\n",
            "Isairtes\n",
            "eky_coronic\n",
            "--------------------------------------------------------------------------------\n",
            "step 794410 | loss 2.6061 | step time 55.31ms\n",
            "step 794420 | loss 2.6230 | step time 56.02ms\n",
            "step 794430 | loss 2.3309 | step time 55.01ms\n",
            "step 794440 | loss 2.3492 | step time 39.34ms\n",
            "step 794450 | loss 2.2598 | step time 38.30ms\n",
            "step 794460 | loss 2.4097 | step time 37.26ms\n",
            "step 794470 | loss 2.4922 | step time 40.52ms\n",
            "step 794480 | loss 2.5982 | step time 40.67ms\n",
            "step 794490 | loss 2.4240 | step time 34.69ms\n",
            "step 794500 | loss 2.6274 | step time 38.08ms\n",
            "step 794500 train loss: 2.440441846847534 test loss: 2.4836976528167725\n",
            "step 794510 | loss 2.2565 | step time 35.40ms\n",
            "step 794520 | loss 2.4918 | step time 35.37ms\n",
            "step 794530 | loss 2.5655 | step time 48.85ms\n",
            "step 794540 | loss 2.4130 | step time 37.55ms\n",
            "step 794550 | loss 2.4400 | step time 37.85ms\n",
            "step 794560 | loss 2.4988 | step time 36.90ms\n",
            "step 794570 | loss 2.6290 | step time 36.73ms\n",
            "step 794580 | loss 2.4186 | step time 39.76ms\n",
            "step 794590 | loss 2.5948 | step time 37.36ms\n",
            "step 794600 | loss 2.6287 | step time 38.06ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Typheist8\n",
            "singh4ever\n",
            "FunctioNyou32\n",
            "maxpunthehigh\n",
            "tposteslikethrowaway\n",
            "jahonteea\n",
            "Bass_Roaly\n",
            "DhinoFallah\n",
            "R13muramys\n",
            "mushr825\n",
            "--------------------------------------------------------------------------------\n",
            "step 794610 | loss 2.5328 | step time 35.52ms\n",
            "step 794620 | loss 2.3572 | step time 37.21ms\n",
            "step 794630 | loss 2.6750 | step time 35.95ms\n",
            "step 794640 | loss 2.4513 | step time 37.74ms\n",
            "step 794650 | loss 2.3498 | step time 39.69ms\n",
            "step 794660 | loss 2.5689 | step time 46.02ms\n",
            "step 794670 | loss 2.4047 | step time 55.29ms\n",
            "step 794680 | loss 2.3988 | step time 50.95ms\n",
            "step 794690 | loss 2.6634 | step time 64.24ms\n",
            "step 794700 | loss 2.6003 | step time 53.94ms\n",
            "step 794710 | loss 2.4145 | step time 58.11ms\n",
            "step 794720 | loss 2.3440 | step time 57.80ms\n",
            "step 794730 | loss 2.4577 | step time 55.21ms\n",
            "step 794740 | loss 2.2360 | step time 64.69ms\n",
            "step 794750 | loss 2.4772 | step time 39.46ms\n",
            "step 794760 | loss 2.4381 | step time 41.49ms\n",
            "step 794770 | loss 2.4036 | step time 38.37ms\n",
            "step 794780 | loss 2.5100 | step time 43.96ms\n",
            "step 794790 | loss 2.4733 | step time 39.24ms\n",
            "step 794800 | loss 2.3429 | step time 36.75ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "mangodstout\n",
            "nMidney1973\n",
            "Destastrobatos\n",
            "IJustMyLosts\n",
            "monieanis\n",
            "lallini\n",
            "ClinkingThrowaway420\n",
            "citishemplib\n",
            "malvinhxr\n",
            "legro_g\n",
            "--------------------------------------------------------------------------------\n",
            "step 794810 | loss 2.3103 | step time 37.84ms\n",
            "step 794820 | loss 2.6637 | step time 36.74ms\n",
            "step 794830 | loss 2.3621 | step time 48.61ms\n",
            "step 794840 | loss 2.4242 | step time 38.74ms\n",
            "step 794850 | loss 2.3183 | step time 37.96ms\n",
            "step 794860 | loss 2.7184 | step time 35.34ms\n",
            "step 794870 | loss 2.5231 | step time 38.53ms\n",
            "step 794880 | loss 2.3920 | step time 37.42ms\n",
            "step 794890 | loss 2.4334 | step time 38.49ms\n",
            "step 794900 | loss 2.5797 | step time 37.83ms\n",
            "step 794910 | loss 2.5875 | step time 37.38ms\n",
            "step 794920 | loss 2.5882 | step time 36.54ms\n",
            "step 794930 | loss 2.2797 | step time 38.89ms\n",
            "step 794940 | loss 2.5703 | step time 39.08ms\n",
            "step 794950 | loss 2.6529 | step time 43.04ms\n",
            "step 794960 | loss 2.5155 | step time 45.47ms\n",
            "step 794970 | loss 2.4879 | step time 37.63ms\n",
            "step 794980 | loss 2.5383 | step time 40.94ms\n",
            "step 794990 | loss 2.3490 | step time 52.12ms\n",
            "step 795000 | loss 2.3033 | step time 51.24ms\n",
            "step 795000 train loss: 2.445035696029663 test loss: 2.486010789871216\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ultendpotato\n",
            "LowdiveDice\n",
            "goem25909\n",
            "thoutme1966\n",
            "mytalkingflam\n",
            "msochiluc\n",
            "Smelly_Wont\n",
            "Brokchagga\n",
            "Gal_heathers\n",
            "adenceb\n",
            "--------------------------------------------------------------------------------\n",
            "step 795010 | loss 2.3748 | step time 56.98ms\n",
            "step 795020 | loss 2.5842 | step time 56.42ms\n",
            "step 795030 | loss 2.5494 | step time 50.60ms\n",
            "step 795040 | loss 2.5274 | step time 35.68ms\n",
            "step 795050 | loss 2.4537 | step time 36.61ms\n",
            "step 795060 | loss 2.3705 | step time 41.03ms\n",
            "step 795070 | loss 2.4381 | step time 38.92ms\n",
            "step 795080 | loss 2.3181 | step time 36.51ms\n",
            "step 795090 | loss 2.5519 | step time 39.49ms\n",
            "step 795100 | loss 2.3289 | step time 36.02ms\n",
            "step 795110 | loss 2.5186 | step time 36.97ms\n",
            "step 795120 | loss 2.3271 | step time 40.60ms\n",
            "step 795130 | loss 2.4846 | step time 38.95ms\n",
            "step 795140 | loss 2.4270 | step time 38.02ms\n",
            "step 795150 | loss 2.4743 | step time 38.42ms\n",
            "step 795160 | loss 2.6032 | step time 37.12ms\n",
            "step 795170 | loss 2.4391 | step time 36.75ms\n",
            "step 795180 | loss 2.3039 | step time 48.75ms\n",
            "step 795190 | loss 2.4084 | step time 37.68ms\n",
            "step 795200 | loss 2.1567 | step time 37.10ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "UberSmarie\n",
            "SheEnelyFoxFyrust\n",
            "higgirlara\n",
            "nickfury\n",
            "nobodyraccount22757\n",
            "TheRiceGoodman\n",
            "JimDucksMindga\n",
            "findforaskycaling\n",
            "panklar\n",
            "Zeebyfee\n",
            "--------------------------------------------------------------------------------\n",
            "step 795210 | loss 2.3883 | step time 45.11ms\n",
            "step 795220 | loss 2.3268 | step time 36.67ms\n",
            "step 795230 | loss 2.3083 | step time 37.18ms\n",
            "step 795240 | loss 2.3474 | step time 37.97ms\n",
            "step 795250 | loss 2.3906 | step time 37.99ms\n",
            "step 795260 | loss 2.4763 | step time 39.14ms\n",
            "step 795270 | loss 2.4834 | step time 56.61ms\n",
            "step 795280 | loss 2.4350 | step time 54.08ms\n",
            "step 795290 | loss 2.5007 | step time 51.97ms\n",
            "step 795300 | loss 2.2316 | step time 53.30ms\n",
            "step 795310 | loss 2.6506 | step time 54.61ms\n",
            "step 795320 | loss 2.5828 | step time 62.38ms\n",
            "step 795330 | loss 2.3994 | step time 62.32ms\n",
            "step 795340 | loss 2.4217 | step time 57.47ms\n",
            "step 795350 | loss 2.4676 | step time 57.54ms\n",
            "step 795360 | loss 2.3693 | step time 38.09ms\n",
            "step 795370 | loss 2.5888 | step time 36.86ms\n",
            "step 795380 | loss 2.3641 | step time 37.31ms\n",
            "step 795390 | loss 2.6114 | step time 36.33ms\n",
            "step 795400 | loss 2.4255 | step time 40.54ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "MrScamW91\n",
            "AMA-vorgin_the---Abu\n",
            "vasy___\n",
            "LiceLover11\n",
            "Ahugastue\n",
            "NivessedCase\n",
            "BucksAfan\n",
            "SWUVBEATS\n",
            "seraaa\n",
            "mbobsz\n",
            "--------------------------------------------------------------------------------\n",
            "step 795410 | loss 2.4237 | step time 41.27ms\n",
            "step 795420 | loss 2.5680 | step time 36.43ms\n",
            "step 795430 | loss 2.4208 | step time 38.34ms\n",
            "step 795440 | loss 2.4722 | step time 39.80ms\n",
            "step 795450 | loss 2.4094 | step time 36.90ms\n",
            "step 795460 | loss 2.5627 | step time 37.11ms\n",
            "step 795470 | loss 2.4096 | step time 38.13ms\n",
            "step 795480 | loss 2.6471 | step time 37.59ms\n",
            "step 795490 | loss 2.4314 | step time 35.65ms\n",
            "step 795500 | loss 2.4696 | step time 46.65ms\n",
            "step 795500 train loss: 2.4650635719299316 test loss: 2.4865174293518066\n",
            "step 795510 | loss 2.3145 | step time 38.92ms\n",
            "step 795520 | loss 2.5614 | step time 37.97ms\n",
            "step 795530 | loss 2.4127 | step time 37.20ms\n",
            "step 795540 | loss 2.4875 | step time 38.02ms\n",
            "step 795550 | loss 2.4683 | step time 38.87ms\n",
            "step 795560 | loss 2.7101 | step time 51.42ms\n",
            "step 795570 | loss 2.4057 | step time 40.14ms\n",
            "step 795580 | loss 2.3929 | step time 56.89ms\n",
            "step 795590 | loss 2.3176 | step time 53.93ms\n",
            "step 795600 | loss 2.5375 | step time 60.75ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "hallonbootazro\n",
            "wallabar0\n",
            "miyagia\n",
            "NJamtopated\n",
            "Great5891\n",
            "FairBroS95\n",
            "BASKINBATOAS\n",
            "daimon1st\n",
            "biloppear15\n",
            "trobertknwm\n",
            "--------------------------------------------------------------------------------\n",
            "step 795610 | loss 2.4362 | step time 63.38ms\n",
            "step 795620 | loss 2.4430 | step time 69.04ms\n",
            "step 795630 | loss 2.5675 | step time 60.78ms\n",
            "step 795640 | loss 2.3838 | step time 37.97ms\n",
            "step 795650 | loss 2.3783 | step time 37.48ms\n",
            "step 795660 | loss 2.5807 | step time 37.60ms\n",
            "step 795670 | loss 2.3052 | step time 36.19ms\n",
            "step 795680 | loss 2.4754 | step time 38.72ms\n",
            "step 795690 | loss 2.4575 | step time 42.68ms\n",
            "step 795700 | loss 2.4270 | step time 36.42ms\n",
            "step 795710 | loss 2.3306 | step time 34.94ms\n",
            "step 795720 | loss 2.5744 | step time 36.31ms\n",
            "step 795730 | loss 2.4106 | step time 36.56ms\n",
            "step 795740 | loss 2.3326 | step time 36.64ms\n",
            "step 795750 | loss 2.2061 | step time 37.16ms\n",
            "step 795760 | loss 2.3913 | step time 37.93ms\n",
            "step 795770 | loss 2.4491 | step time 37.92ms\n",
            "step 795780 | loss 2.4056 | step time 37.93ms\n",
            "step 795790 | loss 2.5243 | step time 40.91ms\n",
            "step 795800 | loss 2.4606 | step time 39.66ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "iisnysteakscommunisu\n",
            "Sig_Very_\n",
            "Iceqgabefair\n",
            "lividonu\n",
            "motasmagistan\n",
            "RAd3345\n",
            "larvtha\n",
            "jimurai\n",
            "umlane\n",
            "0rcezpanda\n",
            "--------------------------------------------------------------------------------\n",
            "step 795810 | loss 2.4024 | step time 38.13ms\n",
            "step 795820 | loss 2.4016 | step time 41.08ms\n",
            "step 795830 | loss 2.6350 | step time 35.08ms\n",
            "step 795840 | loss 2.3440 | step time 37.22ms\n",
            "step 795850 | loss 2.5095 | step time 37.39ms\n",
            "step 795860 | loss 2.3167 | step time 45.82ms\n",
            "step 795870 | loss 2.4182 | step time 35.51ms\n",
            "step 795880 | loss 2.2668 | step time 55.00ms\n",
            "step 795890 | loss 2.5290 | step time 49.83ms\n",
            "step 795900 | loss 2.3694 | step time 52.57ms\n",
            "step 795910 | loss 2.5113 | step time 57.63ms\n",
            "step 795920 | loss 2.3932 | step time 53.74ms\n",
            "step 795930 | loss 2.3794 | step time 56.78ms\n",
            "step 795940 | loss 2.6215 | step time 54.28ms\n",
            "step 795950 | loss 2.3670 | step time 56.18ms\n",
            "step 795960 | loss 2.3292 | step time 59.27ms\n",
            "step 795970 | loss 2.5269 | step time 55.38ms\n",
            "step 795980 | loss 2.3474 | step time 55.58ms\n",
            "step 795990 | loss 2.5106 | step time 39.40ms\n",
            "step 796000 | loss 2.4168 | step time 50.15ms\n",
            "step 796000 train loss: 2.4751203060150146 test loss: 2.490032196044922\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "iwkniffles\n",
            "tb3at\n",
            "invigillyceL\n",
            "proditius\n",
            "Tatann\n",
            "hey1imkiveananaplaaz\n",
            "Neu_Excelest\n",
            "gengerexagare\n",
            "AltianVII\n",
            "Jamethingsgt\n",
            "--------------------------------------------------------------------------------\n",
            "step 796010 | loss 2.4634 | step time 38.84ms\n",
            "step 796020 | loss 2.5424 | step time 38.01ms\n",
            "step 796030 | loss 2.6333 | step time 38.66ms\n",
            "step 796040 | loss 2.4536 | step time 39.53ms\n",
            "step 796050 | loss 2.5170 | step time 37.76ms\n",
            "step 796060 | loss 2.3945 | step time 35.73ms\n",
            "step 796070 | loss 2.5527 | step time 45.37ms\n",
            "step 796080 | loss 2.3531 | step time 38.49ms\n",
            "step 796090 | loss 2.4831 | step time 39.44ms\n",
            "step 796100 | loss 2.4198 | step time 40.72ms\n",
            "step 796110 | loss 2.1351 | step time 39.61ms\n",
            "step 796120 | loss 2.4754 | step time 40.00ms\n",
            "step 796130 | loss 2.3438 | step time 37.86ms\n",
            "step 796140 | loss 2.5204 | step time 42.48ms\n",
            "step 796150 | loss 2.4393 | step time 42.62ms\n",
            "step 796160 | loss 2.6128 | step time 37.28ms\n",
            "step 796170 | loss 2.3377 | step time 38.68ms\n",
            "step 796180 | loss 2.5544 | step time 36.01ms\n",
            "step 796190 | loss 2.5422 | step time 37.39ms\n",
            "step 796200 | loss 2.6049 | step time 50.57ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "twear012\n",
            "Arcelabaron\n",
            "chrisr0120-cheese\n",
            "IAHARDYGirlBroset\n",
            "rmerid\n",
            "ThatsPorseck\n",
            "MeIshaud\n",
            "rodplushix2\n",
            "ninedoy\n",
            "Alb_Dcunnar\n",
            "--------------------------------------------------------------------------------\n",
            "step 796210 | loss 2.4307 | step time 55.03ms\n",
            "step 796220 | loss 2.3587 | step time 56.39ms\n",
            "step 796230 | loss 2.4879 | step time 59.61ms\n",
            "step 796240 | loss 2.5841 | step time 57.65ms\n",
            "step 796250 | loss 2.5131 | step time 60.42ms\n",
            "step 796260 | loss 2.4742 | step time 54.88ms\n",
            "step 796270 | loss 2.4358 | step time 37.03ms\n",
            "step 796280 | loss 2.6117 | step time 36.38ms\n",
            "step 796290 | loss 2.2558 | step time 38.17ms\n",
            "step 796300 | loss 2.4440 | step time 38.00ms\n",
            "step 796310 | loss 2.4378 | step time 36.74ms\n",
            "step 796320 | loss 2.4359 | step time 43.08ms\n",
            "step 796330 | loss 2.4598 | step time 41.62ms\n",
            "step 796340 | loss 2.5547 | step time 41.84ms\n",
            "step 796350 | loss 2.5601 | step time 39.11ms\n",
            "step 796360 | loss 2.6758 | step time 37.39ms\n",
            "step 796370 | loss 2.3903 | step time 37.74ms\n",
            "step 796380 | loss 2.4629 | step time 36.39ms\n",
            "step 796390 | loss 2.6604 | step time 40.09ms\n",
            "step 796400 | loss 2.3599 | step time 36.65ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "theherahb\n",
            "sfaoi\n",
            "Fakechap92\n",
            "ParShitMcBakschle\n",
            "mxask\n",
            "thistormomentstein\n",
            "Solaru\n",
            "CryderSnapp\n",
            "Grillindalan\n",
            "spasndelates\n",
            "--------------------------------------------------------------------------------\n",
            "step 796410 | loss 2.5134 | step time 40.46ms\n",
            "step 796420 | loss 2.5065 | step time 37.27ms\n",
            "step 796430 | loss 2.5025 | step time 38.04ms\n",
            "step 796440 | loss 2.3627 | step time 39.34ms\n",
            "step 796450 | loss 2.4158 | step time 41.91ms\n",
            "step 796460 | loss 2.4878 | step time 37.78ms\n",
            "step 796470 | loss 2.3432 | step time 53.93ms\n",
            "step 796480 | loss 2.4070 | step time 40.52ms\n",
            "step 796490 | loss 2.4783 | step time 38.29ms\n",
            "step 796500 | loss 2.4087 | step time 42.03ms\n",
            "step 796500 train loss: 2.4398088455200195 test loss: 2.484234094619751\n",
            "step 796510 | loss 2.3810 | step time 59.26ms\n",
            "step 796520 | loss 2.2630 | step time 55.42ms\n",
            "step 796530 | loss 2.4111 | step time 59.01ms\n",
            "step 796540 | loss 2.4636 | step time 57.45ms\n",
            "step 796550 | loss 2.2436 | step time 60.83ms\n",
            "step 796560 | loss 2.5863 | step time 56.87ms\n",
            "step 796570 | loss 2.3981 | step time 61.93ms\n",
            "step 796580 | loss 2.4728 | step time 35.31ms\n",
            "step 796590 | loss 2.5226 | step time 38.84ms\n",
            "step 796600 | loss 2.6327 | step time 36.82ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Sweet_Super_hippo\n",
            "superpetrip\n",
            "mubbaansjt\n",
            "quakely_your_eddinle\n",
            "Al_Best_FilSevery\n",
            "gvrpylw\n",
            "barnvener12\n",
            "cintra1\n",
            "mafiandnes\n",
            "BarraWick\n",
            "--------------------------------------------------------------------------------\n",
            "step 796610 | loss 2.3848 | step time 38.06ms\n",
            "step 796620 | loss 2.3616 | step time 41.11ms\n",
            "step 796630 | loss 2.4417 | step time 38.84ms\n",
            "step 796640 | loss 2.4224 | step time 38.65ms\n",
            "step 796650 | loss 2.5939 | step time 42.51ms\n",
            "step 796660 | loss 2.4657 | step time 40.31ms\n",
            "step 796670 | loss 2.6137 | step time 40.68ms\n",
            "step 796680 | loss 2.5400 | step time 41.62ms\n",
            "step 796690 | loss 2.4739 | step time 38.05ms\n",
            "step 796700 | loss 2.3329 | step time 41.04ms\n",
            "step 796710 | loss 2.6892 | step time 37.86ms\n",
            "step 796720 | loss 2.3754 | step time 38.23ms\n",
            "step 796730 | loss 2.5056 | step time 42.41ms\n",
            "step 796740 | loss 2.5492 | step time 39.15ms\n",
            "step 796750 | loss 2.4641 | step time 39.58ms\n",
            "step 796760 | loss 2.4502 | step time 37.43ms\n",
            "step 796770 | loss 2.4479 | step time 39.03ms\n",
            "step 796780 | loss 2.4676 | step time 38.95ms\n",
            "step 796790 | loss 2.4178 | step time 38.68ms\n",
            "step 796800 | loss 2.5518 | step time 39.03ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "merta_red_lauren\n",
            "jamk03pr33\n",
            "pengus_panthopeler\n",
            "Jhkaug\n",
            "ZeroLil\n",
            "spettcrows\n",
            "sekelbookdalily\n",
            "tedtech88\n",
            "wfrnm\n",
            "_______________\n",
            "--------------------------------------------------------------------------------\n",
            "step 796810 | loss 2.2439 | step time 54.18ms\n",
            "step 796820 | loss 2.6196 | step time 65.44ms\n",
            "step 796830 | loss 2.3460 | step time 54.18ms\n",
            "step 796840 | loss 2.7928 | step time 58.94ms\n",
            "step 796850 | loss 2.4636 | step time 56.76ms\n",
            "step 796860 | loss 2.3864 | step time 61.21ms\n",
            "step 796870 | loss 2.5084 | step time 60.70ms\n",
            "step 796880 | loss 2.5459 | step time 52.73ms\n",
            "step 796890 | loss 2.5302 | step time 50.77ms\n",
            "step 796900 | loss 2.3141 | step time 38.51ms\n",
            "step 796910 | loss 2.3434 | step time 39.10ms\n",
            "step 796920 | loss 2.2150 | step time 39.04ms\n",
            "step 796930 | loss 2.5241 | step time 39.42ms\n",
            "step 796940 | loss 2.6670 | step time 39.20ms\n",
            "step 796950 | loss 2.4130 | step time 35.91ms\n",
            "step 796960 | loss 2.4325 | step time 41.63ms\n",
            "step 796970 | loss 2.6333 | step time 39.11ms\n",
            "step 796980 | loss 2.5115 | step time 41.43ms\n",
            "step 796990 | loss 2.3138 | step time 39.75ms\n",
            "step 797000 | loss 2.3876 | step time 37.22ms\n",
            "step 797000 train loss: 2.4260385036468506 test loss: 2.486680269241333\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "dynamit1008\n",
            "Aquejoe\n",
            "lynxos\n",
            "ghhone\n",
            "jo1yak3\n",
            "serna_peats\n",
            "keithecooter\n",
            "hitdosjobson544\n",
            "BeadousRed76\n",
            "atanatickeevi\n",
            "--------------------------------------------------------------------------------\n",
            "step 797010 | loss 2.3624 | step time 36.74ms\n",
            "step 797020 | loss 2.5870 | step time 38.08ms\n",
            "step 797030 | loss 2.2646 | step time 37.87ms\n",
            "step 797040 | loss 2.3338 | step time 44.27ms\n",
            "step 797050 | loss 2.2907 | step time 37.08ms\n",
            "step 797060 | loss 2.5068 | step time 40.98ms\n",
            "step 797070 | loss 2.3984 | step time 39.71ms\n",
            "step 797080 | loss 2.5372 | step time 42.65ms\n",
            "step 797090 | loss 2.3945 | step time 36.71ms\n",
            "step 797100 | loss 2.5042 | step time 58.88ms\n",
            "step 797110 | loss 2.5459 | step time 55.40ms\n",
            "step 797120 | loss 2.5064 | step time 65.64ms\n",
            "step 797130 | loss 2.5044 | step time 54.73ms\n",
            "step 797140 | loss 2.3489 | step time 58.79ms\n",
            "step 797150 | loss 2.5535 | step time 57.27ms\n",
            "step 797160 | loss 2.3725 | step time 57.67ms\n",
            "step 797170 | loss 2.3493 | step time 63.98ms\n",
            "step 797180 | loss 2.4851 | step time 57.66ms\n",
            "step 797190 | loss 2.3910 | step time 41.70ms\n",
            "step 797200 | loss 2.5137 | step time 38.00ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "edavidpin\n",
            "SecretThrowawayOne\n",
            "spven_clanny\n",
            "Turbostang\n",
            "Berthecaker9993\n",
            "mikMass3\n",
            "JonnyGodfara\n",
            "slap_mccarnoob\n",
            "packerson\n",
            "BigMakesNoFox\n",
            "--------------------------------------------------------------------------------\n",
            "step 797210 | loss 2.2096 | step time 36.43ms\n",
            "step 797220 | loss 2.4601 | step time 38.08ms\n",
            "step 797230 | loss 2.3679 | step time 35.43ms\n",
            "step 797240 | loss 2.5183 | step time 38.43ms\n",
            "step 797250 | loss 2.4658 | step time 38.38ms\n",
            "step 797260 | loss 2.4414 | step time 48.22ms\n",
            "step 797270 | loss 2.4233 | step time 37.00ms\n",
            "step 797280 | loss 2.4764 | step time 36.80ms\n",
            "step 797290 | loss 2.5600 | step time 35.63ms\n",
            "step 797300 | loss 2.5969 | step time 35.70ms\n",
            "step 797310 | loss 2.5421 | step time 38.07ms\n",
            "step 797320 | loss 2.5931 | step time 39.32ms\n",
            "step 797330 | loss 2.5669 | step time 42.86ms\n",
            "step 797340 | loss 2.3269 | step time 36.00ms\n",
            "step 797350 | loss 2.4498 | step time 38.11ms\n",
            "step 797360 | loss 2.4071 | step time 35.41ms\n",
            "step 797370 | loss 2.4551 | step time 37.60ms\n",
            "step 797380 | loss 2.2897 | step time 36.77ms\n",
            "step 797390 | loss 2.4612 | step time 38.35ms\n",
            "step 797400 | loss 2.5765 | step time 36.79ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "awayj\n",
            "Kramero\n",
            "Frood348\n",
            "travan3u-kitty-chan\n",
            "scythrowaway712112\n",
            "EMf_b1X\n",
            "abyreerings\n",
            "hellothenimalboss\n",
            "unicornpoly232r\n",
            "mah_l\n",
            "--------------------------------------------------------------------------------\n",
            "step 797410 | loss 2.3423 | step time 41.59ms\n",
            "step 797420 | loss 2.3434 | step time 59.07ms\n",
            "step 797430 | loss 2.4475 | step time 55.21ms\n",
            "step 797440 | loss 2.4682 | step time 56.42ms\n",
            "step 797450 | loss 2.4262 | step time 61.66ms\n",
            "step 797460 | loss 2.3431 | step time 54.83ms\n",
            "step 797470 | loss 2.3631 | step time 56.50ms\n",
            "step 797480 | loss 2.3470 | step time 55.23ms\n",
            "step 797490 | loss 2.3678 | step time 56.55ms\n",
            "step 797500 | loss 2.3522 | step time 65.64ms\n",
            "step 797500 train loss: 2.44533634185791 test loss: 2.4856619834899902\n",
            "step 797510 | loss 2.3819 | step time 43.19ms\n",
            "step 797520 | loss 2.3105 | step time 36.46ms\n",
            "step 797530 | loss 2.4978 | step time 36.81ms\n",
            "step 797540 | loss 2.5048 | step time 38.77ms\n",
            "step 797550 | loss 2.4638 | step time 36.07ms\n",
            "step 797560 | loss 2.4071 | step time 36.88ms\n",
            "step 797570 | loss 2.4469 | step time 37.74ms\n",
            "step 797580 | loss 2.5183 | step time 37.72ms\n",
            "step 797590 | loss 2.2955 | step time 40.24ms\n",
            "step 797600 | loss 2.6429 | step time 37.28ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Tomatrog\n",
            "THE_RMAZ3V22\n",
            "NonpotEwidow\n",
            "CoprekBichGuy111\n",
            "stephsotz3\n",
            "n8ct5iu1u7uj\n",
            "annoentoo\n",
            "Glouvwigantod\n",
            "flappyfree\n",
            "NamezFace\n",
            "--------------------------------------------------------------------------------\n",
            "step 797610 | loss 2.5503 | step time 35.60ms\n",
            "step 797620 | loss 2.4220 | step time 36.50ms\n",
            "step 797630 | loss 2.4663 | step time 37.32ms\n",
            "step 797640 | loss 2.3697 | step time 37.07ms\n",
            "step 797650 | loss 2.3520 | step time 37.69ms\n",
            "step 797660 | loss 2.4372 | step time 39.86ms\n",
            "step 797670 | loss 2.5083 | step time 39.95ms\n",
            "step 797680 | loss 2.4742 | step time 38.80ms\n",
            "step 797690 | loss 2.5894 | step time 42.37ms\n",
            "step 797700 | loss 2.6386 | step time 37.68ms\n",
            "step 797710 | loss 2.4776 | step time 37.95ms\n",
            "step 797720 | loss 2.4361 | step time 37.88ms\n",
            "step 797730 | loss 2.3748 | step time 63.53ms\n",
            "step 797740 | loss 2.6003 | step time 51.33ms\n",
            "step 797750 | loss 2.4759 | step time 50.55ms\n",
            "step 797760 | loss 2.3332 | step time 66.21ms\n",
            "step 797770 | loss 2.3696 | step time 54.32ms\n",
            "step 797780 | loss 2.5776 | step time 53.63ms\n",
            "step 797790 | loss 2.4744 | step time 64.09ms\n",
            "step 797800 | loss 2.3828 | step time 58.27ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Innovemanmanfourth\n",
            "lioweclarewencon\n",
            "lilseallyou68\n",
            "moviemsball\n",
            "Lavcadv\n",
            "deandyfercycliber\n",
            "kortenlyfreak\n",
            "themayway42\n",
            "RagnTrouble\n",
            "esujm\n",
            "--------------------------------------------------------------------------------\n",
            "step 797810 | loss 2.3951 | step time 39.42ms\n",
            "step 797820 | loss 2.3213 | step time 35.90ms\n",
            "step 797830 | loss 2.4453 | step time 47.03ms\n",
            "step 797840 | loss 2.3946 | step time 37.98ms\n",
            "step 797850 | loss 2.4550 | step time 39.16ms\n",
            "step 797860 | loss 2.4303 | step time 38.33ms\n",
            "step 797870 | loss 2.4674 | step time 36.54ms\n",
            "step 797880 | loss 2.3069 | step time 36.07ms\n",
            "step 797890 | loss 2.2581 | step time 38.05ms\n",
            "step 797900 | loss 2.5588 | step time 38.25ms\n",
            "step 797910 | loss 2.5555 | step time 42.24ms\n",
            "step 797920 | loss 2.3321 | step time 37.96ms\n",
            "step 797930 | loss 2.4388 | step time 37.36ms\n",
            "step 797940 | loss 2.4873 | step time 44.81ms\n",
            "step 797950 | loss 2.4588 | step time 42.30ms\n",
            "step 797960 | loss 2.3711 | step time 38.67ms\n",
            "step 797970 | loss 2.4359 | step time 39.80ms\n",
            "step 797980 | loss 2.5492 | step time 36.24ms\n",
            "step 797990 | loss 2.5397 | step time 43.90ms\n",
            "step 798000 | loss 2.5395 | step time 36.15ms\n",
            "step 798000 train loss: 2.423903703689575 test loss: 2.480816602706909\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Gengin41\n",
            "Existeninine\n",
            "ajczzmostoo1\n",
            "mredinus\n",
            "carchar2001\n",
            "mbmrmbs1015\n",
            "th3_womyPK\n",
            "MasterBet\n",
            "lapperino\n",
            "angryhits\n",
            "--------------------------------------------------------------------------------\n",
            "step 798010 | loss 2.5446 | step time 39.71ms\n",
            "step 798020 | loss 2.4742 | step time 61.74ms\n",
            "step 798030 | loss 2.4742 | step time 52.81ms\n",
            "step 798040 | loss 2.4183 | step time 51.40ms\n",
            "step 798050 | loss 2.4155 | step time 58.55ms\n",
            "step 798060 | loss 2.6360 | step time 59.82ms\n",
            "step 798070 | loss 2.7099 | step time 54.19ms\n",
            "step 798080 | loss 2.3882 | step time 56.29ms\n",
            "step 798090 | loss 2.4370 | step time 59.29ms\n",
            "step 798100 | loss 2.4211 | step time 62.34ms\n",
            "step 798110 | loss 2.5338 | step time 59.54ms\n",
            "step 798120 | loss 2.6229 | step time 41.37ms\n",
            "step 798130 | loss 2.4860 | step time 36.84ms\n",
            "step 798140 | loss 2.5786 | step time 37.62ms\n",
            "step 798150 | loss 2.3688 | step time 37.22ms\n",
            "step 798160 | loss 2.4117 | step time 39.14ms\n",
            "step 798170 | loss 2.5023 | step time 39.08ms\n",
            "step 798180 | loss 2.4600 | step time 37.87ms\n",
            "step 798190 | loss 2.4040 | step time 43.97ms\n",
            "step 798200 | loss 2.3740 | step time 42.16ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "tolivera\n",
            "Brain_Brown\n",
            "criez12\n",
            "ryygaregood\n",
            "Biggiefluff989\n",
            "hpinch\n",
            "Whiteforbeline\n",
            "Starsheeskasher\n",
            "titswood\n",
            "Cargubluf\n",
            "--------------------------------------------------------------------------------\n",
            "step 798210 | loss 2.4310 | step time 38.05ms\n",
            "step 798220 | loss 2.3769 | step time 41.19ms\n",
            "step 798230 | loss 2.3948 | step time 38.11ms\n",
            "step 798240 | loss 2.5182 | step time 37.66ms\n",
            "step 798250 | loss 2.4574 | step time 37.86ms\n",
            "step 798260 | loss 2.4754 | step time 44.53ms\n",
            "step 798270 | loss 2.6414 | step time 42.32ms\n",
            "step 798280 | loss 2.5039 | step time 40.06ms\n",
            "step 798290 | loss 2.4020 | step time 37.31ms\n",
            "step 798300 | loss 2.4471 | step time 37.65ms\n",
            "step 798310 | loss 2.3328 | step time 38.21ms\n",
            "step 798320 | loss 2.4379 | step time 37.43ms\n",
            "step 798330 | loss 2.3770 | step time 37.78ms\n",
            "step 798340 | loss 2.5413 | step time 39.54ms\n",
            "step 798350 | loss 2.4535 | step time 38.59ms\n",
            "step 798360 | loss 2.4957 | step time 63.15ms\n",
            "step 798370 | loss 2.5890 | step time 50.16ms\n",
            "step 798380 | loss 2.5607 | step time 62.73ms\n",
            "step 798390 | loss 2.4077 | step time 58.30ms\n",
            "step 798400 | loss 2.4608 | step time 57.71ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "thallowandunned\n",
            "minecrosdailed\n",
            "okribbao\n",
            "Skayschool\n",
            "uberfaces\n",
            "Vnatoriau\n",
            "longradsfear\n",
            "E808\n",
            "KoAlaireazy\n",
            "notrem2dieusernames\n",
            "--------------------------------------------------------------------------------\n",
            "step 798410 | loss 2.3872 | step time 64.14ms\n",
            "step 798420 | loss 2.4891 | step time 59.46ms\n",
            "step 798430 | loss 2.3949 | step time 39.49ms\n",
            "step 798440 | loss 2.4865 | step time 51.67ms\n",
            "step 798450 | loss 2.5613 | step time 37.68ms\n",
            "step 798460 | loss 2.3953 | step time 39.34ms\n",
            "step 798470 | loss 2.5071 | step time 37.72ms\n",
            "step 798480 | loss 2.5140 | step time 35.39ms\n",
            "step 798490 | loss 2.5257 | step time 38.75ms\n",
            "step 798500 | loss 2.2637 | step time 37.48ms\n",
            "step 798500 train loss: 2.43912410736084 test loss: 2.478703737258911\n",
            "step 798510 | loss 2.4660 | step time 41.89ms\n",
            "step 798520 | loss 2.5441 | step time 37.37ms\n",
            "step 798530 | loss 2.3296 | step time 38.31ms\n",
            "step 798540 | loss 2.3257 | step time 38.18ms\n",
            "step 798550 | loss 2.4460 | step time 38.66ms\n",
            "step 798560 | loss 2.3692 | step time 38.53ms\n",
            "step 798570 | loss 2.6188 | step time 40.56ms\n",
            "step 798580 | loss 2.5740 | step time 38.69ms\n",
            "step 798590 | loss 2.4637 | step time 39.94ms\n",
            "step 798600 | loss 2.3688 | step time 36.45ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "nernedcoaty\n",
            "MrEneuchan\n",
            "neptranss\n",
            "Kumar_spank\n",
            "Super_stacker\n",
            "MetCol\n",
            "IngrockEffa33\n",
            "JeSef\n",
            "tomfirsm\n",
            "Sirvaguy\n",
            "--------------------------------------------------------------------------------\n",
            "step 798610 | loss 2.3482 | step time 37.00ms\n",
            "step 798620 | loss 2.4056 | step time 36.48ms\n",
            "step 798630 | loss 2.4434 | step time 39.74ms\n",
            "step 798640 | loss 2.6350 | step time 68.86ms\n",
            "step 798650 | loss 2.5417 | step time 55.35ms\n",
            "step 798660 | loss 2.3896 | step time 53.94ms\n",
            "step 798670 | loss 2.4787 | step time 61.38ms\n",
            "step 798680 | loss 2.3118 | step time 61.02ms\n",
            "step 798690 | loss 2.5034 | step time 53.37ms\n",
            "step 798700 | loss 2.4098 | step time 52.94ms\n",
            "step 798710 | loss 2.4810 | step time 62.66ms\n",
            "step 798720 | loss 2.4199 | step time 60.70ms\n",
            "step 798730 | loss 2.4446 | step time 39.77ms\n",
            "step 798740 | loss 2.4131 | step time 39.09ms\n",
            "step 798750 | loss 2.2987 | step time 35.56ms\n",
            "step 798760 | loss 2.4377 | step time 38.77ms\n",
            "step 798770 | loss 2.4347 | step time 42.14ms\n",
            "step 798780 | loss 2.4940 | step time 40.01ms\n",
            "step 798790 | loss 2.4856 | step time 38.86ms\n",
            "step 798800 | loss 2.4094 | step time 39.95ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "estax1910\n",
            "massweel\n",
            "Ashitponti-O\n",
            "destiny-sam82\n",
            "hini-minecrook\n",
            "J-Fassing8edDead\n",
            "Hulkath\n",
            "PlopEater\n",
            "calphabesund88\n",
            "mjs89\n",
            "--------------------------------------------------------------------------------\n",
            "step 798810 | loss 2.4409 | step time 41.35ms\n",
            "step 798820 | loss 2.3161 | step time 47.91ms\n",
            "step 798830 | loss 2.3889 | step time 39.08ms\n",
            "step 798840 | loss 2.4132 | step time 39.69ms\n",
            "step 798850 | loss 2.3585 | step time 37.47ms\n",
            "step 798860 | loss 2.5857 | step time 41.64ms\n",
            "step 798870 | loss 2.4791 | step time 38.26ms\n",
            "step 798880 | loss 2.4405 | step time 38.87ms\n",
            "step 798890 | loss 2.4264 | step time 36.62ms\n",
            "step 798900 | loss 2.3131 | step time 38.35ms\n",
            "step 798910 | loss 2.4109 | step time 40.33ms\n",
            "step 798920 | loss 2.4051 | step time 39.13ms\n",
            "step 798930 | loss 2.3878 | step time 38.66ms\n",
            "step 798940 | loss 2.3333 | step time 38.21ms\n",
            "step 798950 | loss 2.3919 | step time 41.93ms\n",
            "step 798960 | loss 2.3564 | step time 60.55ms\n",
            "step 798970 | loss 2.3827 | step time 61.03ms\n",
            "step 798980 | loss 2.4395 | step time 48.74ms\n",
            "step 798990 | loss 2.5209 | step time 67.40ms\n",
            "step 799000 | loss 2.5356 | step time 62.98ms\n",
            "step 799000 train loss: 2.478370189666748 test loss: 2.4791996479034424\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "kingofgreedmultinism\n",
            "zc1988\n",
            "tezza1\n",
            "chyrulez007\n",
            "un0n3d4l4ck\n",
            "senseip5\n",
            "jbigg310\n",
            "SelanekFarIan\n",
            "cexnishermanshigan\n",
            "yisdizifra\n",
            "--------------------------------------------------------------------------------\n",
            "step 799010 | loss 2.5822 | step time 59.01ms\n",
            "step 799020 | loss 2.3993 | step time 36.33ms\n",
            "step 799030 | loss 2.4705 | step time 40.43ms\n",
            "step 799040 | loss 2.4821 | step time 40.07ms\n",
            "step 799050 | loss 2.5491 | step time 39.45ms\n",
            "step 799060 | loss 2.3818 | step time 36.84ms\n",
            "step 799070 | loss 2.4319 | step time 39.51ms\n",
            "step 799080 | loss 2.4621 | step time 51.39ms\n",
            "step 799090 | loss 2.4771 | step time 36.49ms\n",
            "step 799100 | loss 2.4166 | step time 38.18ms\n",
            "step 799110 | loss 2.4322 | step time 36.31ms\n",
            "step 799120 | loss 2.4548 | step time 37.09ms\n",
            "step 799130 | loss 2.4668 | step time 34.72ms\n",
            "step 799140 | loss 2.6572 | step time 39.77ms\n",
            "step 799150 | loss 2.2721 | step time 39.46ms\n",
            "step 799160 | loss 2.3637 | step time 46.73ms\n",
            "step 799170 | loss 2.5388 | step time 39.77ms\n",
            "step 799180 | loss 2.3383 | step time 41.10ms\n",
            "step 799190 | loss 2.5701 | step time 40.03ms\n",
            "step 799200 | loss 2.4141 | step time 39.86ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "throwangy12\n",
            "TochedInRight\n",
            "h3rns\n",
            "theoinkousirbe\n",
            "Voset\n",
            "K64kSEESD\n",
            "iceoff\n",
            "barktrann\n",
            "pcmeme14\n",
            "TheIbobOneride\n",
            "--------------------------------------------------------------------------------\n",
            "step 799210 | loss 2.4155 | step time 40.90ms\n",
            "step 799220 | loss 2.3812 | step time 38.31ms\n",
            "step 799230 | loss 2.4699 | step time 40.97ms\n",
            "step 799240 | loss 2.3042 | step time 39.13ms\n",
            "step 799250 | loss 2.3485 | step time 36.98ms\n",
            "step 799260 | loss 2.3774 | step time 56.24ms\n",
            "step 799270 | loss 2.3499 | step time 52.88ms\n",
            "step 799280 | loss 2.5118 | step time 59.81ms\n",
            "step 799290 | loss 2.3568 | step time 55.73ms\n",
            "step 799300 | loss 2.6211 | step time 58.81ms\n",
            "step 799310 | loss 2.4315 | step time 58.27ms\n",
            "step 799320 | loss 2.6638 | step time 58.29ms\n",
            "step 799330 | loss 2.6329 | step time 60.56ms\n",
            "step 799340 | loss 2.3942 | step time 48.93ms\n",
            "step 799350 | loss 2.5469 | step time 39.16ms\n",
            "step 799360 | loss 2.5292 | step time 37.28ms\n",
            "step 799370 | loss 2.3230 | step time 36.01ms\n",
            "step 799380 | loss 2.5114 | step time 36.32ms\n",
            "step 799390 | loss 2.5345 | step time 38.03ms\n",
            "step 799400 | loss 2.5606 | step time 37.77ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "trymousemyboes\n",
            "TheHearthing\n",
            "tadonobitterkate\n",
            "Zussynap\n",
            "Noio\n",
            "ChiratheBumchu\n",
            "Ta_Mikey\n",
            "kinyop429\n",
            "YourWestyHereHereFir\n",
            "dalcini\n",
            "--------------------------------------------------------------------------------\n",
            "step 799410 | loss 2.2312 | step time 38.94ms\n",
            "step 799420 | loss 2.4378 | step time 37.34ms\n",
            "step 799430 | loss 2.5334 | step time 40.60ms\n",
            "step 799440 | loss 2.5068 | step time 36.90ms\n",
            "step 799450 | loss 2.5495 | step time 37.75ms\n",
            "step 799460 | loss 2.4389 | step time 38.63ms\n",
            "step 799470 | loss 2.3175 | step time 39.27ms\n",
            "step 799480 | loss 2.4892 | step time 47.96ms\n",
            "step 799490 | loss 2.6727 | step time 41.53ms\n",
            "step 799500 | loss 2.4648 | step time 37.46ms\n",
            "step 799500 train loss: 2.4505040645599365 test loss: 2.4800612926483154\n",
            "step 799510 | loss 2.5595 | step time 55.39ms\n",
            "step 799520 | loss 2.3603 | step time 40.12ms\n",
            "step 799530 | loss 2.3956 | step time 43.40ms\n",
            "step 799540 | loss 2.5515 | step time 39.16ms\n",
            "step 799550 | loss 2.4717 | step time 56.92ms\n",
            "step 799560 | loss 2.5408 | step time 56.19ms\n",
            "step 799570 | loss 2.3941 | step time 52.23ms\n",
            "step 799580 | loss 2.2726 | step time 67.34ms\n",
            "step 799590 | loss 2.7527 | step time 59.61ms\n",
            "step 799600 | loss 2.3303 | step time 68.68ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Rhundredhakunvu\n",
            "vroyjoyce\n",
            "Just-a-Coach\n",
            "5utovointhewarrior\n",
            "michelrud\n",
            "jimmehairfra\n",
            "mofowerwulfalaben\n",
            "bi_mutamtemars\n",
            "blioard1981\n",
            "alex436\n",
            "--------------------------------------------------------------------------------\n",
            "step 799610 | loss 2.5371 | step time 61.59ms\n",
            "step 799620 | loss 2.4664 | step time 37.16ms\n",
            "step 799630 | loss 2.6496 | step time 39.57ms\n",
            "step 799640 | loss 2.4512 | step time 36.70ms\n",
            "step 799650 | loss 2.4418 | step time 38.76ms\n",
            "step 799660 | loss 2.3255 | step time 38.45ms\n",
            "step 799670 | loss 2.4297 | step time 36.97ms\n",
            "step 799680 | loss 2.5273 | step time 37.31ms\n",
            "step 799690 | loss 2.4427 | step time 37.49ms\n",
            "step 799700 | loss 2.2064 | step time 44.13ms\n",
            "step 799710 | loss 2.5389 | step time 37.24ms\n",
            "step 799720 | loss 2.5051 | step time 38.38ms\n",
            "step 799730 | loss 2.3978 | step time 38.68ms\n",
            "step 799740 | loss 2.3542 | step time 38.41ms\n",
            "step 799750 | loss 2.4922 | step time 42.88ms\n",
            "step 799760 | loss 2.4340 | step time 38.35ms\n",
            "step 799770 | loss 2.5516 | step time 44.16ms\n",
            "step 799780 | loss 2.4550 | step time 40.50ms\n",
            "step 799790 | loss 2.3609 | step time 36.67ms\n",
            "step 799800 | loss 2.2346 | step time 39.97ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "erdeincirclass\n",
            "carlear1233\n",
            "Sheisbird\n",
            "lanidum\n",
            "stevebarejan\n",
            "barkhair5061\n",
            "leifow\n",
            "ImJedik\n",
            "IWantGobutHydrop\n",
            "bettybeagaleese\n",
            "--------------------------------------------------------------------------------\n",
            "step 799810 | loss 2.4176 | step time 37.85ms\n",
            "step 799820 | loss 2.4777 | step time 40.18ms\n",
            "step 799830 | loss 2.3406 | step time 38.09ms\n",
            "step 799840 | loss 2.4215 | step time 40.46ms\n",
            "step 799850 | loss 2.3730 | step time 57.21ms\n",
            "step 799860 | loss 2.4023 | step time 54.22ms\n",
            "step 799870 | loss 2.5905 | step time 59.30ms\n",
            "step 799880 | loss 2.3780 | step time 58.31ms\n",
            "step 799890 | loss 2.3088 | step time 61.81ms\n",
            "step 799900 | loss 2.4792 | step time 55.06ms\n",
            "step 799910 | loss 2.4977 | step time 61.51ms\n",
            "step 799920 | loss 2.5463 | step time 54.07ms\n",
            "step 799930 | loss 2.4069 | step time 58.91ms\n",
            "step 799940 | loss 2.3206 | step time 58.48ms\n",
            "step 799950 | loss 2.5062 | step time 36.60ms\n",
            "step 799960 | loss 2.5267 | step time 36.82ms\n",
            "step 799970 | loss 2.4907 | step time 38.18ms\n",
            "step 799980 | loss 2.2885 | step time 43.67ms\n",
            "step 799990 | loss 2.5535 | step time 37.81ms\n",
            "step 800000 | loss 2.3893 | step time 37.52ms\n",
            "step 800000 train loss: 2.446211576461792 test loss: 2.4790456295013428\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "brayhouse13\n",
            "lai-evag\n",
            "wolfman24\n",
            "ImJoshDog\n",
            "Halo_Biola\n",
            "shrikesou\n",
            "Ivd_Luvstoned\n",
            "same\n",
            "wantitme\n",
            "James0nLife\n",
            "--------------------------------------------------------------------------------\n",
            "step 800010 | loss 2.6633 | step time 38.71ms\n",
            "step 800020 | loss 2.5603 | step time 39.22ms\n",
            "step 800030 | loss 2.5699 | step time 41.41ms\n",
            "step 800040 | loss 2.3934 | step time 40.13ms\n",
            "step 800050 | loss 2.4727 | step time 44.80ms\n",
            "step 800060 | loss 2.4487 | step time 46.71ms\n",
            "step 800070 | loss 2.4829 | step time 38.63ms\n",
            "step 800080 | loss 2.6389 | step time 39.53ms\n",
            "step 800090 | loss 2.5796 | step time 41.00ms\n",
            "step 800100 | loss 2.3721 | step time 43.76ms\n",
            "step 800110 | loss 2.5419 | step time 48.95ms\n",
            "step 800120 | loss 2.6812 | step time 38.60ms\n",
            "step 800130 | loss 2.6134 | step time 38.58ms\n",
            "step 800140 | loss 2.2923 | step time 39.20ms\n",
            "step 800150 | loss 2.6915 | step time 44.38ms\n",
            "step 800160 | loss 2.4966 | step time 38.91ms\n",
            "step 800170 | loss 2.3595 | step time 56.72ms\n",
            "step 800180 | loss 2.3461 | step time 54.51ms\n",
            "step 800190 | loss 2.4948 | step time 51.48ms\n",
            "step 800200 | loss 2.4335 | step time 54.71ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "konicherwife3\n",
            "pandaton_rene\n",
            "Cr8man5t7n\n",
            "nofurdharpy\n",
            "lillonriz123\n",
            "Pezchologicbitches\n",
            "Throwaway288976790x7\n",
            "JustCAMidges\n",
            "mlgkkjawsharks\n",
            "rarshinningitime\n",
            "--------------------------------------------------------------------------------\n",
            "step 800210 | loss 2.3984 | step time 57.39ms\n",
            "step 800220 | loss 2.5388 | step time 58.60ms\n",
            "step 800230 | loss 2.4425 | step time 60.29ms\n",
            "step 800240 | loss 2.5998 | step time 39.10ms\n",
            "step 800250 | loss 2.4486 | step time 38.24ms\n",
            "step 800260 | loss 2.5376 | step time 49.93ms\n",
            "step 800270 | loss 2.4288 | step time 38.74ms\n",
            "step 800280 | loss 2.7084 | step time 38.72ms\n",
            "step 800290 | loss 2.4426 | step time 37.05ms\n",
            "step 800300 | loss 2.3374 | step time 37.44ms\n",
            "step 800310 | loss 2.6734 | step time 38.89ms\n",
            "step 800320 | loss 2.4068 | step time 38.07ms\n",
            "step 800330 | loss 2.5287 | step time 35.76ms\n",
            "step 800340 | loss 2.6215 | step time 38.70ms\n",
            "step 800350 | loss 2.3484 | step time 38.26ms\n",
            "step 800360 | loss 2.3298 | step time 38.38ms\n",
            "step 800370 | loss 2.4326 | step time 39.22ms\n",
            "step 800380 | loss 2.4370 | step time 38.61ms\n",
            "step 800390 | loss 2.2846 | step time 36.99ms\n",
            "step 800400 | loss 2.3758 | step time 39.29ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Zechk\n",
            "bobeofpenis\n",
            "Nick0\n",
            "neugh63212\n",
            "chazzard18\n",
            "magicalstever31\n",
            "ednessanmatt619\n",
            "TheDickYouEctions\n",
            "sunnif90\n",
            "destreet_kevin872\n",
            "--------------------------------------------------------------------------------\n",
            "step 800410 | loss 2.4281 | step time 39.65ms\n",
            "step 800420 | loss 2.3747 | step time 38.79ms\n",
            "step 800430 | loss 2.5188 | step time 39.56ms\n",
            "step 800440 | loss 2.5907 | step time 39.37ms\n",
            "step 800450 | loss 2.5192 | step time 40.21ms\n",
            "step 800460 | loss 2.4502 | step time 37.92ms\n",
            "step 800470 | loss 2.4158 | step time 54.73ms\n",
            "step 800480 | loss 2.3806 | step time 49.41ms\n",
            "step 800490 | loss 2.5396 | step time 52.25ms\n",
            "step 800500 | loss 2.5676 | step time 55.54ms\n",
            "step 800500 train loss: 2.4602277278900146 test loss: 2.4767048358917236\n",
            "step 800510 | loss 2.5255 | step time 72.48ms\n",
            "step 800520 | loss 2.3593 | step time 62.15ms\n",
            "step 800530 | loss 2.3309 | step time 56.00ms\n",
            "step 800540 | loss 2.5255 | step time 60.77ms\n",
            "step 800550 | loss 2.3206 | step time 39.94ms\n",
            "step 800560 | loss 2.4574 | step time 41.29ms\n",
            "step 800570 | loss 2.2989 | step time 38.73ms\n",
            "step 800580 | loss 2.6093 | step time 37.85ms\n",
            "step 800590 | loss 2.4332 | step time 36.26ms\n",
            "step 800600 | loss 2.4380 | step time 42.87ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "GuadiaBugger\n",
            "ambr2580\n",
            "Capn-Max\n",
            "Topthic47\n",
            "Qyeysparoly\n",
            "GoezzoMin\n",
            "Yowdesty\n",
            "Rhad_RDU_R\n",
            "swarthai\n",
            "zacfornsonjagger\n",
            "--------------------------------------------------------------------------------\n",
            "step 800610 | loss 2.3814 | step time 42.98ms\n",
            "step 800620 | loss 2.4023 | step time 36.75ms\n",
            "step 800630 | loss 2.4273 | step time 36.36ms\n",
            "step 800640 | loss 2.6831 | step time 36.72ms\n",
            "step 800650 | loss 2.4072 | step time 39.56ms\n",
            "step 800660 | loss 2.4275 | step time 37.58ms\n",
            "step 800670 | loss 2.4678 | step time 37.48ms\n",
            "step 800680 | loss 2.5231 | step time 36.88ms\n",
            "step 800690 | loss 2.4753 | step time 49.48ms\n",
            "step 800700 | loss 2.3878 | step time 38.10ms\n",
            "step 800710 | loss 2.4081 | step time 39.14ms\n",
            "step 800720 | loss 2.4228 | step time 36.85ms\n",
            "step 800730 | loss 2.5218 | step time 38.17ms\n",
            "step 800740 | loss 2.4277 | step time 37.47ms\n",
            "step 800750 | loss 2.4820 | step time 37.88ms\n",
            "step 800760 | loss 2.4432 | step time 39.37ms\n",
            "step 800770 | loss 2.4718 | step time 38.01ms\n",
            "step 800780 | loss 2.4086 | step time 56.91ms\n",
            "step 800790 | loss 2.4864 | step time 54.77ms\n",
            "step 800800 | loss 2.4663 | step time 53.44ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "cbir_penguin\n",
            "GrabackTape\n",
            "xnFoolyX\n",
            "NeoraJam\n",
            "kevils03339\n",
            "ManWork\n",
            "medicleroast857\n",
            "wot_greheart\n",
            "bbackclue\n",
            "throwawayhelpmehlap\n",
            "--------------------------------------------------------------------------------\n",
            "step 800810 | loss 2.3402 | step time 55.96ms\n",
            "step 800820 | loss 2.5317 | step time 64.62ms\n",
            "step 800830 | loss 2.2972 | step time 60.44ms\n",
            "step 800840 | loss 2.5180 | step time 55.84ms\n",
            "step 800850 | loss 2.3984 | step time 39.02ms\n",
            "step 800860 | loss 2.3877 | step time 40.13ms\n",
            "step 800870 | loss 2.5771 | step time 43.18ms\n",
            "step 800880 | loss 2.3875 | step time 40.83ms\n",
            "step 800890 | loss 2.4971 | step time 36.76ms\n",
            "step 800900 | loss 2.3936 | step time 39.67ms\n",
            "step 800910 | loss 2.4388 | step time 39.68ms\n",
            "step 800920 | loss 2.4534 | step time 46.97ms\n",
            "step 800930 | loss 2.3098 | step time 37.19ms\n",
            "step 800940 | loss 2.3162 | step time 39.19ms\n",
            "step 800950 | loss 2.4206 | step time 39.03ms\n",
            "step 800960 | loss 2.4058 | step time 37.47ms\n",
            "step 800970 | loss 2.5440 | step time 43.63ms\n",
            "step 800980 | loss 2.3797 | step time 35.89ms\n",
            "step 800990 | loss 2.3482 | step time 39.51ms\n",
            "step 801000 | loss 2.4764 | step time 37.70ms\n",
            "step 801000 train loss: 2.43023943901062 test loss: 2.4811713695526123\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "EM_IM\n",
            "Unsrcount\n",
            "TMafaveryuser\n",
            "unhodogson\n",
            "_timez3\n",
            "Polecore_The_84\n",
            "ralkooth0\n",
            "triggaburi\n",
            "mique_im_creepixel\n",
            "PandaPlz9\n",
            "--------------------------------------------------------------------------------\n",
            "step 801010 | loss 2.5080 | step time 37.96ms\n",
            "step 801020 | loss 2.4966 | step time 37.27ms\n",
            "step 801030 | loss 2.5896 | step time 42.88ms\n",
            "step 801040 | loss 2.2969 | step time 37.16ms\n",
            "step 801050 | loss 2.5508 | step time 38.21ms\n",
            "step 801060 | loss 2.3256 | step time 35.82ms\n",
            "step 801070 | loss 2.4764 | step time 58.09ms\n",
            "step 801080 | loss 2.3305 | step time 52.43ms\n",
            "step 801090 | loss 2.6088 | step time 69.01ms\n",
            "step 801100 | loss 2.4035 | step time 56.46ms\n",
            "step 801110 | loss 2.5674 | step time 59.54ms\n",
            "step 801120 | loss 2.3805 | step time 56.10ms\n",
            "step 801130 | loss 2.3234 | step time 57.53ms\n",
            "step 801140 | loss 2.5230 | step time 54.44ms\n",
            "step 801150 | loss 2.5846 | step time 37.57ms\n",
            "step 801160 | loss 2.3387 | step time 37.90ms\n",
            "step 801170 | loss 2.4181 | step time 37.30ms\n",
            "step 801180 | loss 2.4519 | step time 39.90ms\n",
            "step 801190 | loss 2.3448 | step time 39.48ms\n",
            "step 801200 | loss 2.4641 | step time 39.87ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "kavix4916921\n",
            "PM_ME_US_MV\n",
            "starmyfstario\n",
            "notnotafarmate\n",
            "striker1\n",
            "LukaR12\n",
            "zekosohomeweschov\n",
            "zerno_park\n",
            "jacobarsbuggal\n",
            "Doobooobobsgular\n",
            "--------------------------------------------------------------------------------\n",
            "step 801210 | loss 2.5959 | step time 36.20ms\n",
            "step 801220 | loss 2.4066 | step time 49.03ms\n",
            "step 801230 | loss 2.3545 | step time 37.90ms\n",
            "step 801240 | loss 2.3335 | step time 37.16ms\n",
            "step 801250 | loss 2.3774 | step time 37.03ms\n",
            "step 801260 | loss 2.4147 | step time 37.67ms\n",
            "step 801270 | loss 2.5835 | step time 38.36ms\n",
            "step 801280 | loss 2.3465 | step time 38.99ms\n",
            "step 801290 | loss 2.3756 | step time 38.13ms\n",
            "step 801300 | loss 2.4190 | step time 36.81ms\n",
            "step 801310 | loss 2.3892 | step time 36.81ms\n",
            "step 801320 | loss 2.6247 | step time 38.80ms\n",
            "step 801330 | loss 2.4784 | step time 38.27ms\n",
            "step 801340 | loss 2.4452 | step time 37.43ms\n",
            "step 801350 | loss 2.2632 | step time 49.14ms\n",
            "step 801360 | loss 2.4636 | step time 36.61ms\n",
            "step 801370 | loss 2.5567 | step time 39.10ms\n",
            "step 801380 | loss 2.4296 | step time 39.14ms\n",
            "step 801390 | loss 2.4678 | step time 55.43ms\n",
            "step 801400 | loss 2.3933 | step time 53.67ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "renzabeast49\n",
            "jonospater3ot\n",
            "tvtujwwt\n",
            "skavetidont\n",
            "SpiltInVoid\n",
            "slakeypoo\n",
            "cumb121012\n",
            "PageCanadian\n",
            "bionmuddy91\n",
            "emzars2214\n",
            "--------------------------------------------------------------------------------\n",
            "step 801410 | loss 2.4293 | step time 55.92ms\n",
            "step 801420 | loss 2.4594 | step time 57.67ms\n",
            "step 801430 | loss 2.4041 | step time 76.49ms\n",
            "step 801440 | loss 2.4362 | step time 58.47ms\n",
            "step 801450 | loss 2.4822 | step time 56.94ms\n",
            "step 801460 | loss 2.4226 | step time 41.17ms\n",
            "step 801470 | loss 2.3296 | step time 44.06ms\n",
            "step 801480 | loss 2.4670 | step time 40.91ms\n",
            "step 801490 | loss 2.3595 | step time 37.68ms\n",
            "step 801500 | loss 2.6358 | step time 35.06ms\n",
            "step 801500 train loss: 2.4409050941467285 test loss: 2.4874768257141113\n",
            "step 801510 | loss 2.5520 | step time 38.87ms\n",
            "step 801520 | loss 2.5311 | step time 44.69ms\n",
            "step 801530 | loss 2.4316 | step time 40.65ms\n",
            "step 801540 | loss 2.3725 | step time 38.11ms\n",
            "step 801550 | loss 2.2427 | step time 40.37ms\n",
            "step 801560 | loss 2.5253 | step time 37.50ms\n",
            "step 801570 | loss 2.3908 | step time 39.26ms\n",
            "step 801580 | loss 2.3627 | step time 40.91ms\n",
            "step 801590 | loss 2.5702 | step time 40.19ms\n",
            "step 801600 | loss 2.5227 | step time 40.24ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "aplebattermuf\n",
            "angelbresshill\n",
            "zameros\n",
            "SuperfliCWaw315\n",
            "Mikolar\n",
            "Billyowlive\n",
            "suck_dirty_hippie\n",
            "NiedCarvaTion\n",
            "Txxgaml\n",
            "jakechan\n",
            "--------------------------------------------------------------------------------\n",
            "step 801610 | loss 2.4010 | step time 61.32ms\n",
            "step 801620 | loss 2.5114 | step time 39.19ms\n",
            "step 801630 | loss 2.4454 | step time 40.52ms\n",
            "step 801640 | loss 2.3082 | step time 39.92ms\n",
            "step 801650 | loss 2.5308 | step time 41.85ms\n",
            "step 801660 | loss 2.3707 | step time 40.72ms\n",
            "step 801670 | loss 2.5170 | step time 67.82ms\n",
            "step 801680 | loss 2.3828 | step time 51.90ms\n",
            "step 801690 | loss 2.4635 | step time 52.44ms\n",
            "step 801700 | loss 2.5041 | step time 60.92ms\n",
            "step 801710 | loss 2.3493 | step time 59.86ms\n",
            "step 801720 | loss 2.4200 | step time 65.37ms\n",
            "step 801730 | loss 2.4341 | step time 70.41ms\n",
            "step 801740 | loss 2.5916 | step time 59.83ms\n",
            "step 801750 | loss 2.3270 | step time 60.54ms\n",
            "step 801760 | loss 2.3756 | step time 39.86ms\n",
            "step 801770 | loss 2.2370 | step time 39.49ms\n",
            "step 801780 | loss 2.4879 | step time 41.58ms\n",
            "step 801790 | loss 2.4449 | step time 37.89ms\n",
            "step 801800 | loss 2.4592 | step time 40.03ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ytrum\n",
            "Tsy0010000303\n",
            "PannaNiest33\n",
            "sisiaxian\n",
            "Hdy4tobreed\n",
            "brucemailmaise\n",
            "le_throwaway_fortown\n",
            "aericaforget\n",
            "pe08hh\n",
            "Purpler-Jiggla\n",
            "--------------------------------------------------------------------------------\n",
            "step 801810 | loss 2.3928 | step time 39.11ms\n",
            "step 801820 | loss 2.3383 | step time 41.17ms\n",
            "step 801830 | loss 2.5701 | step time 39.50ms\n",
            "step 801840 | loss 2.3582 | step time 36.83ms\n",
            "step 801850 | loss 2.2644 | step time 38.11ms\n",
            "step 801860 | loss 2.4162 | step time 40.50ms\n",
            "step 801870 | loss 2.2944 | step time 40.90ms\n",
            "step 801880 | loss 2.4584 | step time 40.77ms\n",
            "step 801890 | loss 2.5368 | step time 42.28ms\n",
            "step 801900 | loss 2.4097 | step time 42.44ms\n",
            "step 801910 | loss 2.3249 | step time 38.37ms\n",
            "step 801920 | loss 2.4814 | step time 39.38ms\n",
            "step 801930 | loss 2.5108 | step time 39.22ms\n",
            "step 801940 | loss 2.4207 | step time 39.34ms\n",
            "step 801950 | loss 2.2594 | step time 42.78ms\n",
            "step 801960 | loss 2.4227 | step time 43.78ms\n",
            "step 801970 | loss 2.5097 | step time 39.76ms\n",
            "step 801980 | loss 2.4501 | step time 45.77ms\n",
            "step 801990 | loss 2.2845 | step time 61.30ms\n",
            "step 802000 | loss 2.3859 | step time 53.58ms\n",
            "step 802000 train loss: 2.4632318019866943 test loss: 2.4843759536743164\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "bahamatsk\n",
            "sugar_mon_suilet\n",
            "kaleeroll\n",
            "Henrb63_\n",
            "sighicalas\n",
            "cloudity80\n",
            "GoozerBoseph\n",
            "dazzle74\n",
            "Masrag\n",
            "doinnivcoo\n",
            "--------------------------------------------------------------------------------\n",
            "step 802010 | loss 2.4948 | step time 54.63ms\n",
            "step 802020 | loss 2.4565 | step time 59.40ms\n",
            "step 802030 | loss 2.5200 | step time 58.91ms\n",
            "step 802040 | loss 2.5579 | step time 38.17ms\n",
            "step 802050 | loss 2.3613 | step time 39.67ms\n",
            "step 802060 | loss 2.3900 | step time 37.53ms\n",
            "step 802070 | loss 2.3654 | step time 36.05ms\n",
            "step 802080 | loss 2.3707 | step time 38.52ms\n",
            "step 802090 | loss 2.5357 | step time 39.97ms\n",
            "step 802100 | loss 2.4080 | step time 37.05ms\n",
            "step 802110 | loss 2.4059 | step time 37.87ms\n",
            "step 802120 | loss 2.4852 | step time 39.09ms\n",
            "step 802130 | loss 2.3969 | step time 49.94ms\n",
            "step 802140 | loss 2.4808 | step time 39.42ms\n",
            "step 802150 | loss 2.3781 | step time 38.92ms\n",
            "step 802160 | loss 2.3872 | step time 37.45ms\n",
            "step 802170 | loss 2.3191 | step time 43.52ms\n",
            "step 802180 | loss 2.4394 | step time 36.73ms\n",
            "step 802190 | loss 2.4814 | step time 40.32ms\n",
            "step 802200 | loss 2.4346 | step time 37.28ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "3Evenfod5\n",
            "forroveruthind\n",
            "WeooveMuffinQC\n",
            "Kaminsman\n",
            "browningspananame777\n",
            "hadgesthaze1\n",
            "tahanemskop99\n",
            "Haix-Did\n",
            "beatedanequick\n",
            "jojosto8\n",
            "--------------------------------------------------------------------------------\n",
            "step 802210 | loss 2.5861 | step time 37.77ms\n",
            "step 802220 | loss 2.3359 | step time 37.01ms\n",
            "step 802230 | loss 2.5881 | step time 38.72ms\n",
            "step 802240 | loss 2.4446 | step time 37.87ms\n",
            "step 802250 | loss 2.3500 | step time 39.67ms\n",
            "step 802260 | loss 2.3683 | step time 38.94ms\n",
            "step 802270 | loss 2.5844 | step time 38.49ms\n",
            "step 802280 | loss 2.3642 | step time 62.73ms\n",
            "step 802290 | loss 2.5905 | step time 52.67ms\n",
            "step 802300 | loss 2.5093 | step time 51.69ms\n",
            "step 802310 | loss 2.4511 | step time 53.55ms\n",
            "step 802320 | loss 2.4082 | step time 52.41ms\n",
            "step 802330 | loss 2.2806 | step time 58.43ms\n",
            "step 802340 | loss 2.5138 | step time 56.29ms\n",
            "step 802350 | loss 2.4772 | step time 56.83ms\n",
            "step 802360 | loss 2.4511 | step time 36.46ms\n",
            "step 802370 | loss 2.5801 | step time 42.08ms\n",
            "step 802380 | loss 2.3905 | step time 40.00ms\n",
            "step 802390 | loss 2.4833 | step time 35.77ms\n",
            "step 802400 | loss 2.2950 | step time 39.39ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Fyf1012\n",
            "lowthememe_the__Plai\n",
            "Martoriuseroo\n",
            "orgafging\n",
            "pilotbanksdasdarkux\n",
            "Big_Ascale_3123\n",
            "TheDemonTV\n",
            "RachoMaddBerg321\n",
            "SinnalJmY69C\n",
            "Framp_Gyeill7\n",
            "--------------------------------------------------------------------------------\n",
            "step 802410 | loss 2.5626 | step time 43.48ms\n",
            "step 802420 | loss 2.4552 | step time 38.16ms\n",
            "step 802430 | loss 2.4376 | step time 36.96ms\n",
            "step 802440 | loss 2.5905 | step time 39.25ms\n",
            "step 802450 | loss 2.4781 | step time 39.59ms\n",
            "step 802460 | loss 2.4567 | step time 36.95ms\n",
            "step 802470 | loss 2.5082 | step time 37.15ms\n",
            "step 802480 | loss 2.5994 | step time 47.70ms\n",
            "step 802490 | loss 2.3811 | step time 42.22ms\n",
            "step 802500 | loss 2.5113 | step time 41.02ms\n",
            "step 802500 train loss: 2.436828374862671 test loss: 2.482372522354126\n",
            "step 802510 | loss 2.5370 | step time 44.81ms\n",
            "step 802520 | loss 2.5298 | step time 39.19ms\n",
            "step 802530 | loss 2.3906 | step time 38.09ms\n",
            "step 802540 | loss 2.5375 | step time 43.90ms\n",
            "step 802550 | loss 2.5006 | step time 38.61ms\n",
            "step 802560 | loss 2.4268 | step time 38.86ms\n",
            "step 802570 | loss 2.4012 | step time 61.94ms\n",
            "step 802580 | loss 2.4111 | step time 56.82ms\n",
            "step 802590 | loss 2.4038 | step time 50.75ms\n",
            "step 802600 | loss 2.5731 | step time 61.32ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "AUOGP\n",
            "donut_moose\n",
            "Critchia\n",
            "mrzonkeles\n",
            "Sleanesmakes\n",
            "jacktlick000\n",
            "AgersHelf\n",
            "mylo9\n",
            "Bier198442\n",
            "Efrenzierp\n",
            "--------------------------------------------------------------------------------\n",
            "step 802610 | loss 2.3189 | step time 59.43ms\n",
            "step 802620 | loss 2.3051 | step time 61.19ms\n",
            "step 802630 | loss 2.4500 | step time 46.13ms\n",
            "step 802640 | loss 2.5182 | step time 41.24ms\n",
            "step 802650 | loss 2.3044 | step time 40.99ms\n",
            "step 802660 | loss 2.4943 | step time 38.39ms\n",
            "step 802670 | loss 2.3317 | step time 38.48ms\n",
            "step 802680 | loss 2.3828 | step time 46.30ms\n",
            "step 802690 | loss 2.5132 | step time 39.53ms\n",
            "step 802700 | loss 2.6167 | step time 42.91ms\n",
            "step 802710 | loss 2.3849 | step time 40.65ms\n",
            "step 802720 | loss 2.2620 | step time 51.27ms\n",
            "step 802730 | loss 2.4894 | step time 46.64ms\n",
            "step 802740 | loss 2.5442 | step time 45.81ms\n",
            "step 802750 | loss 2.4348 | step time 50.76ms\n",
            "step 802760 | loss 2.5059 | step time 42.98ms\n",
            "step 802770 | loss 2.3114 | step time 44.10ms\n",
            "step 802780 | loss 2.4186 | step time 38.84ms\n",
            "step 802790 | loss 2.4208 | step time 39.18ms\n",
            "step 802800 | loss 2.3842 | step time 40.60ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "bellsponde\n",
            "YuzerWolf\n",
            "wryderax\n",
            "Siucius355\n",
            "BerryChit\n",
            "CottlightRoot99\n",
            "Jamenetti\n",
            "huskingsandpoorcale\n",
            "hdnahfive\n",
            "arrenttdclimb\n",
            "--------------------------------------------------------------------------------\n",
            "step 802810 | loss 2.4087 | step time 37.98ms\n",
            "step 802820 | loss 2.4215 | step time 39.85ms\n",
            "step 802830 | loss 2.3019 | step time 40.46ms\n",
            "step 802840 | loss 2.3115 | step time 43.58ms\n",
            "step 802850 | loss 2.2853 | step time 62.72ms\n",
            "step 802860 | loss 2.3940 | step time 61.44ms\n",
            "step 802870 | loss 2.4503 | step time 53.43ms\n",
            "step 802880 | loss 2.3033 | step time 57.21ms\n",
            "step 802890 | loss 2.3315 | step time 61.81ms\n",
            "step 802900 | loss 2.5291 | step time 60.75ms\n",
            "step 802910 | loss 2.3128 | step time 60.69ms\n",
            "step 802920 | loss 2.4157 | step time 57.10ms\n",
            "step 802930 | loss 2.4929 | step time 58.47ms\n",
            "step 802940 | loss 2.4835 | step time 58.94ms\n",
            "step 802950 | loss 2.5880 | step time 40.55ms\n",
            "step 802960 | loss 2.4729 | step time 39.44ms\n",
            "step 802970 | loss 2.5038 | step time 40.15ms\n",
            "step 802980 | loss 2.3027 | step time 37.83ms\n",
            "step 802990 | loss 2.4590 | step time 39.48ms\n",
            "step 803000 | loss 2.4855 | step time 39.96ms\n",
            "step 803000 train loss: 2.4286863803863525 test loss: 2.4843647480010986\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "RollisHeart\n",
            "Markwarex\n",
            "jamticko\n",
            "Brymimig\n",
            "TruckSocks\n",
            "throwaway23v410\n",
            "S_AssassinShadopati\n",
            "mausha1b\n",
            "Jayseeke\n",
            "URI5teep1P\n",
            "--------------------------------------------------------------------------------\n",
            "step 803010 | loss 2.3712 | step time 38.61ms\n",
            "step 803020 | loss 2.5263 | step time 39.51ms\n",
            "step 803030 | loss 2.4082 | step time 38.54ms\n",
            "step 803040 | loss 2.4669 | step time 43.11ms\n",
            "step 803050 | loss 2.3156 | step time 39.60ms\n",
            "step 803060 | loss 2.4453 | step time 41.57ms\n",
            "step 803070 | loss 2.4926 | step time 43.99ms\n",
            "step 803080 | loss 2.3983 | step time 40.00ms\n",
            "step 803090 | loss 2.5071 | step time 42.83ms\n",
            "step 803100 | loss 2.6695 | step time 47.28ms\n",
            "step 803110 | loss 2.4304 | step time 38.67ms\n",
            "step 803120 | loss 2.4037 | step time 39.06ms\n",
            "step 803130 | loss 2.5178 | step time 40.85ms\n",
            "step 803140 | loss 2.5688 | step time 39.31ms\n",
            "step 803150 | loss 2.4770 | step time 72.25ms\n",
            "step 803160 | loss 2.5004 | step time 55.42ms\n",
            "step 803170 | loss 2.5432 | step time 50.70ms\n",
            "step 803180 | loss 2.4981 | step time 60.09ms\n",
            "step 803190 | loss 2.4990 | step time 58.23ms\n",
            "step 803200 | loss 2.5541 | step time 61.83ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "U_No_Kuzy_IDkurk\n",
            "LJbearGoon\n",
            "rashkills\n",
            "LoolioBawl\n",
            "notsotheraider\n",
            "scurachman\n",
            "rcine\n",
            "Justinarnothingbro\n",
            "edsicabletoffi\n",
            "shadoro\n",
            "--------------------------------------------------------------------------------\n",
            "step 803210 | loss 2.4558 | step time 60.55ms\n",
            "step 803220 | loss 2.5149 | step time 59.87ms\n",
            "step 803230 | loss 2.2691 | step time 42.70ms\n",
            "step 803240 | loss 2.4236 | step time 37.88ms\n",
            "step 803250 | loss 2.4995 | step time 42.60ms\n",
            "step 803260 | loss 2.4951 | step time 38.02ms\n",
            "step 803270 | loss 2.2493 | step time 44.56ms\n",
            "step 803280 | loss 2.3595 | step time 39.40ms\n",
            "step 803290 | loss 2.4318 | step time 43.20ms\n",
            "step 803300 | loss 2.4417 | step time 39.09ms\n",
            "step 803310 | loss 2.2135 | step time 44.11ms\n",
            "step 803320 | loss 2.5722 | step time 42.63ms\n",
            "step 803330 | loss 2.2614 | step time 52.40ms\n",
            "step 803340 | loss 2.5445 | step time 44.50ms\n",
            "step 803350 | loss 2.5127 | step time 41.69ms\n",
            "step 803360 | loss 2.3021 | step time 40.22ms\n",
            "step 803370 | loss 2.5369 | step time 39.90ms\n",
            "step 803380 | loss 2.5102 | step time 47.27ms\n",
            "step 803390 | loss 2.3885 | step time 40.27ms\n",
            "step 803400 | loss 2.4761 | step time 38.47ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "SVacoGao\n",
            "lrekaor\n",
            "CregnerForSquare\n",
            "Pardemul\n",
            "-Tio-\n",
            "Verysine\n",
            "Dandemptart\n",
            "sonictokee\n",
            "Kal-EXC\n",
            "estriamvilia\n",
            "--------------------------------------------------------------------------------\n",
            "step 803410 | loss 2.5308 | step time 38.63ms\n",
            "step 803420 | loss 2.5748 | step time 36.99ms\n",
            "step 803430 | loss 2.3577 | step time 38.34ms\n",
            "step 803440 | loss 2.3515 | step time 39.93ms\n",
            "step 803450 | loss 2.4284 | step time 56.74ms\n",
            "step 803460 | loss 2.5530 | step time 67.24ms\n",
            "step 803470 | loss 2.3586 | step time 51.47ms\n",
            "step 803480 | loss 2.2958 | step time 68.43ms\n",
            "step 803490 | loss 2.7672 | step time 57.60ms\n",
            "step 803500 | loss 2.3164 | step time 55.97ms\n",
            "step 803500 train loss: 2.447449207305908 test loss: 2.49111270904541\n",
            "step 803510 | loss 2.5569 | step time 60.65ms\n",
            "step 803520 | loss 2.4317 | step time 58.39ms\n",
            "step 803530 | loss 2.4523 | step time 38.35ms\n",
            "step 803540 | loss 2.2948 | step time 39.21ms\n",
            "step 803550 | loss 2.2979 | step time 37.62ms\n",
            "step 803560 | loss 2.4655 | step time 39.15ms\n",
            "step 803570 | loss 2.3707 | step time 37.57ms\n",
            "step 803580 | loss 2.5204 | step time 37.37ms\n",
            "step 803590 | loss 2.5594 | step time 44.60ms\n",
            "step 803600 | loss 2.5390 | step time 40.73ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "whynobvt\n",
            "hollurtzelem\n",
            "Carley404\n",
            "mymodman92\n",
            "gwatcredder\n",
            "Carthar\n",
            "bbb-9kwan\n",
            "relegantcolbeverythi\n",
            "SammerTec\n",
            "Coolegik\n",
            "--------------------------------------------------------------------------------\n",
            "step 803610 | loss 2.4988 | step time 39.95ms\n",
            "step 803620 | loss 2.6663 | step time 38.92ms\n",
            "step 803630 | loss 2.3535 | step time 43.41ms\n",
            "step 803640 | loss 2.6216 | step time 39.88ms\n",
            "step 803650 | loss 2.5279 | step time 38.13ms\n",
            "step 803660 | loss 2.4600 | step time 44.82ms\n",
            "step 803670 | loss 2.3666 | step time 39.15ms\n",
            "step 803680 | loss 2.4785 | step time 37.79ms\n",
            "step 803690 | loss 2.3179 | step time 38.15ms\n",
            "step 803700 | loss 2.4176 | step time 52.30ms\n",
            "step 803710 | loss 2.4310 | step time 43.07ms\n",
            "step 803720 | loss 2.5557 | step time 38.98ms\n",
            "step 803730 | loss 2.4024 | step time 39.92ms\n",
            "step 803740 | loss 2.4126 | step time 38.79ms\n",
            "step 803750 | loss 2.5096 | step time 38.32ms\n",
            "step 803760 | loss 2.4296 | step time 61.78ms\n",
            "step 803770 | loss 2.2495 | step time 53.17ms\n",
            "step 803780 | loss 2.3862 | step time 58.19ms\n",
            "step 803790 | loss 2.3597 | step time 56.82ms\n",
            "step 803800 | loss 2.3576 | step time 59.78ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ItsNOwCata\n",
            "pacojacoli\n",
            "ihatemywhut\n",
            "velie_The_Gamer\n",
            "mostlyjuice\n",
            "cnafiewescal\n",
            "Marehenn\n",
            "axqiodie\n",
            "lilaboyold\n",
            "Menamemore\n",
            "--------------------------------------------------------------------------------\n",
            "step 803810 | loss 2.2154 | step time 75.73ms\n",
            "step 803820 | loss 2.5044 | step time 43.41ms\n",
            "step 803830 | loss 2.5187 | step time 40.19ms\n",
            "step 803840 | loss 2.6339 | step time 40.65ms\n",
            "step 803850 | loss 2.3425 | step time 39.99ms\n",
            "step 803860 | loss 2.4571 | step time 43.37ms\n",
            "step 803870 | loss 2.5003 | step time 52.41ms\n",
            "step 803880 | loss 2.3677 | step time 38.96ms\n",
            "step 803890 | loss 2.5366 | step time 40.79ms\n",
            "step 803900 | loss 2.5958 | step time 38.88ms\n",
            "step 803910 | loss 2.5023 | step time 38.69ms\n",
            "step 803920 | loss 2.4104 | step time 41.32ms\n",
            "step 803930 | loss 2.4100 | step time 37.13ms\n",
            "step 803940 | loss 2.3253 | step time 38.32ms\n",
            "step 803950 | loss 2.4744 | step time 37.84ms\n",
            "step 803960 | loss 2.5560 | step time 36.54ms\n",
            "step 803970 | loss 2.5065 | step time 35.62ms\n",
            "step 803980 | loss 2.4752 | step time 38.63ms\n",
            "step 803990 | loss 2.5236 | step time 39.08ms\n",
            "step 804000 | loss 2.4206 | step time 37.80ms\n",
            "step 804000 train loss: 2.4271240234375 test loss: 2.4809792041778564\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "hujampuc\n",
            "Templayern\n",
            "memeacardthrowaway33\n",
            "DarthWoodS\n",
            "ifukeiwaboo\n",
            "ITimplatyZz\n",
            "chudboy\n",
            "Beaveh1neverball\n",
            "Jojoepias\n",
            "melima\n",
            "--------------------------------------------------------------------------------\n",
            "step 804010 | loss 2.4385 | step time 37.91ms\n",
            "step 804020 | loss 2.4427 | step time 36.17ms\n",
            "step 804030 | loss 2.3851 | step time 52.64ms\n",
            "step 804040 | loss 2.4953 | step time 51.68ms\n",
            "step 804050 | loss 2.4912 | step time 52.62ms\n",
            "step 804060 | loss 2.4157 | step time 57.40ms\n",
            "step 804070 | loss 2.4356 | step time 58.74ms\n",
            "step 804080 | loss 2.3905 | step time 66.26ms\n",
            "step 804090 | loss 2.3089 | step time 68.44ms\n",
            "step 804100 | loss 2.3345 | step time 61.20ms\n",
            "step 804110 | loss 2.3008 | step time 67.29ms\n",
            "step 804120 | loss 2.5574 | step time 37.66ms\n",
            "step 804130 | loss 2.3579 | step time 40.98ms\n",
            "step 804140 | loss 2.4203 | step time 36.87ms\n",
            "step 804150 | loss 2.5065 | step time 41.45ms\n",
            "step 804160 | loss 2.3883 | step time 41.62ms\n",
            "step 804170 | loss 2.4019 | step time 35.72ms\n",
            "step 804180 | loss 2.3797 | step time 37.17ms\n",
            "step 804190 | loss 2.3245 | step time 37.63ms\n",
            "step 804200 | loss 2.4980 | step time 40.39ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "walrowndog\n",
            "NextLonelyTheHaver\n",
            "Incideigirldsr\n",
            "cr55789\n",
            "zanecimara\n",
            "GallyingTheYouKu\n",
            "kmyr4\n",
            "nomatrick121\n",
            "NGHToear\n",
            "Desarr\n",
            "--------------------------------------------------------------------------------\n",
            "step 804210 | loss 2.5805 | step time 39.73ms\n",
            "step 804220 | loss 2.4564 | step time 38.67ms\n",
            "step 804230 | loss 2.3260 | step time 38.78ms\n",
            "step 804240 | loss 2.4849 | step time 36.53ms\n",
            "step 804250 | loss 2.2160 | step time 38.39ms\n",
            "step 804260 | loss 2.5331 | step time 37.90ms\n",
            "step 804270 | loss 2.4564 | step time 37.23ms\n",
            "step 804280 | loss 2.6037 | step time 40.20ms\n",
            "step 804290 | loss 2.3702 | step time 36.95ms\n",
            "step 804300 | loss 2.3584 | step time 40.70ms\n",
            "step 804310 | loss 2.4059 | step time 38.81ms\n",
            "step 804320 | loss 2.6395 | step time 39.50ms\n",
            "step 804330 | loss 2.6163 | step time 37.47ms\n",
            "step 804340 | loss 2.4482 | step time 37.84ms\n",
            "step 804350 | loss 2.4255 | step time 36.01ms\n",
            "step 804360 | loss 2.6240 | step time 61.62ms\n",
            "step 804370 | loss 2.4444 | step time 51.15ms\n",
            "step 804380 | loss 2.3597 | step time 49.22ms\n",
            "step 804390 | loss 2.2968 | step time 63.99ms\n",
            "step 804400 | loss 2.4010 | step time 56.95ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "cammyreuchuak\n",
            "Wissy2016\n",
            "t4ng0bliob81\n",
            "ForStreakRFA\n",
            "Sadie2741\n",
            "vifix\n",
            "Optic_Popcorn\n",
            "SclaryKappa\n",
            "ShowMetama\n",
            "JebasMCK\n",
            "--------------------------------------------------------------------------------\n",
            "step 804410 | loss 2.5262 | step time 66.74ms\n",
            "step 804420 | loss 2.4442 | step time 53.94ms\n",
            "step 804430 | loss 2.4155 | step time 42.16ms\n",
            "step 804440 | loss 2.5475 | step time 38.44ms\n",
            "step 804450 | loss 2.6767 | step time 40.58ms\n",
            "step 804460 | loss 2.4307 | step time 39.39ms\n",
            "step 804470 | loss 2.3845 | step time 36.45ms\n",
            "step 804480 | loss 2.4559 | step time 38.23ms\n",
            "step 804490 | loss 2.3462 | step time 37.12ms\n",
            "step 804500 | loss 2.4643 | step time 39.45ms\n",
            "step 804500 train loss: 2.4702906608581543 test loss: 2.482168436050415\n",
            "step 804510 | loss 2.3328 | step time 37.42ms\n",
            "step 804520 | loss 2.3279 | step time 37.54ms\n",
            "step 804530 | loss 2.5636 | step time 37.60ms\n",
            "step 804540 | loss 2.4506 | step time 49.36ms\n",
            "step 804550 | loss 2.5588 | step time 37.75ms\n",
            "step 804560 | loss 2.2976 | step time 36.12ms\n",
            "step 804570 | loss 2.3439 | step time 38.71ms\n",
            "step 804580 | loss 2.4181 | step time 41.32ms\n",
            "step 804590 | loss 2.5750 | step time 38.69ms\n",
            "step 804600 | loss 2.4172 | step time 42.71ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Userturan\n",
            "fogeostrrouva\n",
            "SuperGuenChip\n",
            "Qkakyee\n",
            "aMula\n",
            "LoveTulipto\n",
            "shittyphotoony\n",
            "bowmonkey\n",
            "leslopssorvation\n",
            "sprucyjoe44\n",
            "--------------------------------------------------------------------------------\n",
            "step 804610 | loss 2.3428 | step time 38.90ms\n",
            "step 804620 | loss 2.4963 | step time 40.32ms\n",
            "step 804630 | loss 2.3468 | step time 52.04ms\n",
            "step 804640 | loss 2.4195 | step time 40.86ms\n",
            "step 804650 | loss 2.3560 | step time 56.70ms\n",
            "step 804660 | loss 2.5289 | step time 52.86ms\n",
            "step 804670 | loss 2.4120 | step time 52.49ms\n",
            "step 804680 | loss 2.4603 | step time 57.40ms\n",
            "step 804690 | loss 2.4945 | step time 69.14ms\n",
            "step 804700 | loss 2.4717 | step time 58.27ms\n",
            "step 804710 | loss 2.5797 | step time 58.65ms\n",
            "step 804720 | loss 2.4262 | step time 57.04ms\n",
            "step 804730 | loss 2.5065 | step time 58.18ms\n",
            "step 804740 | loss 2.3892 | step time 39.64ms\n",
            "step 804750 | loss 2.6101 | step time 42.84ms\n",
            "step 804760 | loss 2.4167 | step time 38.40ms\n",
            "step 804770 | loss 2.3011 | step time 39.15ms\n",
            "step 804780 | loss 2.5537 | step time 38.90ms\n",
            "step 804790 | loss 2.4097 | step time 42.68ms\n",
            "step 804800 | loss 2.6703 | step time 39.30ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ShadowHank\n",
            "6rlong\n",
            "qthonquestaff\n",
            "sfeckerwit32\n",
            "larryadirollink\n",
            "Holden_Indo_Your_Sle\n",
            "_hudle\n",
            "pu0nsea1y2\n",
            "Sk0otsik\n",
            "joesiss\n",
            "--------------------------------------------------------------------------------\n",
            "step 804810 | loss 2.5412 | step time 41.79ms\n",
            "step 804820 | loss 2.2840 | step time 40.01ms\n",
            "step 804830 | loss 2.3792 | step time 39.72ms\n",
            "step 804840 | loss 2.3313 | step time 45.82ms\n",
            "step 804850 | loss 2.4731 | step time 40.53ms\n",
            "step 804860 | loss 2.4353 | step time 37.00ms\n",
            "step 804870 | loss 2.3361 | step time 40.83ms\n",
            "step 804880 | loss 2.5338 | step time 39.83ms\n",
            "step 804890 | loss 2.3449 | step time 41.75ms\n",
            "step 804900 | loss 2.2164 | step time 44.86ms\n",
            "step 804910 | loss 2.5878 | step time 42.74ms\n",
            "step 804920 | loss 2.3174 | step time 38.47ms\n",
            "step 804930 | loss 2.4517 | step time 41.52ms\n",
            "step 804940 | loss 2.3396 | step time 42.14ms\n",
            "step 804950 | loss 2.4863 | step time 43.56ms\n",
            "step 804960 | loss 2.5381 | step time 60.11ms\n",
            "step 804970 | loss 2.4636 | step time 56.01ms\n",
            "step 804980 | loss 2.4561 | step time 51.25ms\n",
            "step 804990 | loss 2.4757 | step time 60.84ms\n",
            "step 805000 | loss 2.4512 | step time 63.18ms\n",
            "step 805000 train loss: 2.4484643936157227 test loss: 2.479945659637451\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "FrTcLAmbo\n",
            "StHeWeB\n",
            "ST3N\n",
            "GRizzletro\n",
            "stlandvipus\n",
            "PerioUniqarious\n",
            "Sickfold\n",
            "AfturdFatManDaButTha\n",
            "mount_theindustred\n",
            "bushy21\n",
            "--------------------------------------------------------------------------------\n",
            "step 805010 | loss 2.4040 | step time 53.68ms\n",
            "step 805020 | loss 2.3623 | step time 39.56ms\n",
            "step 805030 | loss 2.3855 | step time 42.81ms\n",
            "step 805040 | loss 2.5046 | step time 37.67ms\n",
            "step 805050 | loss 2.4319 | step time 39.16ms\n",
            "step 805060 | loss 2.4514 | step time 43.77ms\n",
            "step 805070 | loss 2.4985 | step time 39.72ms\n",
            "step 805080 | loss 2.2874 | step time 38.42ms\n",
            "step 805090 | loss 2.5954 | step time 40.71ms\n",
            "step 805100 | loss 2.5418 | step time 40.31ms\n",
            "step 805110 | loss 2.3433 | step time 38.65ms\n",
            "step 805120 | loss 2.4888 | step time 37.07ms\n",
            "step 805130 | loss 2.4895 | step time 37.72ms\n",
            "step 805140 | loss 2.4592 | step time 40.14ms\n",
            "step 805150 | loss 2.5044 | step time 40.06ms\n",
            "step 805160 | loss 2.4989 | step time 39.14ms\n",
            "step 805170 | loss 2.5287 | step time 40.29ms\n",
            "step 805180 | loss 2.5810 | step time 41.64ms\n",
            "step 805190 | loss 2.3063 | step time 36.38ms\n",
            "step 805200 | loss 2.4430 | step time 39.23ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ryandaccount4149\n",
            "JamesDefinit\n",
            "glysticle\n",
            "itspintheonight\n",
            "Sagabi49\n",
            "Nox3lla\n",
            "goal_so_bades\n",
            "Eldermer\n",
            "MrMicnureSands\n",
            "Lolk04\n",
            "--------------------------------------------------------------------------------\n",
            "step 805210 | loss 2.3373 | step time 39.02ms\n",
            "step 805220 | loss 2.3000 | step time 50.51ms\n",
            "step 805230 | loss 2.3053 | step time 40.69ms\n",
            "step 805240 | loss 2.3044 | step time 53.32ms\n",
            "step 805250 | loss 2.4087 | step time 56.71ms\n",
            "step 805260 | loss 2.5037 | step time 55.17ms\n",
            "step 805270 | loss 2.4021 | step time 68.99ms\n",
            "step 805280 | loss 2.4566 | step time 58.03ms\n",
            "step 805290 | loss 2.5873 | step time 60.25ms\n",
            "step 805300 | loss 2.2766 | step time 58.55ms\n",
            "step 805310 | loss 2.4684 | step time 61.06ms\n",
            "step 805320 | loss 2.3556 | step time 59.88ms\n",
            "step 805330 | loss 2.4432 | step time 59.53ms\n",
            "step 805340 | loss 2.4913 | step time 40.07ms\n",
            "step 805350 | loss 2.4874 | step time 37.90ms\n",
            "step 805360 | loss 2.2927 | step time 42.65ms\n",
            "step 805370 | loss 2.3603 | step time 37.69ms\n",
            "step 805380 | loss 2.4392 | step time 37.29ms\n",
            "step 805390 | loss 2.3367 | step time 39.20ms\n",
            "step 805400 | loss 2.4202 | step time 39.34ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "lookoapprone\n",
            "ichanohan\n",
            "MiamCourtnel\n",
            "maoabana\n",
            "BSDLEDPorn\n",
            "tedthow\n",
            "UWorkATruth\n",
            "Breaker2003\n",
            "tvc78935\n",
            "kruidado1\n",
            "--------------------------------------------------------------------------------\n",
            "step 805410 | loss 2.4299 | step time 38.44ms\n",
            "step 805420 | loss 2.2491 | step time 40.09ms\n",
            "step 805430 | loss 2.4602 | step time 43.44ms\n",
            "step 805440 | loss 2.5115 | step time 38.33ms\n",
            "step 805450 | loss 2.6742 | step time 40.06ms\n",
            "step 805460 | loss 2.4466 | step time 37.92ms\n",
            "step 805470 | loss 2.3891 | step time 50.06ms\n",
            "step 805480 | loss 2.4743 | step time 39.28ms\n",
            "step 805490 | loss 2.4428 | step time 37.62ms\n",
            "step 805500 | loss 2.4614 | step time 43.32ms\n",
            "step 805500 train loss: 2.4768714904785156 test loss: 2.482874631881714\n",
            "step 805510 | loss 2.3807 | step time 41.99ms\n",
            "step 805520 | loss 2.5804 | step time 38.28ms\n",
            "step 805530 | loss 2.4917 | step time 37.69ms\n",
            "step 805540 | loss 2.2965 | step time 37.18ms\n",
            "step 805550 | loss 2.3812 | step time 58.81ms\n",
            "step 805560 | loss 2.5536 | step time 53.27ms\n",
            "step 805570 | loss 2.4962 | step time 53.97ms\n",
            "step 805580 | loss 2.4649 | step time 58.78ms\n",
            "step 805590 | loss 2.5884 | step time 57.82ms\n",
            "step 805600 | loss 2.4288 | step time 55.90ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "fallbunny97\n",
            "jewoise1982\n",
            "c-da_too\n",
            "tylosce020\n",
            "chadm\n",
            "ShotYevis\n",
            "wayallask\n",
            "nerfmen1\n",
            "Tyshumo\n",
            "i_bot_dishop\n",
            "--------------------------------------------------------------------------------\n",
            "step 805610 | loss 2.3418 | step time 42.45ms\n",
            "step 805620 | loss 2.2837 | step time 39.10ms\n",
            "step 805630 | loss 2.4032 | step time 37.90ms\n",
            "step 805640 | loss 2.4445 | step time 37.43ms\n",
            "step 805650 | loss 2.6734 | step time 39.23ms\n",
            "step 805660 | loss 2.3623 | step time 38.59ms\n",
            "step 805670 | loss 2.4627 | step time 38.95ms\n",
            "step 805680 | loss 2.4396 | step time 37.49ms\n",
            "step 805690 | loss 2.5364 | step time 44.00ms\n",
            "step 805700 | loss 2.4619 | step time 38.02ms\n",
            "step 805710 | loss 2.6369 | step time 38.09ms\n",
            "step 805720 | loss 2.4403 | step time 38.38ms\n",
            "step 805730 | loss 2.3022 | step time 50.42ms\n",
            "step 805740 | loss 2.5810 | step time 40.24ms\n",
            "step 805750 | loss 2.5046 | step time 45.03ms\n",
            "step 805760 | loss 2.3334 | step time 37.65ms\n",
            "step 805770 | loss 2.3688 | step time 37.11ms\n",
            "step 805780 | loss 2.5439 | step time 38.69ms\n",
            "step 805790 | loss 2.3996 | step time 42.06ms\n",
            "step 805800 | loss 2.2558 | step time 45.52ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "bcjllkw\n",
            "Yalgho\n",
            "denveilde56\n",
            "tylerhockey\n",
            "shawncoltooliresin\n",
            "Draco77\n",
            "teaestart\n",
            "dfsh\n",
            "tonlovedschazentale\n",
            "Bled123\n",
            "--------------------------------------------------------------------------------\n",
            "step 805810 | loss 2.4609 | step time 39.99ms\n",
            "step 805820 | loss 2.5112 | step time 39.19ms\n",
            "step 805830 | loss 2.5408 | step time 37.08ms\n",
            "step 805840 | loss 2.4076 | step time 38.15ms\n",
            "step 805850 | loss 2.4920 | step time 64.18ms\n",
            "step 805860 | loss 2.4809 | step time 56.62ms\n",
            "step 805870 | loss 2.3669 | step time 50.66ms\n",
            "step 805880 | loss 2.4306 | step time 74.84ms\n",
            "step 805890 | loss 2.6623 | step time 54.75ms\n",
            "step 805900 | loss 2.4370 | step time 61.16ms\n",
            "step 805910 | loss 2.3564 | step time 55.53ms\n",
            "step 805920 | loss 2.5501 | step time 51.36ms\n",
            "step 805930 | loss 2.4817 | step time 38.65ms\n",
            "step 805940 | loss 2.3715 | step time 37.68ms\n",
            "step 805950 | loss 2.5114 | step time 37.13ms\n",
            "step 805960 | loss 2.3543 | step time 37.84ms\n",
            "step 805970 | loss 2.5425 | step time 38.67ms\n",
            "step 805980 | loss 2.3143 | step time 37.37ms\n",
            "step 805990 | loss 2.4607 | step time 38.34ms\n",
            "step 806000 | loss 2.4753 | step time 38.67ms\n",
            "step 806000 train loss: 2.454437494277954 test loss: 2.4817609786987305\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "taerabenjoe\n",
            "Beingerfuck\n",
            "obsnipper\n",
            "nwagasandik\n",
            "HarleyTrashplashed\n",
            "dmfe\n",
            "TGroweryButts\n",
            "nalh2\n",
            "CopperConian-\n",
            "_aka17\n",
            "--------------------------------------------------------------------------------\n",
            "step 806010 | loss 2.4797 | step time 35.90ms\n",
            "step 806020 | loss 2.4726 | step time 37.04ms\n",
            "step 806030 | loss 2.5524 | step time 37.26ms\n",
            "step 806040 | loss 2.3700 | step time 36.76ms\n",
            "step 806050 | loss 2.6864 | step time 35.55ms\n",
            "step 806060 | loss 2.3418 | step time 48.25ms\n",
            "step 806070 | loss 2.3728 | step time 37.84ms\n",
            "step 806080 | loss 2.4520 | step time 42.52ms\n",
            "step 806090 | loss 2.3727 | step time 35.59ms\n",
            "step 806100 | loss 2.4452 | step time 42.24ms\n",
            "step 806110 | loss 2.3575 | step time 38.38ms\n",
            "step 806120 | loss 2.4989 | step time 36.54ms\n",
            "step 806130 | loss 2.4383 | step time 40.09ms\n",
            "step 806140 | loss 2.5085 | step time 42.63ms\n",
            "step 806150 | loss 2.5442 | step time 60.63ms\n",
            "step 806160 | loss 2.3152 | step time 54.88ms\n",
            "step 806170 | loss 2.4297 | step time 58.82ms\n",
            "step 806180 | loss 2.4331 | step time 55.29ms\n",
            "step 806190 | loss 2.4419 | step time 56.76ms\n",
            "step 806200 | loss 2.5398 | step time 57.56ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Marhaurv\n",
            "IAmHmurbastedAnyte\n",
            "Snakeybead\n",
            "mitchel1\n",
            "HasDaDianGoodo\n",
            "Infinitemote\n",
            "sthrowaway07\n",
            "LeftSherian\n",
            "camerons7\n",
            "crypthbybaronidal\n",
            "--------------------------------------------------------------------------------\n",
            "step 806210 | loss 2.3681 | step time 57.02ms\n",
            "step 806220 | loss 2.4404 | step time 37.77ms\n",
            "step 806230 | loss 2.4759 | step time 42.60ms\n",
            "step 806240 | loss 2.6474 | step time 52.89ms\n",
            "step 806250 | loss 2.4317 | step time 36.73ms\n",
            "step 806260 | loss 2.3765 | step time 37.25ms\n",
            "step 806270 | loss 2.4961 | step time 38.80ms\n",
            "step 806280 | loss 2.3394 | step time 38.95ms\n",
            "step 806290 | loss 2.3637 | step time 39.29ms\n",
            "step 806300 | loss 2.5424 | step time 38.78ms\n",
            "step 806310 | loss 2.4587 | step time 39.05ms\n",
            "step 806320 | loss 2.4151 | step time 36.85ms\n",
            "step 806330 | loss 2.5067 | step time 35.78ms\n",
            "step 806340 | loss 2.3366 | step time 36.30ms\n",
            "step 806350 | loss 2.5529 | step time 36.87ms\n",
            "step 806360 | loss 2.5535 | step time 36.04ms\n",
            "step 806370 | loss 2.3420 | step time 38.12ms\n",
            "step 806380 | loss 2.4090 | step time 37.95ms\n",
            "step 806390 | loss 2.3727 | step time 38.18ms\n",
            "step 806400 | loss 2.5287 | step time 37.84ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Carey_Swanter\n",
            "jthandeov2\n",
            "ireentherichard\n",
            "asceneusclosednarrea\n",
            "mightbond2334\n",
            "Neana\n",
            "hctoecolp\n",
            "2ungas1\n",
            "PlovidEl\n",
            "Rthemo\n",
            "--------------------------------------------------------------------------------\n",
            "step 806410 | loss 2.4843 | step time 36.61ms\n",
            "step 806420 | loss 2.5286 | step time 39.29ms\n",
            "step 806430 | loss 2.5571 | step time 43.21ms\n",
            "step 806440 | loss 2.4464 | step time 38.69ms\n",
            "step 806450 | loss 2.5502 | step time 54.93ms\n",
            "step 806460 | loss 2.4805 | step time 53.71ms\n",
            "step 806470 | loss 2.4049 | step time 46.84ms\n",
            "step 806480 | loss 2.4782 | step time 58.27ms\n",
            "step 806490 | loss 2.4553 | step time 61.40ms\n",
            "step 806500 | loss 2.2430 | step time 55.02ms\n",
            "step 806500 train loss: 2.4496514797210693 test loss: 2.4755325317382812\n",
            "test loss 2.4755325317382812 is the best so far, saving model to users/model.pt\n",
            "step 806510 | loss 2.5006 | step time 63.80ms\n",
            "step 806520 | loss 2.4098 | step time 59.97ms\n",
            "step 806530 | loss 2.3503 | step time 36.03ms\n",
            "step 806540 | loss 2.4632 | step time 37.93ms\n",
            "step 806550 | loss 2.4540 | step time 39.27ms\n",
            "step 806560 | loss 2.3873 | step time 37.32ms\n",
            "step 806570 | loss 2.6064 | step time 43.79ms\n",
            "step 806580 | loss 2.4325 | step time 39.54ms\n",
            "step 806590 | loss 2.6906 | step time 39.73ms\n",
            "step 806600 | loss 2.4143 | step time 42.14ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Tikkin212\n",
            "Awthrowawayd\n",
            "exigtem\n",
            "litterrtcd\n",
            "terpys\n",
            "litedeur\n",
            "AjoJane\n",
            "Sagamis\n",
            "QJeather\n",
            "icnewlordmash\n",
            "--------------------------------------------------------------------------------\n",
            "step 806610 | loss 2.2684 | step time 37.75ms\n",
            "step 806620 | loss 2.4347 | step time 50.14ms\n",
            "step 806630 | loss 2.3659 | step time 39.37ms\n",
            "step 806640 | loss 2.2536 | step time 39.65ms\n",
            "step 806650 | loss 2.5356 | step time 35.65ms\n",
            "step 806660 | loss 2.3420 | step time 38.80ms\n",
            "step 806670 | loss 2.6922 | step time 41.34ms\n",
            "step 806680 | loss 2.5419 | step time 37.61ms\n",
            "step 806690 | loss 2.3725 | step time 38.03ms\n",
            "step 806700 | loss 2.3826 | step time 36.78ms\n",
            "step 806710 | loss 2.4377 | step time 34.49ms\n",
            "step 806720 | loss 2.5404 | step time 39.95ms\n",
            "step 806730 | loss 2.4455 | step time 36.44ms\n",
            "step 806740 | loss 2.3976 | step time 41.18ms\n",
            "step 806750 | loss 2.3450 | step time 47.24ms\n",
            "step 806760 | loss 2.5172 | step time 60.77ms\n",
            "step 806770 | loss 2.5089 | step time 69.18ms\n",
            "step 806780 | loss 2.4934 | step time 48.26ms\n",
            "step 806790 | loss 2.4862 | step time 64.83ms\n",
            "step 806800 | loss 2.2943 | step time 57.44ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "lost0bodaanddoop\n",
            "KorkoNom\n",
            "Zetroo\n",
            "Kiickyroo\n",
            "non-me-so-makara\n",
            "carlett81b\n",
            "Cpcander\n",
            "Dixe_Pour_15\n",
            "jakeonbox\n",
            "GRRYSTRONE\n",
            "--------------------------------------------------------------------------------\n",
            "step 806810 | loss 2.4323 | step time 59.82ms\n",
            "step 806820 | loss 2.3616 | step time 54.23ms\n",
            "step 806830 | loss 2.4054 | step time 59.47ms\n",
            "step 806840 | loss 2.4371 | step time 42.34ms\n",
            "step 806850 | loss 2.4611 | step time 37.26ms\n",
            "step 806860 | loss 2.3597 | step time 37.38ms\n",
            "step 806870 | loss 2.3186 | step time 40.84ms\n",
            "step 806880 | loss 2.3471 | step time 42.67ms\n",
            "step 806890 | loss 2.3885 | step time 40.01ms\n",
            "step 806900 | loss 2.6331 | step time 40.11ms\n",
            "step 806910 | loss 2.6950 | step time 40.61ms\n",
            "step 806920 | loss 2.2818 | step time 36.72ms\n",
            "step 806930 | loss 2.3734 | step time 35.67ms\n",
            "step 806940 | loss 2.4782 | step time 41.56ms\n",
            "step 806950 | loss 2.3978 | step time 39.78ms\n",
            "step 806960 | loss 2.5073 | step time 38.49ms\n",
            "step 806970 | loss 2.3671 | step time 38.74ms\n",
            "step 806980 | loss 2.4443 | step time 49.97ms\n",
            "step 806990 | loss 2.4151 | step time 41.10ms\n",
            "step 807000 | loss 2.4749 | step time 38.22ms\n",
            "step 807000 train loss: 2.456468343734741 test loss: 2.479792356491089\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "crash111814\n",
            "Bluestaym\n",
            "lynnlias\n",
            "deadbruno78\n",
            "Ksmoffenryan\n",
            "FreeshMez\n",
            "kodamirklez\n",
            "Julianvece\n",
            "kezokkmiss\n",
            "Momer\n",
            "--------------------------------------------------------------------------------\n",
            "step 807010 | loss 2.3351 | step time 38.89ms\n",
            "step 807020 | loss 2.7292 | step time 37.92ms\n",
            "step 807030 | loss 2.4228 | step time 40.21ms\n",
            "step 807040 | loss 2.4380 | step time 40.49ms\n",
            "step 807050 | loss 2.5022 | step time 57.54ms\n",
            "step 807060 | loss 2.6357 | step time 49.38ms\n",
            "step 807070 | loss 2.4840 | step time 53.53ms\n",
            "step 807080 | loss 2.5232 | step time 55.06ms\n",
            "step 807090 | loss 2.3696 | step time 56.48ms\n",
            "step 807100 | loss 2.3500 | step time 53.31ms\n",
            "step 807110 | loss 2.6322 | step time 52.66ms\n",
            "step 807120 | loss 2.3813 | step time 54.10ms\n",
            "step 807130 | loss 2.4262 | step time 55.39ms\n",
            "step 807140 | loss 2.6423 | step time 57.81ms\n",
            "step 807150 | loss 2.2987 | step time 40.20ms\n",
            "step 807160 | loss 2.4335 | step time 38.56ms\n",
            "step 807170 | loss 2.5400 | step time 39.00ms\n",
            "step 807180 | loss 2.2944 | step time 38.21ms\n",
            "step 807190 | loss 2.5966 | step time 45.05ms\n",
            "step 807200 | loss 2.4142 | step time 37.86ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "firthbummy99\n",
            "hilzoberchie\n",
            "Modsmatty\n",
            "Salonium\n",
            "aremeweds32\n",
            "Hummo0rees\n",
            "IfWallacia\n",
            "RobertM\n",
            "iwaardjared03\n",
            "throwwickr\n",
            "--------------------------------------------------------------------------------\n",
            "step 807210 | loss 2.4127 | step time 36.64ms\n",
            "step 807220 | loss 2.4202 | step time 39.26ms\n",
            "step 807230 | loss 2.4082 | step time 45.11ms\n",
            "step 807240 | loss 2.4960 | step time 38.75ms\n",
            "step 807250 | loss 2.4896 | step time 37.63ms\n",
            "step 807260 | loss 2.3598 | step time 35.67ms\n",
            "step 807270 | loss 2.4016 | step time 38.03ms\n",
            "step 807280 | loss 2.3474 | step time 36.36ms\n",
            "step 807290 | loss 2.7074 | step time 38.30ms\n",
            "step 807300 | loss 2.4763 | step time 41.59ms\n",
            "step 807310 | loss 2.4592 | step time 37.56ms\n",
            "step 807320 | loss 2.4727 | step time 36.53ms\n",
            "step 807330 | loss 2.5078 | step time 35.48ms\n",
            "step 807340 | loss 2.3227 | step time 36.30ms\n",
            "step 807350 | loss 2.2527 | step time 34.77ms\n",
            "step 807360 | loss 2.3777 | step time 44.67ms\n",
            "step 807370 | loss 2.3808 | step time 53.09ms\n",
            "step 807380 | loss 2.3296 | step time 62.17ms\n",
            "step 807390 | loss 2.5335 | step time 53.73ms\n",
            "step 807400 | loss 2.6579 | step time 53.33ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "misherus\n",
            "flexistileafirt\n",
            "herpofailium14\n",
            "jakeqs\n",
            "Carbolatilltet21\n",
            "Reaper4447\n",
            "mcster6\n",
            "fugokitio\n",
            "Metawtor1\n",
            "Septyschalang\n",
            "--------------------------------------------------------------------------------\n",
            "step 807410 | loss 2.3748 | step time 60.17ms\n",
            "step 807420 | loss 2.2793 | step time 66.37ms\n",
            "step 807430 | loss 2.4669 | step time 56.72ms\n",
            "step 807440 | loss 2.3300 | step time 56.07ms\n",
            "step 807450 | loss 2.2795 | step time 62.02ms\n",
            "step 807460 | loss 2.4626 | step time 49.43ms\n",
            "step 807470 | loss 2.3018 | step time 35.49ms\n",
            "step 807480 | loss 2.4796 | step time 36.29ms\n",
            "step 807490 | loss 2.3709 | step time 39.54ms\n",
            "step 807500 | loss 2.4233 | step time 37.27ms\n",
            "step 807500 train loss: 2.455679416656494 test loss: 2.4778025150299072\n",
            "step 807510 | loss 2.4514 | step time 37.45ms\n",
            "step 807520 | loss 2.5700 | step time 36.89ms\n",
            "step 807530 | loss 2.4844 | step time 38.13ms\n",
            "step 807540 | loss 2.4587 | step time 38.37ms\n",
            "step 807550 | loss 2.4396 | step time 39.22ms\n",
            "step 807560 | loss 2.5508 | step time 37.74ms\n",
            "step 807570 | loss 2.4246 | step time 36.58ms\n",
            "step 807580 | loss 2.2669 | step time 34.25ms\n",
            "step 807590 | loss 2.4743 | step time 34.03ms\n",
            "step 807600 | loss 2.4519 | step time 39.95ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "arebuttaboude\n",
            "the_chicky\n",
            "taco--almortus\n",
            "Jboriful123\n",
            "minge_made\n",
            "Hagaladley\n",
            "nawotto\n",
            "drinnothereonepa\n",
            "nobodakinga\n",
            "GaRoBasica\n",
            "--------------------------------------------------------------------------------\n",
            "step 807610 | loss 2.2392 | step time 43.41ms\n",
            "step 807620 | loss 2.4585 | step time 37.14ms\n",
            "step 807630 | loss 2.5442 | step time 36.97ms\n",
            "step 807640 | loss 2.3370 | step time 41.28ms\n",
            "step 807650 | loss 2.5008 | step time 40.44ms\n",
            "step 807660 | loss 2.2844 | step time 38.86ms\n",
            "step 807670 | loss 2.5063 | step time 39.34ms\n",
            "step 807680 | loss 2.3767 | step time 62.26ms\n",
            "step 807690 | loss 2.3016 | step time 48.67ms\n",
            "step 807700 | loss 2.4463 | step time 51.93ms\n",
            "step 807710 | loss 2.3027 | step time 52.27ms\n",
            "step 807720 | loss 2.5663 | step time 53.05ms\n",
            "step 807730 | loss 2.6073 | step time 55.43ms\n",
            "step 807740 | loss 2.1771 | step time 56.89ms\n",
            "step 807750 | loss 2.5403 | step time 63.59ms\n",
            "step 807760 | loss 2.4324 | step time 51.43ms\n",
            "step 807770 | loss 2.5770 | step time 37.04ms\n",
            "step 807780 | loss 2.3056 | step time 36.36ms\n",
            "step 807790 | loss 2.4186 | step time 35.45ms\n",
            "step 807800 | loss 2.4573 | step time 38.56ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "shoutler_\n",
            "heebombeR8\n",
            "StrongGrey\n",
            "funge21925\n",
            "rrc5\n",
            "dysh\n",
            "BKW_Carles\n",
            "kimsonaituou\n",
            "AlexJays\n",
            "gibble30\n",
            "--------------------------------------------------------------------------------\n",
            "step 807810 | loss 2.5711 | step time 37.99ms\n",
            "step 807820 | loss 2.3548 | step time 36.36ms\n",
            "step 807830 | loss 2.3726 | step time 35.97ms\n",
            "step 807840 | loss 2.5232 | step time 35.77ms\n",
            "step 807850 | loss 2.6596 | step time 40.42ms\n",
            "step 807860 | loss 2.4344 | step time 36.86ms\n",
            "step 807870 | loss 2.4636 | step time 38.94ms\n",
            "step 807880 | loss 2.5701 | step time 39.33ms\n",
            "step 807890 | loss 2.4375 | step time 40.40ms\n",
            "step 807900 | loss 2.5399 | step time 40.01ms\n",
            "step 807910 | loss 2.5096 | step time 41.45ms\n",
            "step 807920 | loss 2.2209 | step time 41.53ms\n",
            "step 807930 | loss 2.3816 | step time 38.39ms\n",
            "step 807940 | loss 2.5074 | step time 38.60ms\n",
            "step 807950 | loss 2.4254 | step time 39.11ms\n",
            "step 807960 | loss 2.2953 | step time 37.42ms\n",
            "step 807970 | loss 2.4263 | step time 38.33ms\n",
            "step 807980 | loss 2.3030 | step time 35.16ms\n",
            "step 807990 | loss 2.5604 | step time 35.25ms\n",
            "step 808000 | loss 2.5007 | step time 39.42ms\n",
            "step 808000 train loss: 2.419236183166504 test loss: 2.4800360202789307\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "nobodysch\n",
            "Boerg\n",
            "lionjued\n",
            "hansel-of-dope\n",
            "the_tidegg\n",
            "Fanevelie\n",
            "laboysunboy\n",
            "ronellomas1\n",
            "shiixoz\n",
            "dangisflip\n",
            "--------------------------------------------------------------------------------\n",
            "step 808010 | loss 2.2605 | step time 53.27ms\n",
            "step 808020 | loss 2.3103 | step time 56.73ms\n",
            "step 808030 | loss 2.4186 | step time 56.56ms\n",
            "step 808040 | loss 2.3812 | step time 54.77ms\n",
            "step 808050 | loss 2.4460 | step time 37.57ms\n",
            "step 808060 | loss 2.6393 | step time 35.79ms\n",
            "step 808070 | loss 2.3811 | step time 36.68ms\n",
            "step 808080 | loss 2.4257 | step time 41.71ms\n",
            "step 808090 | loss 2.4421 | step time 38.14ms\n",
            "step 808100 | loss 2.3364 | step time 37.47ms\n",
            "step 808110 | loss 2.3177 | step time 39.31ms\n",
            "step 808120 | loss 2.3661 | step time 41.33ms\n",
            "step 808130 | loss 2.4546 | step time 40.54ms\n",
            "step 808140 | loss 2.4541 | step time 37.12ms\n",
            "step 808150 | loss 2.5688 | step time 38.19ms\n",
            "step 808160 | loss 2.3789 | step time 39.45ms\n",
            "step 808170 | loss 2.5407 | step time 37.03ms\n",
            "step 808180 | loss 2.4709 | step time 40.66ms\n",
            "step 808190 | loss 2.3351 | step time 37.77ms\n",
            "step 808200 | loss 2.4892 | step time 36.59ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "EnglisH\n",
            "stankingthrowaway804\n",
            "GodmeatNofap2\n",
            "ksynsgtyzonach\n",
            "kennowardi\n",
            "aWr0227\n",
            "dlilthead\n",
            "Corinic_Punth_Jerry\n",
            "auntieisfex\n",
            "Rammachy\n",
            "--------------------------------------------------------------------------------\n",
            "step 808210 | loss 2.4951 | step time 37.54ms\n",
            "step 808220 | loss 2.5882 | step time 40.39ms\n",
            "step 808230 | loss 2.3781 | step time 38.33ms\n",
            "step 808240 | loss 2.5143 | step time 38.14ms\n",
            "step 808250 | loss 2.4340 | step time 38.28ms\n",
            "step 808260 | loss 2.3263 | step time 35.58ms\n",
            "step 808270 | loss 2.3282 | step time 37.60ms\n",
            "step 808280 | loss 2.3663 | step time 39.36ms\n",
            "step 808290 | loss 2.5617 | step time 54.89ms\n",
            "step 808300 | loss 2.5856 | step time 50.02ms\n",
            "step 808310 | loss 2.5334 | step time 52.39ms\n",
            "step 808320 | loss 2.3543 | step time 57.03ms\n",
            "step 808330 | loss 2.4483 | step time 59.12ms\n",
            "step 808340 | loss 2.4118 | step time 57.10ms\n",
            "step 808350 | loss 2.5116 | step time 61.75ms\n",
            "step 808360 | loss 2.5036 | step time 62.65ms\n",
            "step 808370 | loss 2.5611 | step time 52.66ms\n",
            "step 808380 | loss 2.5422 | step time 39.26ms\n",
            "step 808390 | loss 2.4871 | step time 39.02ms\n",
            "step 808400 | loss 2.5116 | step time 37.46ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "xzuart\n",
            "maranecriticused21\n",
            "PurryFoirSurga\n",
            "tenorbit1\n",
            "SatanaMalibagin\n",
            "ajmiji1709\n",
            "nov29\n",
            "the0diatracerah\n",
            "Nukeflodoni\n",
            "Geniv121\n",
            "--------------------------------------------------------------------------------\n",
            "step 808410 | loss 2.1489 | step time 42.55ms\n",
            "step 808420 | loss 2.4539 | step time 41.83ms\n",
            "step 808430 | loss 2.3325 | step time 39.49ms\n",
            "step 808440 | loss 2.4848 | step time 52.98ms\n",
            "step 808450 | loss 2.4871 | step time 39.62ms\n",
            "step 808460 | loss 2.3837 | step time 36.85ms\n",
            "step 808470 | loss 2.4405 | step time 37.91ms\n",
            "step 808480 | loss 2.5362 | step time 45.00ms\n",
            "step 808490 | loss 2.6752 | step time 36.38ms\n",
            "step 808500 | loss 2.2554 | step time 37.78ms\n",
            "step 808500 train loss: 2.4292805194854736 test loss: 2.481747627258301\n",
            "step 808510 | loss 2.5024 | step time 40.27ms\n",
            "step 808520 | loss 2.4768 | step time 38.65ms\n",
            "step 808530 | loss 2.3070 | step time 40.04ms\n",
            "step 808540 | loss 2.3703 | step time 38.41ms\n",
            "step 808550 | loss 2.4306 | step time 55.37ms\n",
            "step 808560 | loss 2.2963 | step time 40.91ms\n",
            "step 808570 | loss 2.2238 | step time 43.42ms\n",
            "step 808580 | loss 2.4880 | step time 39.36ms\n",
            "step 808590 | loss 2.3185 | step time 52.71ms\n",
            "step 808600 | loss 2.5564 | step time 52.08ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Doflwtf08\n",
            "emantravelbawby\n",
            "MikeRPD\n",
            "youmentmemethjerk\n",
            "Lordeex\n",
            "littletrap\n",
            "tp-ta\n",
            "herlonumaz\n",
            "stushman25\n",
            "al_Wfall1365\n",
            "--------------------------------------------------------------------------------\n",
            "step 808610 | loss 2.4643 | step time 58.16ms\n",
            "step 808620 | loss 2.4414 | step time 57.79ms\n",
            "step 808630 | loss 2.4969 | step time 60.71ms\n",
            "step 808640 | loss 2.3852 | step time 59.95ms\n",
            "step 808650 | loss 2.6021 | step time 65.43ms\n",
            "step 808660 | loss 2.5999 | step time 59.11ms\n",
            "step 808670 | loss 2.4112 | step time 37.59ms\n",
            "step 808680 | loss 2.4113 | step time 36.72ms\n",
            "step 808690 | loss 2.4862 | step time 38.45ms\n",
            "step 808700 | loss 2.4026 | step time 41.05ms\n",
            "step 808710 | loss 2.3390 | step time 36.19ms\n",
            "step 808720 | loss 2.4042 | step time 50.45ms\n",
            "step 808730 | loss 2.5873 | step time 38.20ms\n",
            "step 808740 | loss 2.6527 | step time 36.83ms\n",
            "step 808750 | loss 2.4535 | step time 37.96ms\n",
            "step 808760 | loss 2.5402 | step time 36.75ms\n",
            "step 808770 | loss 2.2716 | step time 38.61ms\n",
            "step 808780 | loss 2.4203 | step time 38.12ms\n",
            "step 808790 | loss 2.4347 | step time 38.51ms\n",
            "step 808800 | loss 2.3894 | step time 37.85ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "iiiiemairenii\n",
            "Superbot_Mifturne\n",
            "avrato\n",
            "enfixin\n",
            "drAvosteVoxd\n",
            "kid_frienis\n",
            "User1607\n",
            "knightsmybadger08\n",
            "yocriese\n",
            "cmoose7302\n",
            "--------------------------------------------------------------------------------\n",
            "step 808810 | loss 2.2370 | step time 37.93ms\n",
            "step 808820 | loss 2.4338 | step time 39.67ms\n",
            "step 808830 | loss 2.5239 | step time 37.66ms\n",
            "step 808840 | loss 2.3721 | step time 40.66ms\n",
            "step 808850 | loss 2.4606 | step time 37.38ms\n",
            "step 808860 | loss 2.4409 | step time 35.70ms\n",
            "step 808870 | loss 2.3766 | step time 41.70ms\n",
            "step 808880 | loss 2.4755 | step time 40.32ms\n",
            "step 808890 | loss 2.4342 | step time 38.70ms\n",
            "step 808900 | loss 2.5258 | step time 37.15ms\n",
            "step 808910 | loss 2.5202 | step time 71.03ms\n",
            "step 808920 | loss 2.4506 | step time 53.34ms\n",
            "step 808930 | loss 2.4281 | step time 64.75ms\n",
            "step 808940 | loss 2.4662 | step time 57.14ms\n",
            "step 808950 | loss 2.5153 | step time 55.80ms\n",
            "step 808960 | loss 2.4514 | step time 56.99ms\n",
            "step 808970 | loss 2.5409 | step time 60.13ms\n",
            "step 808980 | loss 2.4407 | step time 58.70ms\n",
            "step 808990 | loss 2.4339 | step time 57.90ms\n",
            "step 809000 | loss 2.5357 | step time 40.71ms\n",
            "step 809000 train loss: 2.418220281600952 test loss: 2.4797143936157227\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "XUKnext\n",
            "HerTheHolmmasteroni\n",
            "ISABEDCmT_MESTER\n",
            "teponyc2\n",
            "00B_Radiala\n",
            "Omega655\n",
            "inlifeoneishad\n",
            "baburggt\n",
            "Inosi\n",
            "hupponhern\n",
            "--------------------------------------------------------------------------------\n",
            "step 809010 | loss 2.2573 | step time 38.64ms\n",
            "step 809020 | loss 2.4237 | step time 42.15ms\n",
            "step 809030 | loss 2.3395 | step time 40.53ms\n",
            "step 809040 | loss 2.4434 | step time 37.56ms\n",
            "step 809050 | loss 2.3582 | step time 39.71ms\n",
            "step 809060 | loss 2.4448 | step time 38.45ms\n",
            "step 809070 | loss 2.6001 | step time 37.77ms\n",
            "step 809080 | loss 2.4987 | step time 38.92ms\n",
            "step 809090 | loss 2.2938 | step time 43.20ms\n",
            "step 809100 | loss 2.5335 | step time 38.41ms\n",
            "step 809110 | loss 2.5572 | step time 38.44ms\n",
            "step 809120 | loss 2.6386 | step time 39.83ms\n",
            "step 809130 | loss 2.4455 | step time 39.06ms\n",
            "step 809140 | loss 2.3169 | step time 38.76ms\n",
            "step 809150 | loss 2.5197 | step time 38.52ms\n",
            "step 809160 | loss 2.4744 | step time 38.21ms\n",
            "step 809170 | loss 2.4817 | step time 45.19ms\n",
            "step 809180 | loss 2.4297 | step time 39.72ms\n",
            "step 809190 | loss 2.5885 | step time 52.83ms\n",
            "step 809200 | loss 2.3886 | step time 42.61ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "icankevs\n",
            "z_903\n",
            "CriMie\n",
            "Ronic99999999727\n",
            "iowicue612\n",
            "I_hatebatman\n",
            "SushiPostAcrum\n",
            "jam12830\n",
            "newstoppy\n",
            "accountomiguess\n",
            "--------------------------------------------------------------------------------\n",
            "step 809210 | loss 2.4088 | step time 56.64ms\n",
            "step 809220 | loss 2.2949 | step time 59.95ms\n",
            "step 809230 | loss 2.3987 | step time 56.87ms\n",
            "step 809240 | loss 2.4826 | step time 56.98ms\n",
            "step 809250 | loss 2.4848 | step time 53.12ms\n",
            "step 809260 | loss 2.6111 | step time 58.06ms\n",
            "step 809270 | loss 2.4499 | step time 38.10ms\n",
            "step 809280 | loss 2.4048 | step time 36.38ms\n",
            "step 809290 | loss 2.5057 | step time 37.47ms\n",
            "step 809300 | loss 2.5841 | step time 42.25ms\n",
            "step 809310 | loss 2.2432 | step time 37.17ms\n",
            "step 809320 | loss 2.3877 | step time 38.84ms\n",
            "step 809330 | loss 2.3035 | step time 37.55ms\n",
            "step 809340 | loss 2.3651 | step time 39.02ms\n",
            "step 809350 | loss 2.5182 | step time 37.49ms\n",
            "step 809360 | loss 2.4969 | step time 40.44ms\n",
            "step 809370 | loss 2.4864 | step time 39.74ms\n",
            "step 809380 | loss 2.3837 | step time 47.34ms\n",
            "step 809390 | loss 2.5616 | step time 37.90ms\n",
            "step 809400 | loss 2.6166 | step time 37.28ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "jabiii\n",
            "bcbsyns\n",
            "tttt23\n",
            "FluffyKomman\n",
            "Savage-Mama\n",
            "steergx\n",
            "phil586\n",
            "RoundMasterShart18\n",
            "Stalker34\n",
            "Acywei\n",
            "--------------------------------------------------------------------------------\n",
            "step 809410 | loss 2.3647 | step time 37.51ms\n",
            "step 809420 | loss 2.5657 | step time 37.67ms\n",
            "step 809430 | loss 2.5569 | step time 38.16ms\n",
            "step 809440 | loss 2.3702 | step time 35.85ms\n",
            "step 809450 | loss 2.5901 | step time 38.29ms\n",
            "step 809460 | loss 2.5250 | step time 37.05ms\n",
            "step 809470 | loss 2.3694 | step time 38.70ms\n",
            "step 809480 | loss 2.2601 | step time 40.16ms\n",
            "step 809490 | loss 2.3822 | step time 39.90ms\n",
            "step 809500 | loss 2.5778 | step time 39.04ms\n",
            "step 809500 train loss: 2.4511725902557373 test loss: 2.480978012084961\n",
            "step 809510 | loss 2.4300 | step time 48.29ms\n",
            "step 809520 | loss 2.5166 | step time 53.39ms\n",
            "step 809530 | loss 2.4177 | step time 56.59ms\n",
            "step 809540 | loss 2.5042 | step time 58.14ms\n",
            "step 809550 | loss 2.4453 | step time 52.37ms\n",
            "step 809560 | loss 2.4003 | step time 59.24ms\n",
            "step 809570 | loss 2.3705 | step time 61.62ms\n",
            "step 809580 | loss 2.2563 | step time 38.07ms\n",
            "step 809590 | loss 2.5990 | step time 40.83ms\n",
            "step 809600 | loss 2.5024 | step time 41.13ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "thelightlydowner\n",
            "hydrodoportal\n",
            "Extertinhere\n",
            "Pop3theLewy\n",
            "sdkogne\n",
            "emt196\n",
            "bgchi\n",
            "FetiKalvi\n",
            "loganballs\n",
            "pfucker2\n",
            "--------------------------------------------------------------------------------\n",
            "step 809610 | loss 2.4498 | step time 39.91ms\n",
            "step 809620 | loss 2.4165 | step time 40.29ms\n",
            "step 809630 | loss 2.4924 | step time 41.06ms\n",
            "step 809640 | loss 2.3714 | step time 38.34ms\n",
            "step 809650 | loss 2.5098 | step time 38.81ms\n",
            "step 809660 | loss 2.5161 | step time 39.16ms\n",
            "step 809670 | loss 2.5027 | step time 39.72ms\n",
            "step 809680 | loss 2.4950 | step time 37.00ms\n",
            "step 809690 | loss 2.5291 | step time 39.33ms\n",
            "step 809700 | loss 2.4873 | step time 41.61ms\n",
            "step 809710 | loss 2.4509 | step time 37.06ms\n",
            "step 809720 | loss 2.4213 | step time 37.73ms\n",
            "step 809730 | loss 2.4523 | step time 37.65ms\n",
            "step 809740 | loss 2.5639 | step time 37.19ms\n",
            "step 809750 | loss 2.6827 | step time 38.24ms\n",
            "step 809760 | loss 2.3505 | step time 42.24ms\n",
            "step 809770 | loss 2.5957 | step time 39.37ms\n",
            "step 809780 | loss 2.3521 | step time 38.63ms\n",
            "step 809790 | loss 2.7033 | step time 36.07ms\n",
            "step 809800 | loss 2.4919 | step time 40.29ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "LeftMirrorCourt\n",
            "wafflewaffley\n",
            "hurveso18\n",
            "Narvusky\n",
            "Joeboo\n",
            "dicolejack_\n",
            "nurt2bbywizard\n",
            "pulp07\n",
            "timmybutt\n",
            "Addy_Zelle\n",
            "--------------------------------------------------------------------------------\n",
            "step 809810 | loss 2.6520 | step time 56.89ms\n",
            "step 809820 | loss 2.4185 | step time 49.41ms\n",
            "step 809830 | loss 2.6018 | step time 56.58ms\n",
            "step 809840 | loss 2.5101 | step time 59.78ms\n",
            "step 809850 | loss 2.6039 | step time 58.12ms\n",
            "step 809860 | loss 2.5575 | step time 56.09ms\n",
            "step 809870 | loss 2.4928 | step time 70.18ms\n",
            "step 809880 | loss 2.3963 | step time 59.72ms\n",
            "step 809890 | loss 2.6060 | step time 58.33ms\n",
            "step 809900 | loss 2.6762 | step time 38.27ms\n",
            "step 809910 | loss 2.4179 | step time 49.09ms\n",
            "step 809920 | loss 2.3498 | step time 39.15ms\n",
            "step 809930 | loss 2.4675 | step time 37.91ms\n",
            "step 809940 | loss 2.4643 | step time 38.27ms\n",
            "step 809950 | loss 2.4016 | step time 41.70ms\n",
            "step 809960 | loss 2.3865 | step time 38.24ms\n",
            "step 809970 | loss 2.3379 | step time 37.71ms\n",
            "step 809980 | loss 2.3270 | step time 40.71ms\n",
            "step 809990 | loss 2.3037 | step time 40.38ms\n",
            "step 810000 | loss 2.5400 | step time 37.86ms\n",
            "step 810000 train loss: 2.433403491973877 test loss: 2.480391025543213\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "hatedealsdumb\n",
            "mrkiespenis2\n",
            "Lustwimmd\n",
            "Dragon_GyruRuler\n",
            "ValielJ\n",
            "mr3man127\n",
            "throwaway0592347\n",
            "Kruddey-usernarden\n",
            "mbteen\n",
            "bjooglethrow22\n",
            "--------------------------------------------------------------------------------\n",
            "step 810010 | loss 2.3601 | step time 41.02ms\n",
            "step 810020 | loss 2.4177 | step time 37.70ms\n",
            "step 810030 | loss 2.4348 | step time 47.26ms\n",
            "step 810040 | loss 2.6185 | step time 38.77ms\n",
            "step 810050 | loss 2.3752 | step time 42.14ms\n",
            "step 810060 | loss 2.4613 | step time 38.86ms\n",
            "step 810070 | loss 2.4328 | step time 38.01ms\n",
            "step 810080 | loss 2.4412 | step time 37.56ms\n",
            "step 810090 | loss 2.4630 | step time 35.94ms\n",
            "step 810100 | loss 2.3855 | step time 39.01ms\n",
            "step 810110 | loss 2.3947 | step time 44.67ms\n",
            "step 810120 | loss 2.4379 | step time 59.08ms\n",
            "step 810130 | loss 2.3422 | step time 58.96ms\n",
            "step 810140 | loss 2.5473 | step time 60.46ms\n",
            "step 810150 | loss 2.3594 | step time 58.41ms\n",
            "step 810160 | loss 2.4409 | step time 57.46ms\n",
            "step 810170 | loss 2.4130 | step time 54.17ms\n",
            "step 810180 | loss 2.6846 | step time 56.01ms\n",
            "step 810190 | loss 2.3277 | step time 57.41ms\n",
            "step 810200 | loss 2.5274 | step time 58.71ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "oromanbr\n",
            "thesharityph\n",
            "tushiepii10\n",
            "cmlmlyb\n",
            "Killz3336\n",
            "stmminagolios\n",
            "amclerwe\n",
            "throughbutsonmore\n",
            "dawinh97\n",
            "permandukpit\n",
            "--------------------------------------------------------------------------------\n",
            "step 810210 | loss 2.5708 | step time 38.66ms\n",
            "step 810220 | loss 2.6306 | step time 38.27ms\n",
            "step 810230 | loss 2.4809 | step time 37.34ms\n",
            "step 810240 | loss 2.3446 | step time 36.50ms\n",
            "step 810250 | loss 2.4441 | step time 42.84ms\n",
            "step 810260 | loss 2.4856 | step time 37.77ms\n",
            "step 810270 | loss 2.4040 | step time 37.30ms\n",
            "step 810280 | loss 2.3720 | step time 41.34ms\n",
            "step 810290 | loss 2.4015 | step time 42.98ms\n",
            "step 810300 | loss 2.5182 | step time 40.29ms\n",
            "step 810310 | loss 2.4675 | step time 41.36ms\n",
            "step 810320 | loss 2.5754 | step time 39.35ms\n",
            "step 810330 | loss 2.4299 | step time 39.07ms\n",
            "step 810340 | loss 2.4394 | step time 50.40ms\n",
            "step 810350 | loss 2.4033 | step time 41.63ms\n",
            "step 810360 | loss 2.4472 | step time 39.21ms\n",
            "step 810370 | loss 2.4507 | step time 38.07ms\n",
            "step 810380 | loss 2.3894 | step time 40.05ms\n",
            "step 810390 | loss 2.4178 | step time 38.92ms\n",
            "step 810400 | loss 2.4672 | step time 41.67ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "verynects\n",
            "3remane36\n",
            "jnsmdshot\n",
            "imdanspesticclinch\n",
            "Aval_Neeken\n",
            "ForSuperDemonutsShad\n",
            "yapad12\n",
            "Fandawafflesface\n",
            "yeretoday\n",
            "MaycountNDower55\n",
            "--------------------------------------------------------------------------------\n",
            "step 810410 | loss 2.3052 | step time 37.34ms\n",
            "step 810420 | loss 2.4043 | step time 41.57ms\n",
            "step 810430 | loss 2.4872 | step time 42.90ms\n",
            "step 810440 | loss 2.5818 | step time 51.55ms\n",
            "step 810450 | loss 2.5098 | step time 54.29ms\n",
            "step 810460 | loss 2.4177 | step time 61.02ms\n",
            "step 810470 | loss 2.3976 | step time 57.98ms\n",
            "step 810480 | loss 2.4744 | step time 54.76ms\n",
            "step 810490 | loss 2.5808 | step time 56.37ms\n",
            "step 810500 | loss 2.2955 | step time 67.58ms\n",
            "step 810500 train loss: 2.4505999088287354 test loss: 2.480639696121216\n",
            "step 810510 | loss 2.3825 | step time 37.33ms\n",
            "step 810520 | loss 2.5572 | step time 40.13ms\n",
            "step 810530 | loss 2.3695 | step time 36.51ms\n",
            "step 810540 | loss 2.4145 | step time 36.38ms\n",
            "step 810550 | loss 2.4078 | step time 50.03ms\n",
            "step 810560 | loss 2.5200 | step time 37.24ms\n",
            "step 810570 | loss 2.5176 | step time 40.60ms\n",
            "step 810580 | loss 2.4550 | step time 39.61ms\n",
            "step 810590 | loss 2.5733 | step time 40.90ms\n",
            "step 810600 | loss 2.2935 | step time 42.62ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Todoingon\n",
            "CioiJoze\n",
            "kvvcsfusK\n",
            "HouseDevelt\n",
            "xplus\n",
            "saintGAUSEr\n",
            "peaceyes\n",
            "Dyran\n",
            "DTGane\n",
            "waycocodapper\n",
            "--------------------------------------------------------------------------------\n",
            "step 810610 | loss 2.5030 | step time 50.99ms\n",
            "step 810620 | loss 2.5017 | step time 42.32ms\n",
            "step 810630 | loss 2.5492 | step time 38.57ms\n",
            "step 810640 | loss 2.3230 | step time 37.74ms\n",
            "step 810650 | loss 2.3720 | step time 36.57ms\n",
            "step 810660 | loss 2.4547 | step time 36.94ms\n",
            "step 810670 | loss 2.5315 | step time 39.35ms\n",
            "step 810680 | loss 2.4175 | step time 37.96ms\n",
            "step 810690 | loss 2.4712 | step time 37.67ms\n",
            "step 810700 | loss 2.3809 | step time 40.01ms\n",
            "step 810710 | loss 2.4391 | step time 36.18ms\n",
            "step 810720 | loss 2.4281 | step time 39.32ms\n",
            "step 810730 | loss 2.2762 | step time 37.93ms\n",
            "step 810740 | loss 2.3750 | step time 62.00ms\n",
            "step 810750 | loss 2.4818 | step time 52.78ms\n",
            "step 810760 | loss 2.4542 | step time 49.95ms\n",
            "step 810770 | loss 2.5975 | step time 56.23ms\n",
            "step 810780 | loss 2.5043 | step time 67.39ms\n",
            "step 810790 | loss 2.4255 | step time 56.87ms\n",
            "step 810800 | loss 2.5454 | step time 55.37ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "JuanPts\n",
            "Hamdaheppie\n",
            "throwaway-i-away\n",
            "HelpGotCompoKidyou\n",
            "nolecoon\n",
            "generaldeath\n",
            "_urbustist_\n",
            "Agame28\n",
            "Taoulalong\n",
            "WasleyAcpro\n",
            "--------------------------------------------------------------------------------\n",
            "step 810810 | loss 2.4544 | step time 37.41ms\n",
            "step 810820 | loss 2.4370 | step time 42.43ms\n",
            "step 810830 | loss 2.6744 | step time 37.37ms\n",
            "step 810840 | loss 2.6947 | step time 36.17ms\n",
            "step 810850 | loss 2.4497 | step time 36.84ms\n",
            "step 810860 | loss 2.6455 | step time 36.45ms\n",
            "step 810870 | loss 2.2347 | step time 43.43ms\n",
            "step 810880 | loss 2.5440 | step time 36.72ms\n",
            "step 810890 | loss 2.4371 | step time 38.82ms\n",
            "step 810900 | loss 2.4044 | step time 51.36ms\n",
            "step 810910 | loss 2.5522 | step time 36.74ms\n",
            "step 810920 | loss 2.4723 | step time 40.98ms\n",
            "step 810930 | loss 2.4730 | step time 39.41ms\n",
            "step 810940 | loss 2.4759 | step time 38.01ms\n",
            "step 810950 | loss 2.4928 | step time 37.52ms\n",
            "step 810960 | loss 2.4887 | step time 36.41ms\n",
            "step 810970 | loss 2.5558 | step time 37.41ms\n",
            "step 810980 | loss 2.5146 | step time 36.02ms\n",
            "step 810990 | loss 2.5488 | step time 40.78ms\n",
            "step 811000 | loss 2.3334 | step time 38.83ms\n",
            "step 811000 train loss: 2.423797369003296 test loss: 2.486680507659912\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "adagian\n",
            "herlegthefar\n",
            "cocketwombather66\n",
            "LildGringsam\n",
            "Augustinethansodoo\n",
            "Brookwardpalmf\n",
            "codest3160\n",
            "dogeper3\n",
            "dingusgreva\n",
            "vv2\n",
            "--------------------------------------------------------------------------------\n",
            "step 811010 | loss 2.6395 | step time 38.76ms\n",
            "step 811020 | loss 2.5902 | step time 39.15ms\n",
            "step 811030 | loss 2.4806 | step time 60.70ms\n",
            "step 811040 | loss 2.5730 | step time 50.17ms\n",
            "step 811050 | loss 2.4312 | step time 53.08ms\n",
            "step 811060 | loss 2.4131 | step time 62.11ms\n",
            "step 811070 | loss 2.4860 | step time 60.86ms\n",
            "step 811080 | loss 2.4191 | step time 68.44ms\n",
            "step 811090 | loss 2.3411 | step time 57.23ms\n",
            "step 811100 | loss 2.5546 | step time 53.80ms\n",
            "step 811110 | loss 2.5140 | step time 51.87ms\n",
            "step 811120 | loss 2.2814 | step time 35.06ms\n",
            "step 811130 | loss 2.5012 | step time 37.40ms\n",
            "step 811140 | loss 2.3099 | step time 37.32ms\n",
            "step 811150 | loss 2.4019 | step time 42.92ms\n",
            "step 811160 | loss 2.3103 | step time 39.15ms\n",
            "step 811170 | loss 2.5075 | step time 45.23ms\n",
            "step 811180 | loss 2.5085 | step time 40.74ms\n",
            "step 811190 | loss 2.6860 | step time 39.52ms\n",
            "step 811200 | loss 2.4880 | step time 38.26ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "nattw\n",
            "xf0rPUSZ\n",
            "kclash30\n",
            "abidaskenk\n",
            "commentine\n",
            "matt_reptz\n",
            "throw_awayx\n",
            "willsautaboutja\n",
            "tinitar101\n",
            "MacbugHeMovies\n",
            "--------------------------------------------------------------------------------\n",
            "step 811210 | loss 2.2011 | step time 36.96ms\n",
            "step 811220 | loss 2.6038 | step time 38.11ms\n",
            "step 811230 | loss 2.3968 | step time 35.89ms\n",
            "step 811240 | loss 2.3557 | step time 36.61ms\n",
            "step 811250 | loss 2.3402 | step time 36.70ms\n",
            "step 811260 | loss 2.3937 | step time 41.61ms\n",
            "step 811270 | loss 2.6162 | step time 37.70ms\n",
            "step 811280 | loss 2.5354 | step time 37.66ms\n",
            "step 811290 | loss 2.5345 | step time 38.11ms\n",
            "step 811300 | loss 2.3076 | step time 38.48ms\n",
            "step 811310 | loss 2.5020 | step time 37.98ms\n",
            "step 811320 | loss 2.5777 | step time 37.63ms\n",
            "step 811330 | loss 2.6019 | step time 37.64ms\n",
            "step 811340 | loss 2.4508 | step time 38.88ms\n",
            "step 811350 | loss 2.4739 | step time 38.30ms\n",
            "step 811360 | loss 2.5366 | step time 59.69ms\n",
            "step 811370 | loss 2.3625 | step time 51.15ms\n",
            "step 811380 | loss 2.4575 | step time 52.82ms\n",
            "step 811390 | loss 2.6404 | step time 60.70ms\n",
            "step 811400 | loss 2.4057 | step time 56.67ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "mwmt\n",
            "livit_yitfex\n",
            "SuperreXENCEIlin\n",
            "shitbaco\n",
            "kxsmall\n",
            "Messi69\n",
            "dengez\n",
            "JollyKouts2017\n",
            "RobertKing\n",
            "Krp84\n",
            "--------------------------------------------------------------------------------\n",
            "step 811410 | loss 2.4547 | step time 55.54ms\n",
            "step 811420 | loss 2.6155 | step time 55.76ms\n",
            "step 811430 | loss 2.4005 | step time 41.51ms\n",
            "step 811440 | loss 2.5176 | step time 38.63ms\n",
            "step 811450 | loss 2.4779 | step time 39.26ms\n",
            "step 811460 | loss 2.3922 | step time 38.02ms\n",
            "step 811470 | loss 2.2951 | step time 38.63ms\n",
            "step 811480 | loss 2.5230 | step time 36.89ms\n",
            "step 811490 | loss 2.6646 | step time 39.33ms\n",
            "step 811500 | loss 2.5152 | step time 39.25ms\n",
            "step 811500 train loss: 2.462792158126831 test loss: 2.4828617572784424\n",
            "step 811510 | loss 2.2825 | step time 40.64ms\n",
            "step 811520 | loss 2.3321 | step time 36.04ms\n",
            "step 811530 | loss 2.3130 | step time 38.89ms\n",
            "step 811540 | loss 2.3772 | step time 39.84ms\n",
            "step 811550 | loss 2.3316 | step time 37.32ms\n",
            "step 811560 | loss 2.4585 | step time 40.01ms\n",
            "step 811570 | loss 2.3181 | step time 36.45ms\n",
            "step 811580 | loss 2.3863 | step time 39.36ms\n",
            "step 811590 | loss 2.3214 | step time 37.36ms\n",
            "step 811600 | loss 2.2871 | step time 37.76ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "RedditChris\n",
            "Emp-Ofsultian-That\n",
            "jaracooke\n",
            "Jimux_Mommy\n",
            "Sellinghere494\n",
            "leadnexuson\n",
            "fluxchealer22\n",
            "busingforwallz\n",
            "Big_Sponzer\n",
            "indieballie\n",
            "--------------------------------------------------------------------------------\n",
            "step 811610 | loss 2.6552 | step time 41.18ms\n",
            "step 811620 | loss 2.4268 | step time 40.92ms\n",
            "step 811630 | loss 2.4716 | step time 38.09ms\n",
            "step 811640 | loss 2.5926 | step time 43.52ms\n",
            "step 811650 | loss 2.5117 | step time 58.44ms\n",
            "step 811660 | loss 2.4918 | step time 66.47ms\n",
            "step 811670 | loss 2.3231 | step time 51.66ms\n",
            "step 811680 | loss 2.3358 | step time 55.40ms\n",
            "step 811690 | loss 2.3875 | step time 57.45ms\n",
            "step 811700 | loss 2.6498 | step time 56.68ms\n",
            "step 811710 | loss 2.5034 | step time 60.86ms\n",
            "step 811720 | loss 2.3516 | step time 56.08ms\n",
            "step 811730 | loss 2.6485 | step time 38.37ms\n",
            "step 811740 | loss 2.4002 | step time 41.75ms\n",
            "step 811750 | loss 2.6030 | step time 37.46ms\n",
            "step 811760 | loss 2.5400 | step time 45.13ms\n",
            "step 811770 | loss 2.5734 | step time 36.35ms\n",
            "step 811780 | loss 2.4316 | step time 35.50ms\n",
            "step 811790 | loss 2.4354 | step time 38.69ms\n",
            "step 811800 | loss 2.5066 | step time 39.12ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "oW_awh0w\n",
            "Weirdhayy\n",
            "adolftrestingquestio\n",
            "CrookleThipUp\n",
            "Slamblaster1220\n",
            "DogNotRakestHistop\n",
            "Ignorahumur\n",
            "ie24throwaway\n",
            "YourTyClicker\n",
            "houcyoud\n",
            "--------------------------------------------------------------------------------\n",
            "step 811810 | loss 2.3579 | step time 39.76ms\n",
            "step 811820 | loss 2.4909 | step time 48.76ms\n",
            "step 811830 | loss 2.5664 | step time 42.35ms\n",
            "step 811840 | loss 2.6164 | step time 38.41ms\n",
            "step 811850 | loss 2.5637 | step time 39.94ms\n",
            "step 811860 | loss 2.4007 | step time 40.75ms\n",
            "step 811870 | loss 2.5375 | step time 39.99ms\n",
            "step 811880 | loss 2.4460 | step time 39.05ms\n",
            "step 811890 | loss 2.4227 | step time 37.95ms\n",
            "step 811900 | loss 2.3270 | step time 38.19ms\n",
            "step 811910 | loss 2.4565 | step time 40.95ms\n",
            "step 811920 | loss 2.1893 | step time 39.60ms\n",
            "step 811930 | loss 2.4870 | step time 38.91ms\n",
            "step 811940 | loss 2.4813 | step time 40.90ms\n",
            "step 811950 | loss 2.3976 | step time 38.49ms\n",
            "step 811960 | loss 2.3313 | step time 59.53ms\n",
            "step 811970 | loss 2.4342 | step time 55.41ms\n",
            "step 811980 | loss 2.6119 | step time 50.11ms\n",
            "step 811990 | loss 2.4301 | step time 58.69ms\n",
            "step 812000 | loss 2.5203 | step time 56.87ms\n",
            "step 812000 train loss: 2.4581363201141357 test loss: 2.4824440479278564\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "MEMGEDNESS_DE\n",
            "SireneJoel\n",
            "sicovlimmv\n",
            "lost1ist5\n",
            "ringenaccountute2sex\n",
            "M2SexDaccount\n",
            "whatyouberalds\n",
            "forgregious\n",
            "siegentismoleforhelp\n",
            "mavemac_spage_guy\n",
            "--------------------------------------------------------------------------------\n",
            "step 812010 | loss 2.6047 | step time 55.11ms\n",
            "step 812020 | loss 2.5175 | step time 39.96ms\n",
            "step 812030 | loss 2.4285 | step time 39.23ms\n",
            "step 812040 | loss 2.3575 | step time 39.06ms\n",
            "step 812050 | loss 2.3014 | step time 36.57ms\n",
            "step 812060 | loss 2.6181 | step time 41.39ms\n",
            "step 812070 | loss 2.5734 | step time 38.93ms\n",
            "step 812080 | loss 2.3205 | step time 37.69ms\n",
            "step 812090 | loss 2.4285 | step time 37.76ms\n",
            "step 812100 | loss 2.4607 | step time 37.75ms\n",
            "step 812110 | loss 2.4465 | step time 42.95ms\n",
            "step 812120 | loss 2.5646 | step time 44.88ms\n",
            "step 812130 | loss 2.3021 | step time 39.48ms\n",
            "step 812140 | loss 2.7070 | step time 38.24ms\n",
            "step 812150 | loss 2.1366 | step time 39.78ms\n",
            "step 812160 | loss 2.5848 | step time 40.93ms\n",
            "step 812170 | loss 2.3408 | step time 38.28ms\n",
            "step 812180 | loss 2.4268 | step time 38.74ms\n",
            "step 812190 | loss 2.4445 | step time 37.68ms\n",
            "step 812200 | loss 2.4315 | step time 39.93ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "cassuredaven\n",
            "Richarda1234567\n",
            "kaeriarco\n",
            "myrobertshepucks1983\n",
            "sompliciousdoobling\n",
            "Pobbles_12\n",
            "poopscopio\n",
            "_moroye_69\n",
            "Rolowee1269\n",
            "yutyo93\n",
            "--------------------------------------------------------------------------------\n",
            "step 812210 | loss 2.5164 | step time 37.55ms\n",
            "step 812220 | loss 2.3684 | step time 40.81ms\n",
            "step 812230 | loss 2.5416 | step time 39.69ms\n",
            "step 812240 | loss 2.4028 | step time 36.27ms\n",
            "step 812250 | loss 2.7922 | step time 65.49ms\n",
            "step 812260 | loss 2.6407 | step time 54.41ms\n",
            "step 812270 | loss 2.4362 | step time 48.94ms\n",
            "step 812280 | loss 2.2218 | step time 55.29ms\n",
            "step 812290 | loss 2.4337 | step time 57.58ms\n",
            "step 812300 | loss 2.3062 | step time 59.06ms\n",
            "step 812310 | loss 2.4375 | step time 55.21ms\n",
            "step 812320 | loss 2.4167 | step time 56.37ms\n",
            "step 812330 | loss 2.4834 | step time 60.40ms\n",
            "step 812340 | loss 2.4688 | step time 37.71ms\n",
            "step 812350 | loss 2.5315 | step time 37.17ms\n",
            "step 812360 | loss 2.4445 | step time 37.49ms\n",
            "step 812370 | loss 2.3927 | step time 38.60ms\n",
            "step 812380 | loss 2.4899 | step time 38.08ms\n",
            "step 812390 | loss 2.5547 | step time 38.89ms\n",
            "step 812400 | loss 2.4172 | step time 38.26ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "SteeredBob\n",
            "BigPentXmober\n",
            "crystalStrike\n",
            "Part-Ful-A-Defab\n",
            "wowopower\n",
            "DFWeet\n",
            "threebest\n",
            "MAWIC_PORNEL\n",
            "Lizard_Basklare\n",
            "jyzsa\n",
            "--------------------------------------------------------------------------------\n",
            "step 812410 | loss 2.3906 | step time 41.61ms\n",
            "step 812420 | loss 2.3519 | step time 41.73ms\n",
            "step 812430 | loss 2.5584 | step time 37.03ms\n",
            "step 812440 | loss 2.2566 | step time 37.02ms\n",
            "step 812450 | loss 2.3265 | step time 38.64ms\n",
            "step 812460 | loss 2.5552 | step time 42.98ms\n",
            "step 812470 | loss 2.2350 | step time 39.65ms\n",
            "step 812480 | loss 2.4477 | step time 37.30ms\n",
            "step 812490 | loss 2.4970 | step time 45.05ms\n",
            "step 812500 | loss 2.3576 | step time 39.19ms\n",
            "step 812500 train loss: 2.4673681259155273 test loss: 2.4815783500671387\n",
            "step 812510 | loss 2.4311 | step time 39.97ms\n",
            "step 812520 | loss 2.5962 | step time 40.59ms\n",
            "step 812530 | loss 2.3869 | step time 37.17ms\n",
            "step 812540 | loss 2.5748 | step time 39.88ms\n",
            "step 812550 | loss 2.4778 | step time 62.41ms\n",
            "step 812560 | loss 2.3523 | step time 51.54ms\n",
            "step 812570 | loss 2.4303 | step time 52.88ms\n",
            "step 812580 | loss 2.4406 | step time 59.07ms\n",
            "step 812590 | loss 2.3263 | step time 59.05ms\n",
            "step 812600 | loss 2.4898 | step time 58.64ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "xCYxI\n",
            "BloodBear4Dia\n",
            "MarkMA\n",
            "dessannecon\n",
            "Rafeun\n",
            "StiftMiner\n",
            "SaintManGij14\n",
            "hecksectended_\n",
            "Marto-Conce_Critics\n",
            "grandshoiltokesssUwn\n",
            "--------------------------------------------------------------------------------\n",
            "step 812610 | loss 2.4326 | step time 63.01ms\n",
            "step 812620 | loss 2.4742 | step time 40.34ms\n",
            "step 812630 | loss 2.6246 | step time 37.92ms\n",
            "step 812640 | loss 2.3633 | step time 38.97ms\n",
            "step 812650 | loss 2.5985 | step time 38.82ms\n",
            "step 812660 | loss 2.6728 | step time 39.08ms\n",
            "step 812670 | loss 2.3319 | step time 39.88ms\n",
            "step 812680 | loss 2.3435 | step time 45.61ms\n",
            "step 812690 | loss 2.5864 | step time 42.38ms\n",
            "step 812700 | loss 2.4962 | step time 39.72ms\n",
            "step 812710 | loss 2.5165 | step time 38.78ms\n",
            "step 812720 | loss 2.4711 | step time 38.67ms\n",
            "step 812730 | loss 2.2514 | step time 41.72ms\n",
            "step 812740 | loss 2.3205 | step time 40.12ms\n",
            "step 812750 | loss 2.3547 | step time 39.01ms\n",
            "step 812760 | loss 2.3408 | step time 40.23ms\n",
            "step 812770 | loss 2.3723 | step time 40.15ms\n",
            "step 812780 | loss 2.4138 | step time 42.63ms\n",
            "step 812790 | loss 2.4346 | step time 39.49ms\n",
            "step 812800 | loss 2.4935 | step time 39.38ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "carllosh58\n",
            "nickenshark99\n",
            "liondia0223365692\n",
            "jonny786\n",
            "notat_life_rosei\n",
            "onestachedciareftw\n",
            "Red_daddy_\n",
            "tercraftforlost\n",
            "trixymexicansupers\n",
            "bee6f1n\n",
            "--------------------------------------------------------------------------------\n",
            "step 812810 | loss 2.3110 | step time 38.56ms\n",
            "step 812820 | loss 2.3818 | step time 61.73ms\n",
            "step 812830 | loss 2.5276 | step time 38.28ms\n",
            "step 812840 | loss 2.5264 | step time 41.13ms\n",
            "step 812850 | loss 2.5293 | step time 60.01ms\n",
            "step 812860 | loss 2.5707 | step time 54.42ms\n",
            "step 812870 | loss 2.3769 | step time 49.87ms\n",
            "step 812880 | loss 2.3476 | step time 52.60ms\n",
            "step 812890 | loss 2.4118 | step time 60.47ms\n",
            "step 812900 | loss 2.4055 | step time 65.23ms\n",
            "step 812910 | loss 2.6183 | step time 60.27ms\n",
            "step 812920 | loss 2.4437 | step time 68.90ms\n",
            "step 812930 | loss 2.3978 | step time 55.52ms\n",
            "step 812940 | loss 2.3590 | step time 40.09ms\n",
            "step 812950 | loss 2.4670 | step time 40.19ms\n",
            "step 812960 | loss 2.5875 | step time 39.37ms\n",
            "step 812970 | loss 2.3528 | step time 38.08ms\n",
            "step 812980 | loss 2.1299 | step time 39.70ms\n",
            "step 812990 | loss 2.5840 | step time 40.55ms\n",
            "step 813000 | loss 2.4174 | step time 37.64ms\n",
            "step 813000 train loss: 2.420736074447632 test loss: 2.480339527130127\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "hotparleyleonal\n",
            "Minni99\n",
            "avoledaNAir77\n",
            "11JOCKPUS\n",
            "Karivj\n",
            "huydommaaa\n",
            "doesntpro\n",
            "AmbeEvord\n",
            "FixChopSunk\n",
            "LexirLK\n",
            "--------------------------------------------------------------------------------\n",
            "step 813010 | loss 2.4210 | step time 37.35ms\n",
            "step 813020 | loss 2.3802 | step time 37.49ms\n",
            "step 813030 | loss 2.5138 | step time 38.77ms\n",
            "step 813040 | loss 2.4244 | step time 35.83ms\n",
            "step 813050 | loss 2.4136 | step time 41.24ms\n",
            "step 813060 | loss 2.5252 | step time 39.11ms\n",
            "step 813070 | loss 2.2922 | step time 37.14ms\n",
            "step 813080 | loss 2.5788 | step time 42.67ms\n",
            "step 813090 | loss 2.4870 | step time 36.04ms\n",
            "step 813100 | loss 2.4156 | step time 36.14ms\n",
            "step 813110 | loss 2.3442 | step time 37.98ms\n",
            "step 813120 | loss 2.3987 | step time 35.68ms\n",
            "step 813130 | loss 2.4991 | step time 36.26ms\n",
            "step 813140 | loss 2.4328 | step time 38.28ms\n",
            "step 813150 | loss 2.2346 | step time 55.32ms\n",
            "step 813160 | loss 2.5184 | step time 50.99ms\n",
            "step 813170 | loss 2.1983 | step time 52.44ms\n",
            "step 813180 | loss 2.4117 | step time 58.44ms\n",
            "step 813190 | loss 2.6435 | step time 65.37ms\n",
            "step 813200 | loss 2.3658 | step time 60.55ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "SmileySandwich\n",
            "fuggyking123\n",
            "DuB123\n",
            "rew_member10nerdn\n",
            "reaper999\n",
            "helper924\n",
            "7au\n",
            "LegendingShamter\n",
            "maiunjbeda\n",
            "hoffainsinsier\n",
            "--------------------------------------------------------------------------------\n",
            "step 813210 | loss 2.3196 | step time 59.86ms\n",
            "step 813220 | loss 2.4102 | step time 42.64ms\n",
            "step 813230 | loss 2.3763 | step time 39.89ms\n",
            "step 813240 | loss 2.4480 | step time 39.89ms\n",
            "step 813250 | loss 2.6463 | step time 40.75ms\n",
            "step 813260 | loss 2.5930 | step time 37.86ms\n",
            "step 813270 | loss 2.3571 | step time 40.75ms\n",
            "step 813280 | loss 2.3418 | step time 41.30ms\n",
            "step 813290 | loss 2.5927 | step time 37.23ms\n",
            "step 813300 | loss 2.4562 | step time 40.44ms\n",
            "step 813310 | loss 2.4522 | step time 47.38ms\n",
            "step 813320 | loss 2.4382 | step time 38.18ms\n",
            "step 813330 | loss 2.5319 | step time 39.86ms\n",
            "step 813340 | loss 2.5457 | step time 37.23ms\n",
            "step 813350 | loss 2.4454 | step time 41.31ms\n",
            "step 813360 | loss 2.4422 | step time 37.38ms\n",
            "step 813370 | loss 2.4146 | step time 38.84ms\n",
            "step 813380 | loss 2.3949 | step time 39.52ms\n",
            "step 813390 | loss 2.4686 | step time 35.84ms\n",
            "step 813400 | loss 2.4276 | step time 37.45ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "mrVericiguys\n",
            "jimmymnn\n",
            "Averyue\n",
            "bluebuzzi\n",
            "xAvenus\n",
            "AirDSHA\n",
            "emilynn\n",
            "tatugatto\n",
            "Seendsuren\n",
            "callardo_patis\n",
            "--------------------------------------------------------------------------------\n",
            "step 813410 | loss 2.2539 | step time 43.37ms\n",
            "step 813420 | loss 2.3088 | step time 38.44ms\n",
            "step 813430 | loss 2.4528 | step time 46.73ms\n",
            "step 813440 | loss 2.7352 | step time 40.28ms\n",
            "step 813450 | loss 2.3985 | step time 54.97ms\n",
            "step 813460 | loss 2.5303 | step time 51.06ms\n",
            "step 813470 | loss 2.4857 | step time 48.89ms\n",
            "step 813480 | loss 2.4259 | step time 59.28ms\n",
            "step 813490 | loss 2.4345 | step time 56.08ms\n",
            "step 813500 | loss 2.4863 | step time 64.69ms\n",
            "step 813500 train loss: 2.447396993637085 test loss: 2.481546640396118\n",
            "step 813510 | loss 2.7125 | step time 59.87ms\n",
            "step 813520 | loss 2.4741 | step time 35.52ms\n",
            "step 813530 | loss 2.4635 | step time 39.15ms\n",
            "step 813540 | loss 2.4559 | step time 40.84ms\n",
            "step 813550 | loss 2.5855 | step time 38.73ms\n",
            "step 813560 | loss 2.5088 | step time 36.87ms\n",
            "step 813570 | loss 2.4655 | step time 37.32ms\n",
            "step 813580 | loss 2.5060 | step time 37.32ms\n",
            "step 813590 | loss 2.5343 | step time 39.25ms\n",
            "step 813600 | loss 2.4784 | step time 35.88ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "NoThinkFIfHelking2\n",
            "gjsmite\n",
            "mbnmais\n",
            "Olyn70\n",
            "MrExScabMoma\n",
            "bragtws\n",
            "juulbabydogg\n",
            "OriginalMonkeyPotase\n",
            "kiddesdojswgs\n",
            "Klaywormm\n",
            "--------------------------------------------------------------------------------\n",
            "step 813610 | loss 2.3734 | step time 38.28ms\n",
            "step 813620 | loss 2.4899 | step time 38.84ms\n",
            "step 813630 | loss 2.6132 | step time 38.82ms\n",
            "step 813640 | loss 2.4488 | step time 38.25ms\n",
            "step 813650 | loss 2.5094 | step time 39.07ms\n",
            "step 813660 | loss 2.3519 | step time 37.40ms\n",
            "step 813670 | loss 2.4816 | step time 36.13ms\n",
            "step 813680 | loss 2.6334 | step time 37.87ms\n",
            "step 813690 | loss 2.3809 | step time 45.76ms\n",
            "step 813700 | loss 2.2428 | step time 38.88ms\n",
            "step 813710 | loss 2.4699 | step time 38.79ms\n",
            "step 813720 | loss 2.5652 | step time 39.40ms\n",
            "step 813730 | loss 2.4746 | step time 37.06ms\n",
            "step 813740 | loss 2.4634 | step time 36.90ms\n",
            "step 813750 | loss 2.3898 | step time 39.87ms\n",
            "step 813760 | loss 2.6080 | step time 70.32ms\n",
            "step 813770 | loss 2.3852 | step time 54.45ms\n",
            "step 813780 | loss 2.2854 | step time 52.06ms\n",
            "step 813790 | loss 2.2175 | step time 53.55ms\n",
            "step 813800 | loss 2.2438 | step time 75.66ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "l23bubble\n",
            "Gagsaa\n",
            "internett82\n",
            "ndaco\n",
            "Dirttdesperst\n",
            "PerverstaN\n",
            "gmansfans\n",
            "Shatetravel\n",
            "privatemilydawg\n",
            "Red_fancy\n",
            "--------------------------------------------------------------------------------\n",
            "step 813810 | loss 2.5962 | step time 56.13ms\n",
            "step 813820 | loss 2.3411 | step time 63.99ms\n",
            "step 813830 | loss 2.4851 | step time 41.31ms\n",
            "step 813840 | loss 2.2644 | step time 42.74ms\n",
            "step 813850 | loss 2.4353 | step time 35.77ms\n",
            "step 813860 | loss 2.4541 | step time 39.36ms\n",
            "step 813870 | loss 2.3533 | step time 39.54ms\n",
            "step 813880 | loss 2.5201 | step time 44.05ms\n",
            "step 813890 | loss 2.3396 | step time 38.99ms\n",
            "step 813900 | loss 2.3855 | step time 38.44ms\n",
            "step 813910 | loss 2.3064 | step time 36.00ms\n",
            "step 813920 | loss 2.4935 | step time 46.53ms\n",
            "step 813930 | loss 2.3494 | step time 36.70ms\n",
            "step 813940 | loss 2.3558 | step time 38.40ms\n",
            "step 813950 | loss 2.3495 | step time 37.05ms\n",
            "step 813960 | loss 2.5353 | step time 39.64ms\n",
            "step 813970 | loss 2.4156 | step time 38.81ms\n",
            "step 813980 | loss 2.3246 | step time 39.09ms\n",
            "step 813990 | loss 2.3031 | step time 37.22ms\n",
            "step 814000 | loss 2.5858 | step time 38.77ms\n",
            "step 814000 train loss: 2.442814350128174 test loss: 2.4826836585998535\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "cadbearndoo\n",
            "Ouchilo_Forrac\n",
            "S_RV\n",
            "KingWhiteHunter\n",
            "cooperjuni08\n",
            "EnerginX\n",
            "Dungeyf13\n",
            "tilkshk\n",
            "kingsale\n",
            "sucks_magic_bp\n",
            "--------------------------------------------------------------------------------\n",
            "step 814010 | loss 2.2952 | step time 38.94ms\n",
            "step 814020 | loss 2.3954 | step time 50.77ms\n",
            "step 814030 | loss 2.5622 | step time 37.52ms\n",
            "step 814040 | loss 2.4042 | step time 37.72ms\n",
            "step 814050 | loss 2.5191 | step time 52.09ms\n",
            "step 814060 | loss 2.3827 | step time 52.80ms\n",
            "step 814070 | loss 2.5211 | step time 57.39ms\n",
            "step 814080 | loss 2.5104 | step time 59.53ms\n",
            "step 814090 | loss 2.3986 | step time 54.34ms\n",
            "step 814100 | loss 2.5955 | step time 54.46ms\n",
            "step 814110 | loss 2.4070 | step time 53.91ms\n",
            "step 814120 | loss 2.4544 | step time 55.51ms\n",
            "step 814130 | loss 2.3790 | step time 58.33ms\n",
            "step 814140 | loss 2.6623 | step time 60.53ms\n",
            "step 814150 | loss 2.4003 | step time 37.47ms\n",
            "step 814160 | loss 2.5449 | step time 37.55ms\n",
            "step 814170 | loss 2.5181 | step time 38.49ms\n",
            "step 814180 | loss 2.4583 | step time 38.55ms\n",
            "step 814190 | loss 2.6135 | step time 37.34ms\n",
            "step 814200 | loss 2.4836 | step time 39.60ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "YeOnaForce\n",
            "EatchArgents\n",
            "lathay-wilrom\n",
            "audasPorlehappbear\n",
            "donnie5099238\n",
            "smrarg\n",
            "fluthar\n",
            "busted1hol\n",
            "RobRainehk\n",
            "Bothrowino\n",
            "--------------------------------------------------------------------------------\n",
            "step 814210 | loss 2.2982 | step time 39.81ms\n",
            "step 814220 | loss 2.4370 | step time 39.56ms\n",
            "step 814230 | loss 2.2579 | step time 37.65ms\n",
            "step 814240 | loss 2.3677 | step time 40.40ms\n",
            "step 814250 | loss 2.3342 | step time 39.46ms\n",
            "step 814260 | loss 2.5232 | step time 37.27ms\n",
            "step 814270 | loss 2.4783 | step time 38.22ms\n",
            "step 814280 | loss 2.4921 | step time 37.07ms\n",
            "step 814290 | loss 2.6109 | step time 38.71ms\n",
            "step 814300 | loss 2.5224 | step time 37.35ms\n",
            "step 814310 | loss 2.4787 | step time 38.68ms\n",
            "step 814320 | loss 2.3051 | step time 37.53ms\n",
            "step 814330 | loss 2.4833 | step time 37.93ms\n",
            "step 814340 | loss 2.1252 | step time 36.57ms\n",
            "step 814350 | loss 2.6226 | step time 35.33ms\n",
            "step 814360 | loss 2.3417 | step time 35.52ms\n",
            "step 814370 | loss 2.3817 | step time 38.89ms\n",
            "step 814380 | loss 2.3790 | step time 36.26ms\n",
            "step 814390 | loss 2.5864 | step time 59.58ms\n",
            "step 814400 | loss 2.4195 | step time 51.09ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "6mechywifisha\n",
            "Reggivepls\n",
            "Throwat_002\n",
            "Deep6\n",
            "rasmed2005\n",
            "kirothebarz\n",
            "AkandaRapiator\n",
            "dstx1990\n",
            "LisaCrayersProfeason\n",
            "scoxcrunner1\n",
            "--------------------------------------------------------------------------------\n",
            "step 814410 | loss 2.6183 | step time 58.79ms\n",
            "step 814420 | loss 2.4641 | step time 59.45ms\n",
            "step 814430 | loss 2.4325 | step time 58.85ms\n",
            "step 814440 | loss 2.3975 | step time 55.73ms\n",
            "step 814450 | loss 2.3459 | step time 37.20ms\n",
            "step 814460 | loss 2.4613 | step time 37.90ms\n",
            "step 814470 | loss 2.4261 | step time 39.50ms\n",
            "step 814480 | loss 2.4862 | step time 37.02ms\n",
            "step 814490 | loss 2.4721 | step time 48.17ms\n",
            "step 814500 | loss 2.3200 | step time 38.33ms\n",
            "step 814500 train loss: 2.4569952487945557 test loss: 2.4883716106414795\n",
            "step 814510 | loss 2.4641 | step time 36.60ms\n",
            "step 814520 | loss 2.4373 | step time 37.42ms\n",
            "step 814530 | loss 2.3496 | step time 37.06ms\n",
            "step 814540 | loss 2.5920 | step time 36.70ms\n",
            "step 814550 | loss 2.3653 | step time 35.83ms\n",
            "step 814560 | loss 2.4500 | step time 36.24ms\n",
            "step 814570 | loss 2.4203 | step time 41.75ms\n",
            "step 814580 | loss 2.2845 | step time 37.62ms\n",
            "step 814590 | loss 2.4195 | step time 35.60ms\n",
            "step 814600 | loss 2.4053 | step time 40.74ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "throwawaysweet\n",
            "katherhaternippus\n",
            "Powerswag\n",
            "NotYouLineMichaelMes\n",
            "GreorgePeleramotos\n",
            "lunavid288\n",
            "Fillmewave_blake\n",
            "Noravyro\n",
            "trj8tz\n",
            "marndoeskitty\n",
            "--------------------------------------------------------------------------------\n",
            "step 814610 | loss 2.4241 | step time 39.73ms\n",
            "step 814620 | loss 2.3444 | step time 37.96ms\n",
            "step 814630 | loss 2.2929 | step time 37.33ms\n",
            "step 814640 | loss 2.5235 | step time 40.15ms\n",
            "step 814650 | loss 2.2847 | step time 37.70ms\n",
            "step 814660 | loss 2.3495 | step time 55.87ms\n",
            "step 814670 | loss 2.4843 | step time 56.28ms\n",
            "step 814680 | loss 2.4228 | step time 62.88ms\n",
            "step 814690 | loss 2.2270 | step time 52.53ms\n",
            "step 814700 | loss 2.5249 | step time 63.97ms\n",
            "step 814710 | loss 2.4474 | step time 56.01ms\n",
            "step 814720 | loss 2.4291 | step time 55.21ms\n",
            "step 814730 | loss 2.4758 | step time 54.61ms\n",
            "step 814740 | loss 2.3509 | step time 57.52ms\n",
            "step 814750 | loss 2.4121 | step time 56.89ms\n",
            "step 814760 | loss 2.4036 | step time 36.18ms\n",
            "step 814770 | loss 2.3199 | step time 46.19ms\n",
            "step 814780 | loss 2.3896 | step time 37.70ms\n",
            "step 814790 | loss 2.6096 | step time 36.64ms\n",
            "step 814800 | loss 2.5258 | step time 35.71ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Casengo-Man\n",
            "PrusMikez\n",
            "tavldents\n",
            "rcj-\n",
            "emiron2018\n",
            "ShitCreamed\n",
            "scipeszchasen\n",
            "fom-ican-melesagent\n",
            "chuckytoww\n",
            "Mora_Chicken\n",
            "--------------------------------------------------------------------------------\n",
            "step 814810 | loss 2.3399 | step time 38.29ms\n",
            "step 814820 | loss 2.4098 | step time 36.88ms\n",
            "step 814830 | loss 2.5031 | step time 38.07ms\n",
            "step 814840 | loss 2.6080 | step time 36.64ms\n",
            "step 814850 | loss 2.4391 | step time 39.15ms\n",
            "step 814860 | loss 2.3787 | step time 39.53ms\n",
            "step 814870 | loss 2.3343 | step time 39.42ms\n",
            "step 814880 | loss 2.5405 | step time 35.64ms\n",
            "step 814890 | loss 2.5160 | step time 37.19ms\n",
            "step 814900 | loss 2.3983 | step time 37.92ms\n",
            "step 814910 | loss 2.4461 | step time 40.54ms\n",
            "step 814920 | loss 2.3195 | step time 37.76ms\n",
            "step 814930 | loss 2.5660 | step time 37.06ms\n",
            "step 814940 | loss 2.6474 | step time 37.91ms\n",
            "step 814950 | loss 2.3991 | step time 38.42ms\n",
            "step 814960 | loss 2.5484 | step time 37.27ms\n",
            "step 814970 | loss 2.3702 | step time 40.73ms\n",
            "step 814980 | loss 2.3436 | step time 35.71ms\n",
            "step 814990 | loss 2.5224 | step time 37.24ms\n",
            "step 815000 | loss 2.5362 | step time 56.70ms\n",
            "step 815000 train loss: 2.452307939529419 test loss: 2.4850990772247314\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "xFyrendfireX\n",
            "ShyChihi\n",
            "DingerSolia\n",
            "TheSei_Yph\n",
            "paanpicino\n",
            "Red_Of_BellinelLife\n",
            "TiredAfread\n",
            "5MadDners\n",
            "accountainthrad\n",
            "screw00\n",
            "--------------------------------------------------------------------------------\n",
            "step 815010 | loss 2.5852 | step time 59.60ms\n",
            "step 815020 | loss 2.4186 | step time 57.22ms\n",
            "step 815030 | loss 2.4981 | step time 57.74ms\n",
            "step 815040 | loss 2.4614 | step time 58.59ms\n",
            "step 815050 | loss 2.5548 | step time 59.08ms\n",
            "step 815060 | loss 2.4948 | step time 42.09ms\n",
            "step 815070 | loss 2.4057 | step time 36.68ms\n",
            "step 815080 | loss 2.3219 | step time 41.61ms\n",
            "step 815090 | loss 2.5463 | step time 39.01ms\n",
            "step 815100 | loss 2.3946 | step time 38.46ms\n",
            "step 815110 | loss 2.4042 | step time 39.86ms\n",
            "step 815120 | loss 2.3181 | step time 39.82ms\n",
            "step 815130 | loss 2.4403 | step time 38.95ms\n",
            "step 815140 | loss 2.4434 | step time 37.40ms\n",
            "step 815150 | loss 2.2072 | step time 44.04ms\n",
            "step 815160 | loss 2.5283 | step time 38.54ms\n",
            "step 815170 | loss 2.4813 | step time 38.91ms\n",
            "step 815180 | loss 2.4823 | step time 41.19ms\n",
            "step 815190 | loss 2.6195 | step time 37.38ms\n",
            "step 815200 | loss 2.5413 | step time 39.93ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "StripsMp9\n",
            "hight196523\n",
            "icyshooky\n",
            "Jammin_mora\n",
            "Albinu73\n",
            "bbs989569\n",
            "bridgerune\n",
            "dintseveryeailkit\n",
            "photoshfitness2\n",
            "bexaloneo\n",
            "--------------------------------------------------------------------------------\n",
            "step 815210 | loss 2.4984 | step time 38.95ms\n",
            "step 815220 | loss 2.3686 | step time 41.20ms\n",
            "step 815230 | loss 2.3471 | step time 37.11ms\n",
            "step 815240 | loss 2.5493 | step time 41.07ms\n",
            "step 815250 | loss 2.5715 | step time 39.57ms\n",
            "step 815260 | loss 2.5431 | step time 54.86ms\n",
            "step 815270 | loss 2.4654 | step time 38.47ms\n",
            "step 815280 | loss 2.4498 | step time 39.34ms\n",
            "step 815290 | loss 2.4861 | step time 67.23ms\n",
            "step 815300 | loss 2.5160 | step time 53.63ms\n",
            "step 815310 | loss 2.3627 | step time 58.72ms\n",
            "step 815320 | loss 2.4747 | step time 61.33ms\n",
            "step 815330 | loss 2.4625 | step time 57.50ms\n",
            "step 815340 | loss 2.4736 | step time 62.89ms\n",
            "step 815350 | loss 2.5260 | step time 56.63ms\n",
            "step 815360 | loss 2.4383 | step time 56.11ms\n",
            "step 815370 | loss 2.4864 | step time 57.57ms\n",
            "step 815380 | loss 2.4619 | step time 39.85ms\n",
            "step 815390 | loss 2.3600 | step time 42.45ms\n",
            "step 815400 | loss 2.4905 | step time 38.08ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "malgrgoni\n",
            "anynico\n",
            "cole-SA\n",
            "x_brood_at_trish\n",
            "jawkundrun\n",
            "himaravi\n",
            "Mi_SquirrelOff\n",
            "-board-\n",
            "DrumonJefferst\n",
            "themnroamerlin\n",
            "--------------------------------------------------------------------------------\n",
            "step 815410 | loss 2.3862 | step time 40.59ms\n",
            "step 815420 | loss 2.5327 | step time 37.85ms\n",
            "step 815430 | loss 2.4134 | step time 42.69ms\n",
            "step 815440 | loss 2.4669 | step time 38.92ms\n",
            "step 815450 | loss 2.3435 | step time 41.73ms\n",
            "step 815460 | loss 2.4413 | step time 39.60ms\n",
            "step 815470 | loss 2.6174 | step time 40.76ms\n",
            "step 815480 | loss 2.5549 | step time 38.13ms\n",
            "step 815490 | loss 2.5233 | step time 54.04ms\n",
            "step 815500 | loss 2.4938 | step time 40.18ms\n",
            "step 815500 train loss: 2.470227003097534 test loss: 2.4794344902038574\n",
            "step 815510 | loss 2.5040 | step time 39.05ms\n",
            "step 815520 | loss 2.5595 | step time 38.69ms\n",
            "step 815530 | loss 2.3392 | step time 39.65ms\n",
            "step 815540 | loss 2.6109 | step time 40.06ms\n",
            "step 815550 | loss 2.6161 | step time 38.20ms\n",
            "step 815560 | loss 2.3698 | step time 43.18ms\n",
            "step 815570 | loss 2.2971 | step time 37.95ms\n",
            "step 815580 | loss 2.2908 | step time 38.88ms\n",
            "step 815590 | loss 2.1464 | step time 59.99ms\n",
            "step 815600 | loss 2.6731 | step time 50.44ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "boetrotkingsking_\n",
            "throwawayk9\n",
            "Doller858\n",
            "dontmfdismy\n",
            "squallsocially\n",
            "BENTperrepLove\n",
            "Acamru\n",
            "The_Spunky_kifa\n",
            "mtalows\n",
            "GenCuCasfeix\n",
            "--------------------------------------------------------------------------------\n",
            "step 815610 | loss 2.3947 | step time 60.03ms\n",
            "step 815620 | loss 2.3158 | step time 66.90ms\n",
            "step 815630 | loss 2.4004 | step time 62.18ms\n",
            "step 815640 | loss 2.5431 | step time 57.56ms\n",
            "step 815650 | loss 2.5026 | step time 61.81ms\n",
            "step 815660 | loss 2.6551 | step time 37.52ms\n",
            "step 815670 | loss 2.3581 | step time 40.96ms\n",
            "step 815680 | loss 2.5844 | step time 37.99ms\n",
            "step 815690 | loss 2.2865 | step time 38.26ms\n",
            "step 815700 | loss 2.5265 | step time 39.27ms\n",
            "step 815710 | loss 2.4625 | step time 38.02ms\n",
            "step 815720 | loss 2.3396 | step time 40.76ms\n",
            "step 815730 | loss 2.4972 | step time 41.55ms\n",
            "step 815740 | loss 2.5833 | step time 40.47ms\n",
            "step 815750 | loss 2.3225 | step time 42.61ms\n",
            "step 815760 | loss 2.5693 | step time 38.19ms\n",
            "step 815770 | loss 2.4392 | step time 39.98ms\n",
            "step 815780 | loss 2.5588 | step time 36.29ms\n",
            "step 815790 | loss 2.3857 | step time 39.29ms\n",
            "step 815800 | loss 2.5809 | step time 37.47ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Krunchkin\n",
            "SIANGLOLPRO\n",
            "yosh_fast_bitch\n",
            "Rightonisunara\n",
            "usedway1color\n",
            "fryponthandstar\n",
            "Muchjungebusdo1967\n",
            "jimmithusin\n",
            "Cokvanguyenamer\n",
            "NeidewayImsi\n",
            "--------------------------------------------------------------------------------\n",
            "step 815810 | loss 2.6269 | step time 39.21ms\n",
            "step 815820 | loss 2.3488 | step time 38.75ms\n",
            "step 815830 | loss 2.6172 | step time 40.72ms\n",
            "step 815840 | loss 2.5157 | step time 41.32ms\n",
            "step 815850 | loss 2.3723 | step time 39.94ms\n",
            "step 815860 | loss 2.5242 | step time 38.45ms\n",
            "step 815870 | loss 2.3517 | step time 38.72ms\n",
            "step 815880 | loss 2.5304 | step time 69.20ms\n",
            "step 815890 | loss 2.1985 | step time 51.97ms\n",
            "step 815900 | loss 2.5635 | step time 54.40ms\n",
            "step 815910 | loss 2.4002 | step time 57.37ms\n",
            "step 815920 | loss 2.3731 | step time 54.76ms\n",
            "step 815930 | loss 2.4355 | step time 54.69ms\n",
            "step 815940 | loss 2.3071 | step time 57.96ms\n",
            "step 815950 | loss 2.5478 | step time 55.66ms\n",
            "step 815960 | loss 2.3992 | step time 60.40ms\n",
            "step 815970 | loss 2.5510 | step time 41.24ms\n",
            "step 815980 | loss 2.4734 | step time 43.53ms\n",
            "step 815990 | loss 2.5382 | step time 36.93ms\n",
            "step 816000 | loss 2.4679 | step time 37.22ms\n",
            "step 816000 train loss: 2.4295766353607178 test loss: 2.481727123260498\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Danzyginsy\n",
            "stupidkids\n",
            "Imperator\n",
            "Lizzerslek\n",
            "Sakgeferm10\n",
            "BatLooks\n",
            "hApitherReaper\n",
            "pickpartymc\n",
            "vitarisp\n",
            "ElfectEevequences\n",
            "--------------------------------------------------------------------------------\n",
            "step 816010 | loss 2.3193 | step time 37.97ms\n",
            "step 816020 | loss 2.1596 | step time 39.77ms\n",
            "step 816030 | loss 2.3460 | step time 38.14ms\n",
            "step 816040 | loss 2.6027 | step time 36.36ms\n",
            "step 816050 | loss 2.4967 | step time 42.12ms\n",
            "step 816060 | loss 2.4111 | step time 39.19ms\n",
            "step 816070 | loss 2.3709 | step time 37.96ms\n",
            "step 816080 | loss 2.6702 | step time 40.14ms\n",
            "step 816090 | loss 2.4321 | step time 49.95ms\n",
            "step 816100 | loss 2.1832 | step time 37.21ms\n",
            "step 816110 | loss 2.3496 | step time 41.16ms\n",
            "step 816120 | loss 2.4725 | step time 38.28ms\n",
            "step 816130 | loss 2.5492 | step time 39.28ms\n",
            "step 816140 | loss 2.5035 | step time 41.42ms\n",
            "step 816150 | loss 2.3954 | step time 39.38ms\n",
            "step 816160 | loss 2.4139 | step time 35.37ms\n",
            "step 816170 | loss 2.4428 | step time 40.76ms\n",
            "step 816180 | loss 2.3956 | step time 62.20ms\n",
            "step 816190 | loss 2.4953 | step time 53.88ms\n",
            "step 816200 | loss 2.4339 | step time 60.52ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "kianossiel\n",
            "zimere7897\n",
            "thunder_9985\n",
            "Over_In_Sight_Kround\n",
            "carbonie1\n",
            "Zanorok723\n",
            "gMbgil\n",
            "Hiefiest\n",
            "sterllarsnutar1\n",
            "kallenturialt\n",
            "--------------------------------------------------------------------------------\n",
            "step 816210 | loss 2.4955 | step time 59.35ms\n",
            "step 816220 | loss 2.5612 | step time 62.52ms\n",
            "step 816230 | loss 2.3771 | step time 58.76ms\n",
            "step 816240 | loss 2.5076 | step time 40.00ms\n",
            "step 816250 | loss 2.5112 | step time 38.87ms\n",
            "step 816260 | loss 2.5908 | step time 41.62ms\n",
            "step 816270 | loss 2.3359 | step time 42.04ms\n",
            "step 816280 | loss 2.4483 | step time 38.44ms\n",
            "step 816290 | loss 2.5600 | step time 35.87ms\n",
            "step 816300 | loss 2.3194 | step time 39.72ms\n",
            "step 816310 | loss 2.4543 | step time 41.20ms\n",
            "step 816320 | loss 2.5083 | step time 38.89ms\n",
            "step 816330 | loss 2.4009 | step time 39.39ms\n",
            "step 816340 | loss 2.4024 | step time 38.36ms\n",
            "step 816350 | loss 2.4766 | step time 38.96ms\n",
            "step 816360 | loss 2.5242 | step time 39.90ms\n",
            "step 816370 | loss 2.4538 | step time 41.26ms\n",
            "step 816380 | loss 2.5701 | step time 39.45ms\n",
            "step 816390 | loss 2.6450 | step time 39.04ms\n",
            "step 816400 | loss 2.4289 | step time 41.39ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "OhTheKane\n",
            "macksonkler7\n",
            "ManEfeever\n",
            "Lagh1000h3g0i_norms\n",
            "houndhelp2200\n",
            "NicoleEmezart\n",
            "lovegames\n",
            "15go6\n",
            "RoomBeard\n",
            "caultar01\n",
            "--------------------------------------------------------------------------------\n",
            "step 816410 | loss 2.2368 | step time 42.34ms\n",
            "step 816420 | loss 2.4277 | step time 42.07ms\n",
            "step 816430 | loss 2.4034 | step time 40.28ms\n",
            "step 816440 | loss 2.4846 | step time 40.82ms\n",
            "step 816450 | loss 2.4532 | step time 52.71ms\n",
            "step 816460 | loss 2.6194 | step time 37.85ms\n",
            "step 816470 | loss 2.6375 | step time 67.60ms\n",
            "step 816480 | loss 2.6496 | step time 54.06ms\n",
            "step 816490 | loss 2.4105 | step time 59.48ms\n",
            "step 816500 | loss 2.3578 | step time 60.26ms\n",
            "step 816500 train loss: 2.4404115676879883 test loss: 2.4800610542297363\n",
            "step 816510 | loss 2.5310 | step time 59.62ms\n",
            "step 816520 | loss 2.3605 | step time 59.07ms\n",
            "step 816530 | loss 2.4874 | step time 58.11ms\n",
            "step 816540 | loss 2.3749 | step time 37.25ms\n",
            "step 816550 | loss 2.5299 | step time 39.48ms\n",
            "step 816560 | loss 2.4319 | step time 38.67ms\n",
            "step 816570 | loss 2.3298 | step time 36.46ms\n",
            "step 816580 | loss 2.3029 | step time 38.81ms\n",
            "step 816590 | loss 2.2552 | step time 37.94ms\n",
            "step 816600 | loss 2.5933 | step time 39.26ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "ymptyne\n",
            "darthy75\n",
            "Lightenimrido9\n",
            "yunobbeoblew\n",
            "vcfln\n",
            "hairyOccupgf\n",
            "bushla\n",
            "obversxsloge\n",
            "TrollingFattyProgica\n",
            "setakanqueus\n",
            "--------------------------------------------------------------------------------\n",
            "step 816610 | loss 2.4391 | step time 43.17ms\n",
            "step 816620 | loss 2.4881 | step time 38.91ms\n",
            "step 816630 | loss 2.4104 | step time 38.96ms\n",
            "step 816640 | loss 2.4114 | step time 42.22ms\n",
            "step 816650 | loss 2.3711 | step time 39.34ms\n",
            "step 816660 | loss 2.5229 | step time 44.61ms\n",
            "step 816670 | loss 2.3631 | step time 39.76ms\n",
            "step 816680 | loss 2.4143 | step time 40.34ms\n",
            "step 816690 | loss 2.6094 | step time 41.07ms\n",
            "step 816700 | loss 2.2635 | step time 37.32ms\n",
            "step 816710 | loss 2.3267 | step time 37.65ms\n",
            "step 816720 | loss 2.5725 | step time 37.74ms\n",
            "step 816730 | loss 2.6176 | step time 38.81ms\n",
            "step 816740 | loss 2.5106 | step time 39.82ms\n",
            "step 816750 | loss 2.4925 | step time 38.81ms\n",
            "step 816760 | loss 2.3326 | step time 40.27ms\n",
            "step 816770 | loss 2.4783 | step time 57.92ms\n",
            "step 816780 | loss 2.2627 | step time 52.34ms\n",
            "step 816790 | loss 2.5148 | step time 53.89ms\n",
            "step 816800 | loss 2.4653 | step time 59.54ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "thesleepermal\n",
            "TEXMSKm\n",
            "chico240\n",
            "ireallyonefreak\n",
            "EnterAndHate\n",
            "Rumbo_FD\n",
            "Kaenasgvee\n",
            "karrychi\n",
            "MagnKnight\n",
            "Chopical_YShow\n",
            "--------------------------------------------------------------------------------\n",
            "step 816810 | loss 2.4313 | step time 55.96ms\n",
            "step 816820 | loss 2.4497 | step time 70.85ms\n",
            "step 816830 | loss 2.4194 | step time 54.82ms\n",
            "step 816840 | loss 2.5230 | step time 40.54ms\n",
            "step 816850 | loss 2.1839 | step time 40.70ms\n",
            "step 816860 | loss 2.4712 | step time 40.42ms\n",
            "step 816870 | loss 2.5529 | step time 41.04ms\n",
            "step 816880 | loss 2.5928 | step time 38.32ms\n",
            "step 816890 | loss 2.4818 | step time 37.36ms\n",
            "step 816900 | loss 2.4573 | step time 37.89ms\n",
            "step 816910 | loss 2.7956 | step time 41.07ms\n",
            "step 816920 | loss 2.4149 | step time 38.04ms\n",
            "step 816930 | loss 2.3825 | step time 39.66ms\n",
            "step 816940 | loss 2.4087 | step time 43.98ms\n",
            "step 816950 | loss 2.4828 | step time 40.78ms\n",
            "step 816960 | loss 2.4003 | step time 37.88ms\n",
            "step 816970 | loss 2.4984 | step time 42.49ms\n",
            "step 816980 | loss 2.4797 | step time 42.81ms\n",
            "step 816990 | loss 2.4928 | step time 39.73ms\n",
            "step 817000 | loss 2.4611 | step time 43.17ms\n",
            "step 817000 train loss: 2.436436414718628 test loss: 2.480760335922241\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "AnarcusTheEmios\n",
            "Weasel35\n",
            "mnb1114\n",
            "Igoldeniginawill\n",
            "NeverTechStacka\n",
            "DRLDDBUDG\n",
            "thefromfactory\n",
            "HustlerRomes\n",
            "grandmaster100\n",
            "InvilleGirl\n",
            "--------------------------------------------------------------------------------\n",
            "step 817010 | loss 2.3285 | step time 38.79ms\n",
            "step 817020 | loss 2.5451 | step time 43.79ms\n",
            "step 817030 | loss 2.6336 | step time 40.38ms\n",
            "step 817040 | loss 2.3825 | step time 38.32ms\n",
            "step 817050 | loss 2.5745 | step time 55.85ms\n",
            "step 817060 | loss 2.6113 | step time 54.19ms\n",
            "step 817070 | loss 2.3466 | step time 60.56ms\n",
            "step 817080 | loss 2.4985 | step time 57.75ms\n",
            "step 817090 | loss 2.5082 | step time 61.51ms\n",
            "step 817100 | loss 2.3873 | step time 60.51ms\n",
            "step 817110 | loss 2.4483 | step time 60.66ms\n",
            "step 817120 | loss 2.4740 | step time 64.00ms\n",
            "step 817130 | loss 2.3913 | step time 38.05ms\n",
            "step 817140 | loss 2.3006 | step time 39.34ms\n",
            "step 817150 | loss 2.4355 | step time 39.44ms\n",
            "step 817160 | loss 2.5480 | step time 42.45ms\n",
            "step 817170 | loss 2.5426 | step time 39.40ms\n",
            "step 817180 | loss 2.5970 | step time 39.34ms\n",
            "step 817190 | loss 2.4591 | step time 38.63ms\n",
            "step 817200 | loss 2.5878 | step time 39.50ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "Uniphonecomex\n",
            "bqreadypop\n",
            "kimuhsb\n",
            "rawrenca\n",
            "mexican96\n",
            "corriousthrowawayfak\n",
            "\n",
            "LostMan\n",
            "spiffercloudsaderyou\n",
            "buizku\n",
            "--------------------------------------------------------------------------------\n",
            "step 817210 | loss 2.4487 | step time 39.65ms\n",
            "step 817220 | loss 2.3947 | step time 38.49ms\n",
            "step 817230 | loss 2.4180 | step time 55.25ms\n",
            "step 817240 | loss 2.5692 | step time 37.42ms\n",
            "step 817250 | loss 2.5099 | step time 38.91ms\n",
            "step 817260 | loss 2.3859 | step time 38.50ms\n",
            "step 817270 | loss 2.4949 | step time 39.94ms\n",
            "step 817280 | loss 2.5129 | step time 37.90ms\n",
            "step 817290 | loss 2.3186 | step time 39.83ms\n",
            "step 817300 | loss 2.4522 | step time 39.82ms\n",
            "step 817310 | loss 2.3795 | step time 39.37ms\n",
            "step 817320 | loss 2.4738 | step time 39.87ms\n",
            "step 817330 | loss 2.4051 | step time 40.94ms\n",
            "step 817340 | loss 2.4207 | step time 47.54ms\n",
            "step 817350 | loss 2.5123 | step time 40.64ms\n",
            "step 817360 | loss 2.4230 | step time 57.35ms\n",
            "step 817370 | loss 2.4648 | step time 53.43ms\n",
            "step 817380 | loss 2.4492 | step time 54.10ms\n",
            "step 817390 | loss 2.5159 | step time 58.22ms\n",
            "step 817400 | loss 2.4853 | step time 60.70ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "FanoMoreMcGrandi83\n",
            "irmaingaposnii\n",
            "meatlon101\n",
            "KoushenPhustle\n",
            "RedditLebort\n",
            "TamanoCak\n",
            "trider94\n",
            "gotshoptoolsauce\n",
            "Pamoli\n",
            "baser92\n",
            "--------------------------------------------------------------------------------\n",
            "step 817410 | loss 2.4819 | step time 58.32ms\n",
            "step 817420 | loss 2.6647 | step time 59.36ms\n",
            "step 817430 | loss 2.5435 | step time 38.68ms\n",
            "step 817440 | loss 2.4912 | step time 39.56ms\n",
            "step 817450 | loss 2.6012 | step time 39.62ms\n",
            "step 817460 | loss 2.3188 | step time 37.82ms\n",
            "step 817470 | loss 2.3441 | step time 36.60ms\n",
            "step 817480 | loss 2.5309 | step time 45.81ms\n",
            "step 817490 | loss 2.4682 | step time 37.71ms\n",
            "step 817500 | loss 2.5003 | step time 38.38ms\n",
            "step 817500 train loss: 2.447896718978882 test loss: 2.481537342071533\n",
            "step 817510 | loss 2.4197 | step time 41.05ms\n",
            "step 817520 | loss 2.4045 | step time 37.27ms\n",
            "step 817530 | loss 2.6168 | step time 38.12ms\n",
            "step 817540 | loss 2.3737 | step time 43.94ms\n",
            "step 817550 | loss 2.4605 | step time 37.44ms\n",
            "step 817560 | loss 2.4847 | step time 38.83ms\n",
            "step 817570 | loss 2.6224 | step time 37.24ms\n",
            "step 817580 | loss 2.2741 | step time 39.17ms\n",
            "step 817590 | loss 2.3318 | step time 39.58ms\n",
            "step 817600 | loss 2.5646 | step time 37.54ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "RevelalFedorias\n",
            "Swag42\n",
            "Shinahilla\n",
            "inAshamajor\n",
            "natih90ayou3\n",
            "thr0waway77z73\n",
            "rudnke\n",
            "MariousTUS\n",
            "yovball\n",
            "csrfx\n",
            "--------------------------------------------------------------------------------\n",
            "step 817610 | loss 2.5282 | step time 38.27ms\n",
            "step 817620 | loss 2.2314 | step time 39.83ms\n",
            "step 817630 | loss 2.2587 | step time 54.19ms\n",
            "step 817640 | loss 2.5550 | step time 34.85ms\n",
            "step 817650 | loss 2.7181 | step time 55.32ms\n",
            "step 817660 | loss 2.4941 | step time 55.30ms\n",
            "step 817670 | loss 2.4519 | step time 55.15ms\n",
            "step 817680 | loss 2.5315 | step time 56.24ms\n",
            "step 817690 | loss 2.4442 | step time 58.87ms\n",
            "step 817700 | loss 2.4199 | step time 59.28ms\n",
            "step 817710 | loss 2.3033 | step time 60.37ms\n",
            "step 817720 | loss 2.2979 | step time 54.30ms\n",
            "step 817730 | loss 2.2983 | step time 36.57ms\n",
            "step 817740 | loss 2.5733 | step time 39.69ms\n",
            "step 817750 | loss 2.5880 | step time 38.80ms\n",
            "step 817760 | loss 2.3905 | step time 41.35ms\n",
            "step 817770 | loss 2.3711 | step time 38.66ms\n",
            "step 817780 | loss 2.5203 | step time 38.00ms\n",
            "step 817790 | loss 2.5701 | step time 39.32ms\n",
            "step 817800 | loss 2.6524 | step time 36.98ms\n",
            "--------------------------------------------------------------------------------\n",
            "0 samples that are in train:\n",
            "0 samples that are in test:\n",
            "10 samples that are new:\n",
            "HelpMeCantsPosts\n",
            "seeker207\n",
            "wozzlew\n",
            "GhaPsling\n",
            "iwantada11\n",
            "AlassonBled\n",
            "BotherBio809DC\n",
            "picshanch\n",
            "AliceTiger\n",
            "Archod13\n",
            "--------------------------------------------------------------------------------\n",
            "step 817810 | loss 2.3501 | step time 36.77ms\n",
            "step 817820 | loss 2.2464 | step time 38.26ms\n",
            "step 817830 | loss 2.3548 | step time 42.34ms\n",
            "step 817840 | loss 2.4259 | step time 46.90ms\n",
            "step 817850 | loss 2.3334 | step time 37.13ms\n",
            "step 817860 | loss 2.3241 | step time 35.56ms\n",
            "step 817870 | loss 2.2621 | step time 44.49ms\n",
            "step 817880 | loss 2.4802 | step time 37.57ms\n",
            "step 817890 | loss 2.4362 | step time 38.28ms\n",
            "step 817900 | loss 2.4356 | step time 39.38ms\n",
            "step 817910 | loss 2.3140 | step time 38.58ms\n",
            "step 817920 | loss 2.4258 | step time 38.60ms\n",
            "step 817930 | loss 2.5011 | step time 37.38ms\n",
            "step 817940 | loss 2.2821 | step time 40.54ms\n",
            "step 817950 | loss 2.3979 | step time 37.51ms\n",
            "step 817960 | loss 2.4958 | step time 35.11ms\n",
            "step 817970 | loss 2.2562 | step time 55.15ms\n",
            "step 817980 | loss 2.4927 | step time 52.10ms\n",
            "step 817990 | loss 2.4524 | step time 61.35ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ypfq2RtPUz7b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}